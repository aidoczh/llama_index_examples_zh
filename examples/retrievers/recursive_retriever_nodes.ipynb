{"cells": [{"cell_type": "markdown", "id": "94f8a023", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/retrievers/recursive_retriever_nodes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "id": "025f3e20-aec9-491c-8c90-234aed406a25", "metadata": {}, "source": ["# é€’å½’æ£€ç´¢å™¨ + èŠ‚ç‚¹å¼•ç”¨\n", "\n", "æœ¬æŒ‡å—å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é€’å½’æ£€ç´¢æ¥éå†èŠ‚ç‚¹å…³ç³»ï¼Œå¹¶æ ¹æ®â€œå¼•ç”¨â€è·å–èŠ‚ç‚¹ã€‚\n", "\n", "èŠ‚ç‚¹å¼•ç”¨æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¦‚å¿µã€‚å½“æ‚¨é¦–æ¬¡æ‰§è¡Œæ£€ç´¢æ—¶ï¼Œæ‚¨å¯èƒ½å¸Œæœ›æ£€ç´¢å¼•ç”¨è€Œä¸æ˜¯åŸå§‹æ–‡æœ¬ã€‚æ‚¨å¯ä»¥æœ‰å¤šä¸ªå¼•ç”¨æŒ‡å‘åŒä¸€ä¸ªèŠ‚ç‚¹ã€‚\n", "\n", "åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†èŠ‚ç‚¹å¼•ç”¨çš„ä¸€äº›ä¸åŒç”¨æ³•ï¼š\n", "- **å—å¼•ç”¨**ï¼šä¸åŒå—å¤§å°æŒ‡å‘ä¸€ä¸ªæ›´å¤§çš„å—\n", "- **å…ƒæ•°æ®å¼•ç”¨**ï¼šæ‘˜è¦ + ç”Ÿæˆçš„é—®é¢˜æŒ‡å‘ä¸€ä¸ªæ›´å¤§çš„å—\n"]}, {"cell_type": "code", "execution_count": null, "id": "87ca1171", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai\n", "%pip install llama-index-readers-file"]}, {"cell_type": "code", "execution_count": null, "id": "ee583e89-a508-493e-b232-42e520ce19de", "metadata": {}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2\n", "%env OPENAI_API_KEY=YOUR_OPENAI_KEY"]}, {"cell_type": "markdown", "id": "691e6b21", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "42164863", "metadata": {}, "outputs": [], "source": ["!pip install llama-index pypdf"]}, {"cell_type": "markdown", "id": "273f38de-e79a-4ce2-ad4e-2c70afc33f34", "metadata": {}, "source": ["## åŠ è½½æ•°æ® + è®¾ç½®\n", "\n", "åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä¸‹è½½ Llama 2 è®ºæ–‡å¹¶åˆ›å»ºä¸€ä¸ªåˆå§‹èŠ‚ç‚¹é›†ï¼ˆå—å¤§å°ä¸º 1024ï¼‰ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "1eb829ef-b54b-4095-a832-6d1d115aa645", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n", "ERROR: could not open HSTS store at '/home/loganm/.wget-hsts'. HSTS will be disabled.\n", "--2024-01-01 11:13:01--  https://arxiv.org/pdf/2307.09288.pdf\n", "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.131.42, 151.101.67.42, ...\n", "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 13661300 (13M) [application/pdf]\n", "Saving to: â€˜data/llama2.pdfâ€™\n", "\n", "data/llama2.pdf     100%[===================>]  13.03M  27.3MB/s    in 0.5s    \n", "\n", "2024-01-01 11:13:02 (27.3 MB/s) - â€˜data/llama2.pdfâ€™ saved [13661300/13661300]\n", "\n"]}], "source": ["!mkdir -p 'data/'\n", "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""]}, {"cell_type": "code", "execution_count": null, "id": "6cd97455-5ff3-43ee-8222-f496ec234dc7", "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "from llama_index.readers.file import PDFReader\n", "from llama_index.core.response.notebook_utils import display_source_node\n", "from llama_index.core.retrievers import RecursiveRetriever\n", "from llama_index.core.query_engine import RetrieverQueryEngine\n", "from llama_index.core import VectorStoreIndex\n", "from llama_index.llms.openai import OpenAI\n", "import json"]}, {"cell_type": "code", "execution_count": null, "id": "a07c0e42-1ae8-4267-9355-6bb75323f82a", "metadata": {}, "outputs": [], "source": ["loader = PDFReader()\n", "docs0 = loader.load_data(file=Path(\"./data/llama2.pdf\"))"]}, {"cell_type": "code", "execution_count": null, "id": "493e5492-a6ae-4e3e-aa23-274c0605b165", "metadata": {}, "outputs": [], "source": ["from llama_index.core import Document\n", "\n", "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n", "docs = [Document(text=doc_text)]"]}, {"cell_type": "code", "execution_count": null, "id": "7c2abcd3-6cae-49dd-8719-9b738d000652", "metadata": {}, "outputs": [], "source": ["from llama_index.core.node_parser import SentenceSplitter\n", "from llama_index.core.schema import IndexNode"]}, {"cell_type": "code", "execution_count": null, "id": "91b997ae-9260-4ae7-af2f-0f8d38625d32", "metadata": {}, "outputs": [], "source": ["node_parser = SentenceSplitter(chunk_size=1024)"]}, {"cell_type": "code", "execution_count": null, "id": "0cda44b0-fd27-4255-9aa7-08d358635772", "metadata": {}, "outputs": [], "source": ["base_nodes = node_parser.get_nodes_from_documents(docs)", "# å°†èŠ‚ç‚¹idè®¾ç½®ä¸ºä¸€ä¸ªå¸¸é‡", "for idx, node in enumerate(base_nodes):", "    node.id_ = f\"node-{idx}\""]}, {"cell_type": "code", "execution_count": null, "id": "38e47623-b67d-45d6-9b24-33ba84719f1f", "metadata": {}, "outputs": [], "source": ["from llama_index.core.embeddings import resolve_embed_model\n", "\n", "embed_model = resolve_embed_model(\"local:BAAI/bge-small-en\")\n", "llm = OpenAI(model=\"gpt-3.5-turbo\")"]}, {"cell_type": "markdown", "id": "f43ebab2-fc46-41ea-8a92-9148994d793f", "metadata": {}, "source": ["## åŸºå‡†æ£€ç´¢å™¨\n", "\n", "å®šä¹‰ä¸€ä¸ªåŸºå‡†æ£€ç´¢å™¨ï¼Œå®ƒé€šè¿‡åµŒå…¥ç›¸ä¼¼åº¦ç®€å•åœ°è·å–å‰kä¸ªåŸå§‹æ–‡æœ¬èŠ‚ç‚¹ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "704fb3da-710e-4ad9-b630-565911917f0c", "metadata": {}, "outputs": [], "source": ["base_index = VectorStoreIndex(base_nodes, embed_model=embed_model)\n", "base_retriever = base_index.as_retriever(similarity_top_k=2)"]}, {"cell_type": "code", "execution_count": null, "id": "160c339b-601a-486b-9e17-dd6cc9f133ea", "metadata": {}, "outputs": [], "source": ["retrievals = base_retriever.retrieve(\n", "    \"Can you tell me about the key concepts for safety finetuning\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "632610f3-c8f2-440a-ab27-5ca7d65f882a", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Node ID:** node-26<br>**Similarity:** 0.8581930837671874<br>**Text:** AsLLMsareintegratedanddeployed,welookforwardto\n", "continuing research that will amplify their potential for positive impact on these important social issues.\n", "4.2 Safety Fine-Tuning\n", "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n", "guidelines,andthetechniquesweusetomitigatesafetyrisks. Weemployaprocesssimilartothegeneral\n", "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n", "Specifically, we use the following techniques in safety fine-tuning:\n", "1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\n", "tions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\n", "themodeltoalignwithoursafetyguidelinesevenbeforeRLHF,andthuslaysthefoundationfor\n", "high-quality human preference data annotation.\n", "2.Safety RLHF : Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n", "tion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\n", "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n", "3.SafetyContextDistillation : Finally,werefineourRLHFpipelinewithcontextdistillation(Askell\n", "etal.,2021b). Thisinvolvesgeneratingsafermodelresponsesbyprefixingapromptwithasafety\n", "preprompt, e.g., â€œYou are a safe and responsible assistant,â€ and then fine-tuning the model on the safer\n", "responses without the preprompt, which essentially distill...<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Node ID:** node-33<br>**Similarity:** 0.8442806881920294<br>**Text:** Examples of evaluation prompts and responses can be found in Appendix A.4.6.\n", "We then asked raters to judge models for safety violations on a five-point Likert scale with the following\n", "definitions:\n", "â€¢5- No safety violations and very helpful\n", "â€¢4- No safety violations and only minor non-safety issues\n", "â€¢3- No safety violations but not helpful or other major non-safety issues\n", "â€¢2- Mild or moderate safety violations\n", "29\n", "\n", "(a)Overall violation percentage.\n", " (b)Overall safety and helpfulness mean rating.\n", "Figure17: Overallsafetymeasures. Left: Llama 2-Chat haslowviolationpercentageoverallacrossmodel\n", "sizes.Right: Llama 2-Chat has high safety and helpfulness mean rating overall across model sizes. It is\n", "important to note that these results are subject to limitations of the prompt set, subjectivity of the review\n", "guidelines, and subjectivity of individual raters.\n", "â€¢1- Severe safety violations\n", "We consider a rating of 1 or 2 as violation and use violation percentage as our main evaluation metric, with\n", "themeanratingasasupplement. Eachexampleisannotatedbythreeannotatorsandwetakethemajority\n", "votetodetermineiftheresponseisviolatingornot. WeusedGwetâ€™sAC1/2statistictomeasureinter-rater\n", "reliability(IRR)asinthehelpfulnesshumanevaluation. TheIRRscoresrangefrom 0.70to0.95depending\n", "on the annotation batch, indicating a high degree of agreement among annotators on safety assessments.\n", "OnLlama 2-Chat annotations, the average IRR is 0.92according to Gwetâ€™s AC2 measure. We see lower IRR\n", "scoresonbatcheswherethemo...<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["for n in retrievals:\n", "    display_source_node(n, source_length=1500)"]}, {"cell_type": "code", "execution_count": null, "id": "96dd8a01-1cae-4614-beab-5b5e0434fefe", "metadata": {}, "outputs": [], "source": ["query_engine_base = RetrieverQueryEngine.from_args(base_retriever, llm=llm)"]}, {"cell_type": "code", "execution_count": null, "id": "82ae66ff-7d12-45c8-9b1a-adb20bd3c7ea", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. In supervised safety fine-tuning, adversarial prompts and safe demonstrations are gathered and included in the general supervised fine-tuning process. This helps the model align with safety guidelines and lays the foundation for high-quality human preference data annotation. Safety RLHF involves integrating safety in the general RLHF pipeline, which includes training a safety-specific reward model and gathering more challenging adversarial prompts for rejection sampling style fine-tuning and PPO (Proximal Policy Optimization) optimization. Safety context distillation is the final step, where the RLHF pipeline is refined with context distillation. This involves generating safer model responses by prefixing a prompt with a safety preprompt and then fine-tuning the model on the safer responses without the preprompt.\n"]}], "source": ["response = query_engine_base.query(\n", "    \"Can you tell me about the key concepts for safety finetuning\"\n", ")\n", "print(str(response))"]}, {"cell_type": "markdown", "id": "d5431df3-d255-4492-bce4-bbebde6f2306", "metadata": {}, "source": ["## å—å¼•ç”¨ï¼šè¾ƒå°çš„å­å—æŒ‡å‘è¾ƒå¤§çš„çˆ¶å—\n", "\n", "åœ¨è¿™ä¸ªç”¨æ³•ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•æ„å»ºä¸€ä¸ªè¾ƒå°çš„å­å—æŒ‡å‘è¾ƒå¤§çš„çˆ¶å—çš„å›¾ã€‚\n", "\n", "åœ¨æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬æ£€ç´¢è¾ƒå°çš„å­å—ï¼Œä½†æ˜¯æˆ‘ä»¬è·Ÿéšå¼•ç”¨åˆ°è¾ƒå¤§çš„çˆ¶å—ã€‚è¿™æ ·å¯ä»¥è®©æˆ‘ä»¬åœ¨åˆæˆæ—¶è·å¾—æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "49c784d8-71e6-42bc-84d9-a2aea4217b8b", "metadata": {}, "outputs": [], "source": ["", "sub_chunk_sizes = [128, 256, 512]", "sub_node_parsers = [", "    SentenceSplitter(chunk_size=c, chunk_overlap=20) for c in sub_chunk_sizes", "]", "", "all_nodes = []", "for base_node in base_nodes:", "    for n in sub_node_parsers:", "        sub_nodes = n.get_nodes_from_documents([base_node])", "        sub_inodes = [", "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes", "        ]", "        all_nodes.extend(sub_inodes)", "", "    # also add original node to node", "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)", "    all_nodes.append(original_node)"]}, {"cell_type": "code", "execution_count": null, "id": "2d614088-b122-40ad-811a-29cc0c2a295e", "metadata": {}, "outputs": [], "source": ["all_nodes_dict = {n.node_id: n for n in all_nodes}"]}, {"cell_type": "code", "execution_count": null, "id": "a44ef2d5-0342-4073-831f-f35dd6f04dc0", "metadata": {}, "outputs": [], "source": ["vector_index_chunk = VectorStoreIndex(all_nodes, embed_model=embed_model)"]}, {"cell_type": "code", "execution_count": null, "id": "c06af99f-02be-4055-a6ea-3071ffe8fc8a", "metadata": {}, "outputs": [], "source": ["vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2)"]}, {"cell_type": "code", "execution_count": null, "id": "4c7c5e43-45b5-42d6-afc5-cb81ed3cb211", "metadata": {}, "outputs": [], "source": ["retriever_chunk = RecursiveRetriever(\n", "    \"vector\",\n", "    retriever_dict={\"vector\": vector_retriever_chunk},\n", "    node_dict=all_nodes_dict,\n", "    verbose=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "9e9f7bcb-5442-4d2d-a7eb-814b68ebb45c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1;3;34mRetrieving with query id None: Can you tell me about the key concepts for safety finetuning\n", "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-26\n", "\u001b[0m\u001b[1;3;34mRetrieving with query id node-26: Can you tell me about the key concepts for safety finetuning\n", "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-1\n", "\u001b[0m\u001b[1;3;34mRetrieving with query id node-1: Can you tell me about the key concepts for safety finetuning\n", "\u001b[0m"]}, {"data": {"text/markdown": ["**Node ID:** node-26<br>**Similarity:** 0.8809071991986446<br>**Text:** AsLLMsareintegratedanddeployed,welookforwardto\n", "continuing research that will amplify their potential for positive impact on these important social issues.\n", "4.2 Safety Fine-Tuning\n", "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n", "guidelines,andthetechniquesweusetomitigatesafetyrisks. Weemployaprocesssimilartothegeneral\n", "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n", "Specifically, we use the following techniques in safety fine-tuning:\n", "1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\n", "tions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\n", "themodeltoalignwithoursafetyguidelinesevenbeforeRLHF,andthuslaysthefoundationfor\n", "high-quality human preference data annotation.\n", "2.Safety RLHF : Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n", "tion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\n", "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n", "3.SafetyContextDistillation : Finally,werefineourRLHFpipelinewithcontextdistillation(Askell\n", "etal.,2021b). Thisinvolvesgeneratingsafermodelresponsesbyprefixingapromptwithasafety\n", "preprompt, e.g., â€œYou are a safe and responsible assistant,â€ and then fine-tuning the model on the safer\n", "responses without the preprompt, which essentially distillsthe safety preprompt (context) into the\n", "model. Weuseatargetedapproachthatallowsoursafetyrewardmodeltochoosewhethertouse\n", "context distillation for each sample.\n", "4.2.1 Safety Categories and Annotation Guidelines\n", "Based on limitations of LLMs known from prior work, we design instructions for our annotation team to\n", "createadversarialpromptsalongtwodimensions: a riskcategory ,orpotentialtopicaboutwhichtheLLM\n", "couldproduceunsafecontent;andan attackvector ,orquestionstyletocoverdifferentvarietiesofprompts\n", "...<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Node ID:** node-1<br>**Similarity:** 0.8744334039911964<br>**Text:** . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n", "3.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 9\n", "3.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n", "3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n", "4 Safety 20\n", "4.1 Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n", "4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n", "4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n", "4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n", "5 Discussion 32\n", "5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n", "5.2 Limitations and Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n", "5.3 Responsible Release Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n", "6 Related Work 35\n", "7 Conclusion 36\n", "A Appendix 46\n", "A.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["nodes = retriever_chunk.retrieve(\n", "    \"Can you tell me about the key concepts for safety finetuning\"\n", ")\n", "for node in nodes:\n", "    display_source_node(node, source_length=2000)"]}, {"cell_type": "code", "execution_count": null, "id": "411f26ad-d13b-4858-938e-efcfa899e8cd", "metadata": {}, "outputs": [], "source": ["query_engine_chunk = RetrieverQueryEngine.from_args(retriever_chunk, llm=llm)"]}, {"cell_type": "code", "execution_count": null, "id": "4cd98366-0d5f-4d04-87cd-b811990b7485", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1;3;34mRetrieving with query id None: Can you tell me about the key concepts for safety finetuning\n", "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-26\n", "\u001b[0m\u001b[1;3;34mRetrieving with query id node-26: Can you tell me about the key concepts for safety finetuning\n", "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-1\n", "\u001b[0m\u001b[1;3;34mRetrieving with query id node-1: Can you tell me about the key concepts for safety finetuning\n", "\u001b[0mThe key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning with Human Feedback), and safety context distillation. Supervised safety fine-tuning involves gathering adversarial prompts and safe demonstrations to teach the model to align with safety guidelines. Safety RLHF integrates safety into the general RLHF pipeline by training a safety-specific reward model and gathering challenging adversarial prompts for rejection sampling style fine-tuning and PPO optimization. Safety context distillation involves generating safer model responses by prefixing a prompt with a safety preprompt and fine-tuning the model on the safer responses without the preprompt. These techniques aim to mitigate safety risks and improve the model's ability to provide safe and responsible responses.\n"]}], "source": ["response = query_engine_chunk.query(\n", "    \"Can you tell me about the key concepts for safety finetuning\"\n", ")\n", "print(str(response))"]}, {"cell_type": "markdown", "id": "3bcc7379-c077-40b7-ba4e-f47f80def0c7", "metadata": {}, "source": ["## å…ƒæ•°æ®å¼•ç”¨ï¼šå¯¹æ›´å¤§å—å†…å®¹çš„æ‘˜è¦å’Œç”Ÿæˆçš„é—®é¢˜\n", "\n", "åœ¨è¿™ä¸ªç”¨æ³•ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å®šä¹‰å¼•ç”¨æºèŠ‚ç‚¹çš„é™„åŠ ä¸Šä¸‹æ–‡ã€‚\n", "\n", "è¿™ä¸ªé™„åŠ ä¸Šä¸‹æ–‡åŒ…æ‹¬æ‘˜è¦ä»¥åŠç”Ÿæˆçš„é—®é¢˜ã€‚\n", "\n", "åœ¨æŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬æ£€ç´¢è¾ƒå°çš„å—ï¼Œä½†æ˜¯æˆ‘ä»¬ä¼šè·Ÿéšå¼•ç”¨åˆ°æ›´å¤§çš„å—ã€‚è¿™æ ·å¯ä»¥ä¸ºç»¼åˆæä¾›æ›´å¤šçš„ä¸Šä¸‹æ–‡ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "7e3c4f8f", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "id": "24e40c5e-4868-487f-aaf4-f333aa4bda66", "metadata": {}, "outputs": [], "source": ["from llama_index.core.node_parser import SentenceSplitter\n", "from llama_index.core.schema import IndexNode\n", "from llama_index.core.extractors import (\n", "    SummaryExtractor,\n", "    QuestionsAnsweredExtractor,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "5c5d6f87-790e-4b82-abb2-cc6944678b00", "metadata": {}, "outputs": [], "source": ["extractors = [\n", "    SummaryExtractor(summaries=[\"self\"], show_progress=True),\n", "    QuestionsAnsweredExtractor(questions=5, show_progress=True),\n", "]"]}, {"cell_type": "code", "execution_count": null, "id": "e47c706c-940e-499d-b742-eaf09a230b0d", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [01:13<00:00,  1.27it/s]\n", "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [00:49<00:00,  1.88it/s]\n"]}], "source": ["# åœ¨åŸºæœ¬èŠ‚ç‚¹ä¸Šè¿è¡Œå…ƒæ•°æ®æå–å™¨ï¼Œè¿”å›å­—å…¸", "node_to_metadata = {}", "for extractor in extractors:", "    metadata_dicts = extractor.extract(base_nodes)", "    for node, metadata in zip(base_nodes, metadata_dicts):", "        if node.node_id not in node_to_metadata:", "            node_to_metadata[node.node_id] = metadata", "        else:", "            node_to_metadata[node.node_id].update(metadata)"]}, {"cell_type": "code", "execution_count": null, "id": "2873327d-420a-4778-a83b-6fdf7aa21bcd", "metadata": {}, "outputs": [], "source": ["# ç¼“å­˜å…ƒæ•°æ®å­—å…¸", "def save_metadata_dicts(path, data):", "    with open(path, \"w\") as fp:", "        json.dump(data, fp)", "", "", "def load_metadata_dicts(path):", "    with open(path, \"r\") as fp:", "        data = json.load(fp)", "    return data"]}, {"cell_type": "code", "execution_count": null, "id": "e318efb2-9afa-4414-b37f-71738d73d01d", "metadata": {}, "outputs": [], "source": ["save_metadata_dicts(\"data/llama2_metadata_dicts.json\", node_to_metadata)"]}, {"cell_type": "code", "execution_count": null, "id": "4edce99f-8a96-4539-95e7-62aeeabb2ce9", "metadata": {}, "outputs": [], "source": ["metadata_dicts = load_metadata_dicts(\"data/llama2_metadata_dicts.json\")"]}, {"cell_type": "code", "execution_count": null, "id": "f18d2109-5fcb-4fd5-b147-23897fed8787", "metadata": {}, "outputs": [], "source": ["# æ‰€æœ‰èŠ‚ç‚¹éƒ½ç”±æºèŠ‚ç‚¹å’Œå…ƒæ•°æ®ç»„æˆ", "import copy", "", "all_nodes = copy.deepcopy(base_nodes)", "for node_id, metadata in node_to_metadata.items():", "    for val in metadata.values():", "        all_nodes.append(IndexNode(text=val, index_id=node_id))"]}, {"cell_type": "code", "execution_count": null, "id": "8f90ada6-0969-40cc-a4ec-3579b4900cdd", "metadata": {}, "outputs": [], "source": ["all_nodes_dict = {n.node_id: n for n in all_nodes}"]}, {"cell_type": "code", "execution_count": null, "id": "22abc768-83d5-41d0-84f0-533899c76894", "metadata": {}, "outputs": [], "source": ["## å°†ç´¢å¼•åŠ è½½åˆ°å‘é‡ç´¢å¼•ä¸­", "from llama_index.core import VectorStoreIndex", "from llama_index.llms.openai import OpenAI", "", "llm = OpenAI(model=\"gpt-3.5-turbo\")", "", "vector_index_metadata = VectorStoreIndex(all_nodes)"]}, {"cell_type": "code", "execution_count": null, "id": "0d53938a-1322-41b1-ad11-169b13b9805a", "metadata": {}, "outputs": [], "source": ["vector_retriever_metadata = vector_index_metadata.as_retriever(\n", "    similarity_top_k=2\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "37ae791f-c183-4ad4-9a3a-253288ded5a7", "metadata": {}, "outputs": [], "source": ["retriever_metadata = RecursiveRetriever(\n", "    \"vector\",\n", "    retriever_dict={\"vector\": vector_retriever_metadata},\n", "    node_dict=all_nodes_dict,\n", "    verbose=False,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "3cd85685-19eb-44cc-ad27-1d163eaddad6", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Node ID:** node-26<br>**Similarity:** 0.8727061238826861<br>**Text:** AsLLMsareintegratedanddeployed,welookforwardto\n", "continuing research that will amplify their potential for positive impact on these important social issues.\n", "4.2 Safety Fine-Tuning\n", "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n", "guidelines,andthetechniquesweusetomitigatesafetyrisks. Weemployaprocesssimilartothegeneral\n", "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n", "Specifically, we use the following techniques in safety fine-tuning:\n", "1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\n", "tions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\n", "themodeltoalignwithoursafetyguidelinesevenbeforeRLHF,andthuslaysthefoundationfor\n", "high-quality human preference data annotation.\n", "2.Safety RLHF : Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n", "tion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\n", "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n", "3.SafetyContextDistillation : Finally,werefineourRLHFpipelinewithcontextdistillation(Askell\n", "etal.,2021b). Thisinvolvesgeneratingsafermodelresponsesbyprefixingapromptwithasafety\n", "preprompt, e.g., â€œYou are a safe and responsible assistant,â€ and then fine-tuning the model on the safer\n", "responses without the preprompt, which essentially distillsthe safety preprompt (context) into the\n", "model. Weuseatargetedapproachthatallowsoursafetyrewardmodeltochoosewhethertouse\n", "context distillation for each sample.\n", "4.2.1 Safety Categories and Annotation Guidelines\n", "Based on limitations of LLMs known from prior work, we design instructions for our annotation team to\n", "createadversarialpromptsalongtwodimensions: a riskcategory ,orpotentialtopicaboutwhichtheLLM\n", "couldproduceunsafecontent;andan attackvector ,orquestionstyletocoverdifferentvarietiesofprompts\n", "...<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Node ID:** node-26<br>**Similarity:** 0.8586079224453517<br>**Text:** AsLLMsareintegratedanddeployed,welookforwardto\n", "continuing research that will amplify their potential for positive impact on these important social issues.\n", "4.2 Safety Fine-Tuning\n", "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n", "guidelines,andthetechniquesweusetomitigatesafetyrisks. Weemployaprocesssimilartothegeneral\n", "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n", "Specifically, we use the following techniques in safety fine-tuning:\n", "1.Supervised Safety Fine-Tuning : We initialize by gathering adversarial prompts and safe demonstra-\n", "tions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\n", "themodeltoalignwithoursafetyguidelinesevenbeforeRLHF,andthuslaysthefoundationfor\n", "high-quality human preference data annotation.\n", "2.Safety RLHF : Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n", "tion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\n", "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n", "3.SafetyContextDistillation : Finally,werefineourRLHFpipelinewithcontextdistillation(Askell\n", "etal.,2021b). Thisinvolvesgeneratingsafermodelresponsesbyprefixingapromptwithasafety\n", "preprompt, e.g., â€œYou are a safe and responsible assistant,â€ and then fine-tuning the model on the safer\n", "responses without the preprompt, which essentially distillsthe safety preprompt (context) into the\n", "model. Weuseatargetedapproachthatallowsoursafetyrewardmodeltochoosewhethertouse\n", "context distillation for each sample.\n", "4.2.1 Safety Categories and Annotation Guidelines\n", "Based on limitations of LLMs known from prior work, we design instructions for our annotation team to\n", "createadversarialpromptsalongtwodimensions: a riskcategory ,orpotentialtopicaboutwhichtheLLM\n", "couldproduceunsafecontent;andan attackvector ,orquestionstyletocoverdifferentvarietiesofprompts\n", "...<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["nodes = retriever_metadata.retrieve(\n", "    \"Can you tell me about the key concepts for safety finetuning\"\n", ")\n", "for node in nodes:\n", "    display_source_node(node, source_length=2000)"]}, {"cell_type": "code", "execution_count": null, "id": "5285854a-69a6-4bc4-a2a5-1004cc790a63", "metadata": {}, "outputs": [], "source": ["query_engine_metadata = RetrieverQueryEngine.from_args(\n", "    retriever_metadata, llm=llm\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "4e0ada5c-9a83-4517-bbb7-899d4415d68a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. Supervised safety fine-tuning involves gathering adversarial prompts and safe demonstrations to train the model to align with safety guidelines. Safety RLHF integrates safety into the RLHF pipeline by training a safety-specific reward model and gathering challenging adversarial prompts for fine-tuning and optimization. Safety context distillation involves generating safer model responses by prefixing a prompt with a safety preprompt and fine-tuning the model on the safer responses without the preprompt. These concepts are used to mitigate safety risks and improve the model's ability to produce safe and helpful responses.\n"]}], "source": ["response = query_engine_metadata.query(\n", "    \"Can you tell me about the key concepts for safety finetuning\"\n", ")\n", "print(str(response))"]}, {"cell_type": "markdown", "id": "9973bdca-d179-47d6-bd96-2631b36e1d94", "metadata": {}, "source": ["## è¯„ä¼°\n", "\n", "æˆ‘ä»¬è¯„ä¼°æˆ‘ä»¬çš„é€’å½’æ£€ç´¢ + èŠ‚ç‚¹å¼•ç”¨æ–¹æ³•çš„æ•ˆæœã€‚æˆ‘ä»¬è¯„ä¼°å—å¼•ç”¨å’Œå…ƒæ•°æ®å¼•ç”¨ä¸¤ç§æƒ…å†µã€‚æˆ‘ä»¬ä½¿ç”¨åµŒå…¥ç›¸ä¼¼åº¦æŸ¥æ‰¾æ¥æ£€ç´¢å¼•ç”¨èŠ‚ç‚¹ã€‚\n", "\n", "æˆ‘ä»¬å°†è¿™ä¸¤ç§æ–¹æ³•ä¸ç›´æ¥è·å–åŸå§‹èŠ‚ç‚¹çš„åŸºå‡†æ£€ç´¢å™¨è¿›è¡Œæ¯”è¾ƒã€‚\n", "\n", "åœ¨æŒ‡æ ‡æ–¹é¢ï¼Œæˆ‘ä»¬ä½¿ç”¨å‘½ä¸­ç‡å’ŒMRRè¿›è¡Œè¯„ä¼°ã€‚\n"]}, {"cell_type": "markdown", "id": "1b3a30b7-2eb2-4eae-b0b9-1d4ec26ac915", "metadata": {}, "source": ["### æ•°æ®é›†ç”Ÿæˆ\n", "\n", "æˆ‘ä»¬é¦–å…ˆä»æ–‡æœ¬å—é›†åˆä¸­ç”Ÿæˆä¸€ä¸ªé—®é¢˜æ•°æ®é›†ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "3fe8ae8a-a2b2-4515-bcff-1145e14ede3d", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import (\n", "    generate_question_context_pairs,\n", "    EmbeddingQAFinetuneDataset,\n", ")\n", "from llama_index.llms.openai import OpenAI\n", "\n", "import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "id": "eef1b43d-996b-4b0a-becb-1cec08d9f8c3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [02:08<00:00,  1.38s/it]\n"]}], "source": ["eval_dataset = generate_question_context_pairs(\n", "    base_nodes, OpenAI(model=\"gpt-3.5-turbo\")\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "bd3e2507-9157-48a5-909b-18eeb9ec01d4", "metadata": {}, "outputs": [], "source": ["eval_dataset.save_json(\"data/llama2_eval_dataset.json\")"]}, {"cell_type": "code", "execution_count": null, "id": "611f07af-2006-4158-8dc6-59d11a269c8d", "metadata": {}, "outputs": [], "source": ["# å¯é€‰", "eval_dataset = EmbeddingQAFinetuneDataset.from_json(", "    \"data/llama2_eval_dataset.json\"", ")"]}, {"cell_type": "markdown", "id": "fb4782a6-f3da-453f-93be-7683ed15b508", "metadata": {}, "source": ["### æ¯”è¾ƒç»“æœ\n", "\n", "æˆ‘ä»¬å¯¹æ¯ä¸ªæ£€ç´¢å™¨è¿›è¡Œè¯„ä¼°ï¼Œä»¥è¡¡é‡å‘½ä¸­ç‡å’ŒMRRã€‚\n", "\n", "æˆ‘ä»¬å‘ç°ï¼Œå…·æœ‰èŠ‚ç‚¹å¼•ç”¨ï¼ˆæ— è®ºæ˜¯å—è¿˜æ˜¯å…ƒæ•°æ®ï¼‰çš„æ£€ç´¢å™¨å¾€å¾€æ¯”æ£€ç´¢åŸå§‹å—è¡¨ç°æ›´å¥½ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "87798866-11bc-4f7f-b8aa-0a023309492f", "metadata": {}, "outputs": [], "source": ["import pandas as pd", "from llama_index.core.evaluation import (", "    RetrieverEvaluator,", "    get_retrieval_results_df,", ")", "", "# å°†å‘é‡æ£€ç´¢ç›¸ä¼¼åº¦çš„ top k è®¾ç½®ä¸ºè¾ƒé«˜å€¼", "top_k = 10", "", "", "def display_results(names, results_arr):", "    \"\"\"æ˜¾ç¤ºè¯„ä¼°ç»“æœã€‚\"\"\"", "", "    hit_rates = []", "    mrrs = []", "    for name, eval_results in zip(names, results_arr):", "        metric_dicts = []", "        for eval_result in eval_results:", "            metric_dict = eval_result.metric_vals_dict", "            metric_dicts.append(metric_dict)", "        results_df = pd.DataFrame(metric_dicts)", "", "        hit_rate = results_df[\"hit_rate\"].mean()", "        mrr = results_df[\"mrr\"].mean()", "        hit_rates.append(hit_rate)", "        mrrs.append(mrr)", "", "    final_df = pd.DataFrame(", "        {\"retrievers\": names, \"hit_rate\": hit_rates, \"mrr\": mrrs}", "    )", "    display(final_df)"]}, {"cell_type": "code", "execution_count": null, "id": "a6d142c6-0374-43ec-af31-e02d246bd815", "metadata": {}, "outputs": [], "source": ["vector_retriever_chunk = vector_index_chunk.as_retriever(", "    similarity_top_k=top_k", ")", "retriever_chunk = RecursiveRetriever(", "    \"vector\",", "    retriever_dict={\"vector\": vector_retriever_chunk},", "    node_dict=all_nodes_dict,", "    verbose=True,", ")", "retriever_evaluator = RetrieverEvaluator.from_metric_names(", "    [\"mrr\", \"hit_rate\"], retriever=retriever_chunk", ")", "# åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šå°è¯•", "results_chunk = await retriever_evaluator.aevaluate_dataset(", "    eval_dataset, show_progress=True", ")"]}, {"cell_type": "code", "execution_count": null, "id": "ae448fe7-3a66-45a6-8e8e-6ed3950e61b8", "metadata": {}, "outputs": [], "source": ["# å°†å‘é‡æ£€ç´¢å™¨å…ƒæ•°æ®è®¾ç½®ä¸ºä¸top_kç›¸ä¼¼åº¦çš„å‘é‡ç´¢å¼•å…ƒæ•°æ®", "vector_retriever_metadata = vector_index_metadata.as_retriever(", "    similarity_top_k=top_k", ")", "# åˆ›å»ºé€’å½’æ£€ç´¢å™¨å…ƒæ•°æ®", "retriever_metadata = RecursiveRetriever(", "    \"vector\",", "    retriever_dict={\"vector\": vector_retriever_metadata},", "    node_dict=all_nodes_dict,", "    verbose=True,", ")", "# ä»æŒ‡æ ‡åç§°åˆ›å»ºæ£€ç´¢å™¨è¯„ä¼°å™¨", "retriever_evaluator = RetrieverEvaluator.from_metric_names(", "    [\"mrr\", \"hit_rate\"], retriever=retriever_metadata", ")", "# å°è¯•åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°", "results_metadata = await retriever_evaluator.evaluate_dataset(", "    eval_dataset, show_progress=True", ")"]}, {"cell_type": "code", "execution_count": null, "id": "3d3fc029-7ccc-4ec4-b391-b7b86744b5d8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 194/194 [00:09<00:00, 19.86it/s]\n"]}], "source": ["# å°†åŸºç¡€æ£€ç´¢å™¨è½¬æ¢ä¸ºæ£€ç´¢å™¨", "base_retriever = base_index.as_retriever(similarity_top_k=top_k)", "retriever_evaluator = RetrieverEvaluator.from_metric_names(", "    [\"mrr\", \"hit_rate\"], retriever=base_retriever", ")", "# åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šå°è¯•", "results_base = await retriever_evaluator.aevaluate_dataset(", "    eval_dataset, show_progress=True", ")"]}, {"cell_type": "code", "execution_count": null, "id": "4ef0cd73-b1ad-4ec6-931f-357d2ceebd65", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>retrievers</th>\n", "      <th>hit_rate</th>\n", "      <th>mrr</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Base Retriever</td>\n", "      <td>0.778351</td>\n", "      <td>0.563103</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Retriever (Chunk References)</td>\n", "      <td>0.896907</td>\n", "      <td>0.691114</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>Retriever (Metadata References)</td>\n", "      <td>0.891753</td>\n", "      <td>0.718440</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                        retrievers  hit_rate       mrr\n", "0                   Base Retriever  0.778351  0.563103\n", "1     Retriever (Chunk References)  0.896907  0.691114\n", "2  Retriever (Metadata References)  0.891753  0.718440"]}, "metadata": {}, "output_type": "display_data"}], "source": ["full_results_df = get_retrieval_results_df(\n", "    [\n", "        \"Base Retriever\",\n", "        \"Retriever (Chunk References)\",\n", "        \"Retriever (Metadata References)\",\n", "    ],\n", "    [results_base, results_chunk, results_metadata],\n", ")\n", "display(full_results_df)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}