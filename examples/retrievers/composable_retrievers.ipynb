{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 可组合对象\n", "\n", "在这个笔记本中，我们将展示如何将多个对象组合成一个顶层索引。\n", "\n", "这种方法是通过设置`IndexNode`对象来实现的，其中包含一个指向以下内容的`obj`字段：\n", "- 查询引擎\n", "- 检索器\n", "- 查询管道\n", "- 另一个节点！\n", "\n", "```python\n", "object = IndexNode(index_id=\"my_object\", obj=query_engine, text=\"关于这个对象的一些文本\")\n", "```\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 数据设置\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-storage-docstore-mongodb\n", "%pip install llama-index-vector-stores-qdrant\n", "%pip install llama-index-storage-docstore-firestore\n", "%pip install llama-index-retrievers-bm25\n", "%pip install llama-index-storage-docstore-redis\n", "%pip install llama-index-storage-docstore-dynamodb\n", "%pip install llama-index-readers-file pymupdf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"./llama2.pdf\"\n", "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/1706.03762.pdf\" -O \"./attention.pdf\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import download_loader\n", "\n", "from llama_index.readers.file import PyMuPDFReader\n", "\n", "llama2_docs = PyMuPDFReader().load_data(\n", "    file_path=\"./llama2.pdf\", metadata=True\n", ")\n", "attention_docs = PyMuPDFReader().load_data(\n", "    file_path=\"./attention.pdf\", metadata=True\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 检索器设置\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.node_parser import TokenTextSplitter\n", "\n", "nodes = TokenTextSplitter(\n", "    chunk_size=1024, chunk_overlap=128\n", ").get_nodes_from_documents(llama2_docs + attention_docs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.storage.docstore import SimpleDocumentStore\n", "from llama_index.storage.docstore.redis import RedisDocumentStore\n", "from llama_index.storage.docstore.mongodb import MongoDocumentStore\n", "from llama_index.storage.docstore.firestore import FirestoreDocumentStore\n", "from llama_index.storage.docstore.dynamodb import DynamoDBDocumentStore\n", "\n", "docstore = SimpleDocumentStore()\n", "docstore.add_documents(nodes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, StorageContext\n", "from llama_index.retrievers.bm25 import BM25Retriever\n", "from llama_index.vector_stores.qdrant import QdrantVectorStore\n", "from qdrant_client import QdrantClient\n", "\n", "client = QdrantClient(path=\"./qdrant_data\")\n", "vector_store = QdrantVectorStore(\"composable\", client=client)\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n", "\n", "index = VectorStoreIndex(nodes=nodes)\n", "vector_retriever = index.as_retriever(similarity_top_k=2)\n", "bm25_retriever = BM25Retriever.from_defaults(\n", "    docstore=docstore, similarity_top_k=2\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 组合对象\n", "\n", "在这里，我们构建`IndexNodes`。请注意，文本是顶层索引用于索引节点的内容。\n", "\n", "对于向量索引，文本被嵌入其中，对于关键字索引，文本用于关键字。\n", "\n", "在这个例子中，使用了`SummaryIndex`，它在检索时实际上不需要文本，因为它总是检索所有节点。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.schema import IndexNode\n", "\n", "vector_obj = IndexNode(\n", "    index_id=\"vector\", obj=vector_retriever, text=\"Vector Retriever\"\n", ")\n", "bm25_obj = IndexNode(\n", "    index_id=\"bm25\", obj=bm25_retriever, text=\"BM25 Retriever\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SummaryIndex\n", "\n", "summary_index = SummaryIndex(objects=[vector_obj, bm25_obj])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 查询\n", "\n", "当我们进行查询时，将检索所有对象并用于生成节点以获得最终答案。\n", "\n", "使用`tree_summarize`和`aquery()`可以确保并发执行和更快的响应。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = summary_index.as_query_engine(\n", "    response_mode=\"tree_summarize\", verbose=True\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1;3;38;2;11;159;203mRetrieval entering vector: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering bm25: BM25Retriever\n", "\u001b[0m"]}], "source": ["response = await query_engine.aquery(\n", "    \"How does attention work in transformers?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Attention in transformers works by mapping a query and a set of key-value pairs to an output. The output is computed as a weighted sum of the values, where the weights are determined by the similarity between the query and the keys. In the transformer model, attention is used in three different ways: \n", "\n", "1. Encoder-decoder attention: The queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\n", "\n", "2. Self-attention in the encoder: In a self-attention layer, all of the keys, values, and queries come from the same place, which is the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n", "\n", "3. Self-attention in the decoder: Similar to the encoder, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. However, leftward information flow in the decoder is prevented to preserve the auto-regressive property.\n", "\n", "Overall, attention in transformers allows the model to jointly attend to information from different representation subspaces at different positions, improving the model's ability to capture dependencies and relationships between different parts of the input sequence.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1;3;38;2;11;159;203mRetrieval entering vector: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering bm25: BM25Retriever\n", "\u001b[0m"]}], "source": ["response = await query_engine.aquery(\n", "    \"What is the architecture of Llama2 based on?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The architecture of Llama 2 is based on the transformer model.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1;3;38;2;11;159;203mRetrieval entering vector: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering bm25: BM25Retriever\n", "\u001b[0m"]}], "source": ["response = await query_engine.aquery(\n", "    \"What was used before attention in transformers?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Recurrent neural networks, such as long short-term memory (LSTM) and gated recurrent neural networks, were commonly used before attention in transformers. These models were widely used in sequence modeling and transduction problems, including language modeling and machine translation.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 保存和加载注意事项\n", "\n", "由于对象在技术上不可序列化，因此在保存和加载时，需要在加载时提供它们。\n", "\n", "以下是我可能如何保存/加载这个设置的示例。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 保存\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# qdrant已经自动保存了！", "# 我们只需要在这里保存docstore", "", "# 保存我们的docstore节点用于bm25", "docstore.persist(\"./docstore.json\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 加载\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.storage.docstore import SimpleDocumentStore\n", "from llama_index.vector_stores.qdrant import QdrantVectorStore\n", "from qdrant_client import QdrantClient\n", "\n", "docstore = SimpleDocumentStore.from_persist_path(\"./docstore.json\")\n", "\n", "client = QdrantClient(path=\"./qdrant_data\")\n", "vector_store = QdrantVectorStore(\"composable\", client=client)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["index = VectorStoreIndex.from_vector_store(vector_store)\n", "vector_retriever = index.as_retriever(similarity_top_k=2)\n", "bm25_retriever = BM25Retriever.from_defaults(\n", "    docstore=docstore, similarity_top_k=2\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.schema import IndexNode\n", "\n", "vector_obj = IndexNode(\n", "    index_id=\"vector\", obj=vector_retriever, text=\"Vector Retriever\"\n", ")\n", "bm25_obj = IndexNode(\n", "    index_id=\"bm25\", obj=bm25_retriever, text=\"BM25 Retriever\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 如果我们将常规节点添加到摘要索引中，我们也可以保存/加载它", "# summary_index.persist(\"./summary_index.json\")", "# summary_index = load_index_from_storage(storage_context, objects=objects)", "", "from llama_index.core import SummaryIndex", "", "summary_index = SummaryIndex(objects=[vector_obj, bm25_obj])"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}