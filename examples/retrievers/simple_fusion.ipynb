{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 简单融合检索器\n", "\n", "在这个示例中，我们将介绍如何将多个查询和多个索引的检索结果进行合并。\n", "\n", "检索到的节点将作为所有查询和索引中的前k个返回，同时处理任何节点的去重。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import openai\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 设置\n", "\n", "在本笔记本中，我们将使用我们文档中的两个非常相似的页面，每个页面都存储在单独的索引中。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "documents_1 = SimpleDirectoryReader(\n", "    input_files=[\"../../community/integrations/vector_stores.md\"]\n", ").load_data()\n", "documents_2 = SimpleDirectoryReader(\n", "    input_files=[\"../../module_guides/storing/vector_stores.md\"]\n", ").load_data()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "\n", "index_1 = VectorStoreIndex.from_documents(documents_1)\n", "index_2 = VectorStoreIndex.from_documents(documents_2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 融合索引！\n", "\n", "在这一步中，我们将我们的索引融合成一个单一的检索器。这个检索器还会通过生成与原始问题相关的额外查询来增强我们的查询，并汇总结果。\n", "\n", "这个设置将查询4次，一次使用您的原始查询，并生成3个额外的查询。\n", "\n", "默认情况下，它使用以下提示来生成额外的查询：\n", "\n", "```python\n", "QUERY_GEN_PROMPT = (\n", "    \"您是一个乐于助人的助手，根据单个输入查询生成多个搜索查询。生成{num_queries}个搜索查询，每行一个，与以下输入查询相关：\\n\"\n", "    \"查询：{query}\\n\"\n", "    \"查询：\\n\"\n", ")\n", "```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "from llama_index.core.retrievers import QueryFusionRetriever", "", "retriever = QueryFusionRetriever(", "    [index_1.as_retriever(), index_2.as_retriever()],", "    similarity_top_k=2,", "    num_queries=4,  # 将其设置为1以禁用查询生成", "    use_async=True,", "    verbose=True,", "    # query_gen_prompt=\"...\",  # 在这里可以覆盖查询生成提示", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 将嵌套的异步应用于在笔记本中运行", "import nest_asyncio", "", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Generated queries:\n", "1. What are the steps to set up a chroma vector store?\n", "2. Best practices for configuring a chroma vector store\n", "3. Troubleshooting common issues when setting up a chroma vector store\n"]}], "source": ["nodes_with_scores = retriever.retrieve(\"How do I setup a chroma vector store?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Score: 0.78 - # Vector Stores\n", "\n", "Vector stores contain embedding vectors of ingested document chunks\n", "(and sometimes ...\n", "Score: 0.78 - # Using Vector Stores\n", "\n", "LlamaIndex offers multiple integration points with vector stores / vector dat...\n"]}], "source": ["for node in nodes_with_scores:\n", "    print(f\"Score: {node.score:.2f} - {node.text[:100]}...\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 在查询引擎中使用！\n", "\n", "现在，我们可以将我们的检索器插入到查询引擎中，以合成自然语言响应。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_engine import RetrieverQueryEngine\n", "\n", "query_engine = RetrieverQueryEngine.from_args(retriever)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Generated queries:\n", "1. How to set up a chroma vector store?\n", "2. Step-by-step guide for creating a chroma vector store.\n", "3. Examples of chroma vector store setups and configurations.\n"]}], "source": ["response = query_engine.query(\n", "    \"How do I setup a chroma vector store? Can you give an example?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/markdown": ["**`Final Response:`** To set up a Chroma vector store, you need to follow these steps:\n", "\n", "1. Import the necessary libraries:\n", "```python\n", "import chromadb\n", "from llama_index.vector_stores.chroma import ChromaVectorStore\n", "```\n", "\n", "2. Create a Chroma client:\n", "```python\n", "chroma_client = chromadb.EphemeralClient()\n", "chroma_collection = chroma_client.create_collection(\"quickstart\")\n", "```\n", "\n", "3. Construct the vector store:\n", "```python\n", "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n", "```\n", "\n", "Here's an example of how to set up a Chroma vector store using the above steps:\n", "\n", "```python\n", "import chromadb\n", "from llama_index.vector_stores.chroma import ChromaVectorStore\n", "\n", "# Creating a Chroma client\n", "# EphemeralClient operates purely in-memory, PersistentClient will also save to disk\n", "chroma_client = chromadb.EphemeralClient()\n", "chroma_collection = chroma_client.create_collection(\"quickstart\")\n", "\n", "# construct vector store\n", "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n", "```\n", "\n", "This example demonstrates how to create a Chroma client, create a collection named \"quickstart\", and then construct a Chroma vector store using that collection."], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from llama_index.core.response.notebook_utils import display_response\n", "\n", "display_response(response)"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v3", "language": "python", "name": "llama_index_v3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}