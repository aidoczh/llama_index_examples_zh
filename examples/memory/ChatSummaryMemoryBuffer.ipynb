{"cells": [{"cell_type": "markdown", "id": "c62c1447-0afb-4fad-8dc6-389949c3496e", "metadata": {}, "source": ["# 聊天摘要内存缓冲区\n", "在这个演示中，我们使用新的*ChatSummaryMemoryBuffer*来限制聊天历史记录的令牌长度，并迭代地总结所有不适合在内存缓冲区中的消息。如果您希望限制成本和延迟（假设摘要提示使用和生成的令牌少于包括整个历史记录），这可能会很有用。\n", "\n", "原始的*ChatMemoryBuffer*允许您在一定数量的令牌后截断历史记录，这对于限制成本和延迟很有用，但也会从聊天历史记录中删除潜在相关的信息。\n", "\n", "更新的*ChatSummaryMemoryBuffer*旨在使其更加灵活，以便用户对保留哪些聊天历史记录具有更多控制。\n"]}, {"cell_type": "code", "execution_count": null, "id": "c00a753b-df2c-4164-90c3-76b8a15f74c9", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai\n", "%pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "c1f24186-b86e-4580-b7b4-072e719d424f", "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\""]}, {"cell_type": "code", "execution_count": null, "id": "db66e3bc-9791-497b-9e7d-386765dccf74", "metadata": {}, "outputs": [], "source": ["from llama_index.core.memory import ChatSummaryMemoryBuffer\n", "from llama_index.core.llms import ChatMessage, MessageRole\n", "from llama_index.llms.openai import OpenAI as OpenAiLlm\n", "import tiktoken"]}, {"cell_type": "markdown", "id": "68e26f76-f819-4e1a-bc47-f2ea855ee189", "metadata": {}, "source": ["首先，我们模拟一些聊天记录，这些记录无法完全放入内存缓冲区中。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6402621a-4131-465c-b92d-1d9a8e7ee985", "metadata": {}, "outputs": [], "source": ["chat_history = [", "    ChatMessage(role=\"user\", content=\"什么是LlamaIndex?\"),", "    ChatMessage(", "        role=\"assistant\",", "        content=\"LlamaIndex是用于构建LLM应用程序的主要数据框架\",", "    ),", "    ChatMessage(role=\"user\", content=\"你能给我一些更多的细节吗?\"),", "    ChatMessage(", "        role=\"assistant\",", "        content=\"\"\"LlamaIndex是用于构建上下文增强的LLM应用程序的框架。上下文增强是指在您的私人或领域特定数据之上应用LLM的任何用例。一些常见的用例包括以下内容：", "        问答聊天机器人（通常称为RAG系统，代表“检索增强生成”），文档理解和提取，能够进行研究并采取行动的自主代理", "        LlamaIndex提供了从原型到生产构建任何上述用例的工具。这些工具允许您同时摄取/处理这些数据并实现将数据访问与LLM提示相结合的复杂查询工作流。\"\"\",", "    ),", "]"]}, {"cell_type": "markdown", "id": "f057a791-9d8e-43e5-b40a-6675b28f6fd0", "metadata": {}, "source": ["我们设置了summarizer_llm，并创建了一个*ChatSummaryMemoryBuffer*实例。\n"]}, {"cell_type": "code", "execution_count": null, "id": "67dbb654-4a66-43cf-9f6a-daead87a1084", "metadata": {}, "outputs": [], "source": ["model = \"gpt-4-0125-preview\"\n", "summarizer_llm = OpenAiLlm(model_name=model, max_tokens=256)\n", "tokenizer_fn = tiktoken.encoding_for_model(model).encode\n", "memory = ChatSummaryMemoryBuffer.from_defaults(\n", "    chat_history=chat_history,\n", "    llm=summarizer_llm,\n", "    token_limit=2,\n", "    tokenizer_fn=tokenizer_fn,\n", ")\n", "\n", "history = memory.get()"]}, {"cell_type": "markdown", "id": "4e4e333c-6c33-4e01-b1a8-750d21800076", "metadata": {}, "source": ["在打印历史记录时，我们可以观察到较早的消息已经被总结了。\n"]}, {"cell_type": "code", "execution_count": null, "id": "0821eb89-4164-4a06-b66c-ea2632706e11", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='The user inquired about LlamaIndex, a leading data framework for developing LLM applications. The assistant explained that LlamaIndex is used for building context-augmented LLM applications, giving examples such as Question-Answering Chatbots, Document Understanding and Extraction, and Autonomous Agents. It was mentioned that LlamaIndex provides tools for ingesting and processing data, as well as implementing complex query workflows combining data access with LLM prompting.', additional_kwargs={})]\n"]}], "source": ["print(history)"]}, {"cell_type": "markdown", "id": "fae3efe0-6889-49f7-9f21-6ced186f0609", "metadata": {}, "source": ["让我们添加一些新的聊天记录。\n"]}, {"cell_type": "code", "execution_count": null, "id": "7ddb295c-c5c0-4faf-b0e5-a451d1d26d60", "metadata": {}, "outputs": [], "source": ["new_chat_history = [\n", "    ChatMessage(role=\"user\", content=\"Why context augmentation?\"),\n", "    ChatMessage(\n", "        role=\"assistant\",\n", "        content=\"LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data. However, they are not trained on your data, which may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks. LlamaIndex provides tooling to enable context augmentation. A popular example is Retrieval-Augmented Generation (RAG) which combines context with LLMs at inference time. Another is finetuning.\",\n", "    ),\n", "    ChatMessage(role=\"user\", content=\"Who is LlamaIndex for?\"),\n", "    ChatMessage(\n", "        role=\"assistant\",\n", "        content=\"LlamaIndex provides tools for beginners, advanced users, and everyone in between. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.\",\n", "    ),\n", "]\n", "memory.put(new_chat_history[0])\n", "memory.put(new_chat_history[1])\n", "memory.put(new_chat_history[2])\n", "memory.put(new_chat_history[3])\n", "history = memory.get()"]}, {"cell_type": "markdown", "id": "9cedc25a-cf6c-45c2-977d-c3fef14f1ea5", "metadata": {}, "source": ["历史记录现在将被更新，包含最新的信息摘要。\n"]}, {"cell_type": "code", "execution_count": null, "id": "2e03428c-069f-4c17-9882-17f3fd0dcf4c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='The user asked about LlamaIndex and why context augmentation is important. The assistant explained that LlamaIndex is for building context-augmented LLM applications, which are necessary because LLMs are not trained on specific private or problem-specific data. LlamaIndex provides tools for beginners and advanced users to ingest and query data easily, as well as customize modules for more complex applications.', additional_kwargs={})]\n"]}], "source": ["print(history)"]}, {"cell_type": "markdown", "id": "f402643f-74ee-4c8f-902f-59296d4d8edf", "metadata": {}, "source": ["使用更长的*token_limit_full_text*可以让用户控制保留完整聊天记录和摘要之间的平衡。\n"]}, {"cell_type": "code", "execution_count": null, "id": "91d34b9b-04c3-4453-85ab-8afe59a7e763", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='The user inquired about LlamaIndex, and the assistant explained that it is a data framework for creating context-augmented LLM applications. These applications can include question-answering chatbots, document understanding and extraction, and autonomous agents for research and actions. LlamaIndex provides tools for building these applications from prototype to production, enabling data processing, complex query workflows, and LLM prompting.', additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Why context augmentation?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\"LLMs offer a natural language interface between humans and data. Widely available models come pre-trained on huge amounts of publicly available data. However, they are not trained on your data, which may be private or specific to the problem you're trying to solve. It's behind APIs, in SQL databases, or trapped in PDFs and slide decks. LlamaIndex provides tooling to enable context augmentation. A popular example is Retrieval-Augmented Generation (RAG) which combines context with LLMs at inference time. Another is finetuning.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Who is LlamaIndex for?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='LlamaIndex provides tools for beginners, advanced users, and everyone in between. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. For more complex applications, our lower-level APIs allow advanced users to customize and extend any module—data connectors, indices, retrievers, query engines, reranking modules—to fit their needs.', additional_kwargs={})]\n"]}], "source": ["memory = ChatSummaryMemoryBuffer.from_defaults(\n", "    chat_history=chat_history + new_chat_history,\n", "    llm=summarizer_llm,\n", "    token_limit=256,\n", "    tokenizer_fn=tokenizer_fn,\n", ")\n", "print(memory.get())"]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}