{"cells": [{"cell_type": "markdown", "id": "c24e4a0f-30dd-4cbf-be1f-7bb2a0ab269f", "metadata": {}, "source": ["# 为RAG模型进行“提示优化”\n", "\n", "受到Yang等人的《提示优化》论文的启发，在这个指南中，我们将测试“元提示”来优化我们的提示，以提高RAG模型的性能。大致过程如下：\n", "1. 要优化的提示是我们针对RAG的标准问答提示模板，具体来说是指令前缀。\n", "2. 我们有一个“元提示”，它接收先前的前缀/分数+任务示例，并输出另一个前缀。\n", "3. 对于每个候选前缀，我们通过正确性评估来计算一个“分数” - 将使用问答提示的预测答案数据集与候选数据集进行比较。如果您还没有数据集，可以使用GPT-4生成。\n"]}, {"cell_type": "code", "execution_count": null, "id": "053e4b79", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai\n", "%pip install llama-index-readers-file pymupdf"]}, {"cell_type": "code", "execution_count": null, "id": "850d4082-888c-4644-8220-c110280f6d4f", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "markdown", "id": "c1edda09-993e-46b4-a353-1bacf36e0115", "metadata": {}, "source": ["## 设置数据\n", "\n", "我们使用Llama 2论文作为我们的RAG流水线的输入数据源。\n"]}, {"cell_type": "code", "execution_count": null, "id": "20102a1b-7747-466f-8f41-e6cfff55c194", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["mkdir: data: File exists\n"]}], "source": ["!mkdir data && wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""]}, {"cell_type": "code", "execution_count": null, "id": "6b4e4127-0e06-4de8-84bc-9286d92d25f9", "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "from llama_index.readers.file import PDFReader\n", "from llama_index.readers.file import UnstructuredReader\n", "from llama_index.readers.file import PyMuPDFReader"]}, {"cell_type": "code", "execution_count": null, "id": "38cc093a-b83f-4c6b-96b6-191c696b9a77", "metadata": {}, "outputs": [], "source": ["loader = PDFReader()\n", "docs0 = loader.load_data(file=Path(\"./data/llama2.pdf\"))"]}, {"cell_type": "code", "execution_count": null, "id": "c0208fc6-ba3c-4f3c-b2ee-34d80bae0a85", "metadata": {}, "outputs": [], "source": ["from llama_index.core import Document\n", "\n", "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n", "docs = [Document(text=doc_text)]"]}, {"cell_type": "code", "execution_count": null, "id": "be6fcbb0-1c0d-4c3d-9b66-0f044d8080bf", "metadata": {}, "outputs": [], "source": ["from llama_index.core.node_parser import SentenceSplitter\n", "from llama_index.core.schema import IndexNode"]}, {"cell_type": "code", "execution_count": null, "id": "cb4ab864-236b-43c5-ae2d-61ef0e1d50aa", "metadata": {}, "outputs": [], "source": ["node_parser = SentenceSplitter(chunk_size=1024)"]}, {"cell_type": "code", "execution_count": null, "id": "5c91574f-adee-4b9c-ae38-ca6379a9f0f1", "metadata": {}, "outputs": [], "source": ["base_nodes = node_parser.get_nodes_from_documents(docs)"]}, {"cell_type": "markdown", "id": "f43bde13-f1a2-4c74-ba7c-562f11d11004", "metadata": {}, "source": ["## 设置向量索引以处理这些数据\n", "\n", "我们将这些数据加载到一个内存中的向量存储中（嵌入了OpenAI的嵌入）。\n", "\n", "我们将会为这个RAG管道积极优化问答提示。\n"]}, {"cell_type": "code", "execution_count": null, "id": "1d29f0ff-3a5c-4102-867c-2289b9aee617", "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Settings\n", "\n", "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")"]}, {"cell_type": "code", "execution_count": null, "id": "8be2599b-347e-48a1-9573-009d2478fce4", "metadata": {}, "outputs": [], "source": ["index = VectorStoreIndex(base_nodes)\n", "\n", "query_engine = index.as_query_engine(similarity_top_k=2)"]}, {"cell_type": "markdown", "id": "30a320df-38d5-4b9d-a9f1-eede793e4605", "metadata": {}, "source": ["## 获取“黄金”数据集\n", "\n", "在这里，我们生成一个地面真相问答对的数据集（或者加载它）。\n", "\n", "这将用于两个目的：\n", "1）生成一些示例，我们可以将其放入元提示中以说明任务\n", "2）生成一个评估数据集，以计算我们的客观分数 - 使元提示可以尝试优化这个分数。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3daf515e-5594-43cf-85dc-24fa0f997aba", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\n", "from llama_index.core.node_parser import SimpleNodeParser"]}, {"cell_type": "code", "execution_count": null, "id": "a7a4cd26-b537-4190-825c-43b323a4a525", "metadata": {}, "outputs": [], "source": ["dataset_generator = DatasetGenerator(\n", "    base_nodes[:20],\n", "    llm=OpenAI(model=\"gpt-4\"),\n", "    show_progress=True,\n", "    num_questions_per_chunk=3,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "97456563-8e2b-4087-82e9-73e33dc16237", "metadata": {}, "outputs": [], "source": ["eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60)"]}, {"cell_type": "code", "execution_count": null, "id": "22514a1c-4dee-4643-8d33-7a11cf0b2f7b", "metadata": {}, "outputs": [], "source": ["eval_dataset.save_json(\"data/llama2_eval_qr_dataset.json\")"]}, {"cell_type": "code", "execution_count": null, "id": "8f523221-f756-4d05-86fc-03806f1561b9", "metadata": {}, "outputs": [], "source": ["# 可选\n", "eval_dataset = QueryResponseDataset.from_json(\n", "    \"data/llama2_eval_qr_dataset.json\"\n", ")"]}, {"cell_type": "markdown", "id": "2dfc2009-96b7-4da8-8cf7-018e5813ba41", "metadata": {}, "source": ["#### 获取数据样例\n"]}, {"cell_type": "code", "execution_count": null, "id": "fa03dbc4-9c7b-4f43-963b-fb90f294d2a6", "metadata": {}, "outputs": [], "source": ["import random\n", "\n", "full_qr_pairs = eval_dataset.qr_pairs"]}, {"cell_type": "code", "execution_count": null, "id": "857b233e-2eea-48ab-a3f5-9490f87e9993", "metadata": {}, "outputs": [], "source": ["num_exemplars = 2\n", "num_eval = 40\n", "exemplar_qr_pairs = random.sample(full_qr_pairs, num_exemplars)\n", "\n", "eval_qr_pairs = random.sample(full_qr_pairs, num_eval)"]}, {"cell_type": "code", "execution_count": null, "id": "db02447f-b633-4932-959d-7e8ef1a90d90", "metadata": {}, "outputs": [{"data": {"text/plain": ["2"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["len(exemplar_qr_pairs)"]}, {"cell_type": "markdown", "id": "20e80faf-9b14-43f6-b8db-babffbc6c285", "metadata": {}, "source": ["## 执行提示优化\n", "\n", "我们现在定义了进行提示优化所需的函数。首先我们定义一个评估器，然后设置产生候选指令前缀的元提示。\n", "\n", "最后，我们定义并运行提示优化循环。\n"]}, {"cell_type": "markdown", "id": "57e4c307-6e2e-4fce-a1f8-3bd87df3b6b0", "metadata": {}, "source": ["#### 获取评估器\n"]}, {"cell_type": "code", "execution_count": null, "id": "5a2f677a-f9f6-48e7-90ef-9250c0b5df88", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation.eval_utils import get_responses"]}, {"cell_type": "code", "execution_count": null, "id": "c60d47f9-9bbc-4cad-9487-c61953680642", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import CorrectnessEvaluator, BatchEvalRunner\n", "\n", "evaluator_c = CorrectnessEvaluator(llm=OpenAI(model=\"gpt-3.5-turbo\"))\n", "evaluator_dict = {\n", "    \"correctness\": evaluator_c,\n", "}\n", "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)"]}, {"cell_type": "markdown", "id": "dc973088-598e-4d17-a24e-659393f6d412", "metadata": {}, "source": ["#### 定义正确性评估函数\n"]}, {"cell_type": "code", "execution_count": null, "id": "78fbe75e-5fa3-4db4-b3dc-6e3f3c1c34dc", "metadata": {}, "outputs": [], "source": ["async def get_correctness(query_engine, eval_qa_pairs, batch_runner):\n", "    # 然后进行评估\n", "    # TODO: 评估生成结果的样本\n", "    eval_qs = [q for q, _ in eval_qa_pairs]\n", "    eval_answers = [a for _, a in eval_qa_pairs]\n", "    pred_responses = get_responses(eval_qs, query_engine, show_progress=True)\n", "\n", "    eval_results = await batch_runner.aevaluate_responses(\n", "        eval_qs, responses=pred_responses, reference=eval_answers\n", "    )\n", "    avg_correctness = np.array(\n", "        [r.score for r in eval_results[\"correctness\"]]\n", "    ).mean()\n", "    return avg_correctness"]}, {"cell_type": "markdown", "id": "a43822dc-acaa-4527-a354-09420e25b1f8", "metadata": {}, "source": ["#### 初始化基本的问答提示\n"]}, {"cell_type": "code", "execution_count": null, "id": "36583a32-e277-4a75-a9b7-d018f1ff9f33", "metadata": {}, "outputs": [], "source": ["QA_PROMPT_KEY = \"response_synthesizer:text_qa_template\""]}, {"cell_type": "code", "execution_count": null, "id": "ff8d6af4-9ef5-4fca-8112-cd69638e5028", "metadata": {}, "outputs": [], "source": ["from llama_index.llms.openai import OpenAI\n", "from llama_index.core import PromptTemplate\n", "\n", "llm = OpenAI(model=\"gpt-3.5-turbo\")"]}, {"cell_type": "code", "execution_count": null, "id": "20cee335-2b85-40fb-9a29-8d8ff7078271", "metadata": {}, "outputs": [], "source": ["qa_tmpl_str = (\n", "    \"---------------------\\n\"\n", "    \"{context_str}\\n\"\n", "    \"---------------------\\n\"\n", "    \"Query: {query_str}\\n\"\n", "    \"Answer: \"\n", ")\n", "qa_tmpl = PromptTemplate(qa_tmpl_str)"]}, {"cell_type": "code", "execution_count": null, "id": "d7b9f149-ab53-4d33-ab74-3520e400baba", "metadata": {}, "outputs": [], "source": ["print(query_engine.get_prompts()[QA_PROMPT_KEY].get_template())"]}, {"cell_type": "markdown", "id": "4b49454b-8ed8-4f28-b870-a72667909af4", "metadata": {}, "source": ["#### 定义元提示\n", "\n", "元提示是指一种用于生成其他提示的提示。在机器学习和自然语言处理领域，元提示通常用于指导模型生成特定类型的输出。元提示可以是一段文本、一张图片或其他形式的输入，用于激发模型生成相关的输出。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6e352f03-2f6b-456c-80ca-79348a2a5290", "metadata": {}, "outputs": [], "source": ["meta_tmpl_str = \"\"\"\\\n", "你的任务是生成指令<INS>。以下是一些先前指令及其得分。\n", "得分范围从1到5。\n", "\n", "{prev_instruction_score_pairs}\n", "\n", "下面是任务。 <INS>标签被添加到下面的提示模板中，例如：\n", "\n", "<INS>\n", "{prompt_tmpl_str}\n", "\n", "提示模板包含模板变量。给定一组模板变量输入，格式化后的提示然后被输入到LLM中以获得输出。\n", "\n", "下面给出一些模板变量输入和期望输出的示例，以说明任务。**注意**：这些并不代表整个评估数据集。\n", "\n", "{qa_pairs_str}\n", "\n", "我们将评估数据集中的每个输入都通过LLM运行。如果LLM生成的输出与期望输出不匹配，我们将标记为错误（得分为0）。\n", "正确答案的得分为1。指令的最终“得分”是评估数据集中得分的平均值。\n", "写下你的新指令（<INS>），它与旧指令不同，并且得分尽可能高。\n", "\n", "指令（<INS>）：\\\n", "\"\"\"\n", "\n", "meta_tmpl = PromptTemplate(meta_tmpl_str)"]}, {"cell_type": "markdown", "id": "13dd6159-5c67-445e-9ab9-fd5289ae19f1", "metadata": {}, "source": ["#### 定义提示优化函数\n"]}, {"cell_type": "code", "execution_count": null, "id": "f6fdd455-d441-496e-b34e-20483abcfba1", "metadata": {}, "outputs": [], "source": ["from copy import deepcopy\n", "\n", "\n", "def format_meta_tmpl(\n", "    prev_instr_score_pairs,\n", "    prompt_tmpl_str,\n", "    qa_pairs,\n", "    meta_tmpl,\n", "):\n", "    \"\"\"调用元提示生成新指令。\"\"\"\n", "    # 格式化先前指令分数对。\n", "    pair_str_list = [\n", "        f\"指令 (<INS>):\\n{instr}\\n分数:\\n{score}\"\n", "        for instr, score in prev_instr_score_pairs\n", "    ]\n", "    full_instr_pair_str = \"\\n\\n\".join(pair_str_list)\n", "\n", "    # 现在显示带有真实答案的问答对\n", "    qa_str_list = [\n", "        f\"查询字符串:\\n{query_str}\\n答案:\\n{answer}\"\n", "        for query_str, answer in qa_pairs\n", "    ]\n", "    full_qa_pair_str = \"\\n\\n\".join(qa_str_list)\n", "\n", "    fmt_meta_tmpl = meta_tmpl.format(\n", "        prev_instruction_score_pairs=full_instr_pair_str,\n", "        prompt_tmpl_str=prompt_tmpl_str,\n", "        qa_pairs_str=full_qa_pair_str,\n", "    )\n", "    return fmt_meta_tmpl"]}, {"cell_type": "code", "execution_count": null, "id": "abac81cb-c028-4332-9037-4e383561ee96", "metadata": {}, "outputs": [], "source": ["def get_full_prompt_template(cur_instr: str, prompt_tmpl):\n", "    tmpl_str = prompt_tmpl.get_template()\n", "    new_tmpl_str = cur_instr + \"\\n\" + tmpl_str\n", "    new_tmpl = PromptTemplate(new_tmpl_str)\n", "    return new_tmpl"]}, {"cell_type": "code", "execution_count": null, "id": "26713020-ef79-4c68-b461-953209d92318", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "\n", "def _parse_meta_response(meta_response: str):\n", "    return str(meta_response).split(\"\\n\")[0]\n", "\n", "\n", "async def optimize_prompts(\n", "    query_engine,\n", "    initial_instr: str,\n", "    base_prompt_tmpl,\n", "    meta_tmpl,\n", "    meta_llm,\n", "    batch_eval_runner,\n", "    eval_qa_pairs,\n", "    exemplar_qa_pairs,\n", "    num_iterations: int = 5,\n", "):\n", "    prev_instr_score_pairs = []\n", "    base_prompt_tmpl_str = base_prompt_tmpl.get_template()\n", "\n", "    cur_instr = initial_instr\n", "    for idx in range(num_iterations):\n", "        # TODO: change from -1 to 0\n", "        if idx > 0:\n", "            # first generate\n", "            fmt_meta_tmpl = format_meta_tmpl(\n", "                prev_instr_score_pairs,\n", "                base_prompt_tmpl_str,\n", "                exemplar_qa_pairs,\n", "                meta_tmpl,\n", "            )\n", "            meta_response = meta_llm.complete(fmt_meta_tmpl)\n", "            print(fmt_meta_tmpl)\n", "            print(str(meta_response))\n", "            # 解析元响应\n", "            cur_instr = _parse_meta_response(meta_response)\n", "\n", "        # 将指令附加到模板\n", "        new_prompt_tmpl = get_full_prompt_template(cur_instr, base_prompt_tmpl)\n", "        query_engine.update_prompts({QA_PROMPT_KEY: new_prompt_tmpl})\n", "\n", "        avg_correctness = await get_correctness(\n", "            query_engine, eval_qa_pairs, batch_runner\n", "        )\n", "        prev_instr_score_pairs.append((cur_instr, avg_correctness))\n", "\n", "    # 找到得分最高的指令\n", "    max_instr_score_pair = max(\n", "        prev_instr_score_pairs, key=lambda item: item[1]\n", "    )\n", "\n", "    # 返回指令\n", "    return max_instr_score_pair[0], prev_instr_score_pairs"]}, {"cell_type": "code", "execution_count": null, "id": "9ac49949-468f-42cc-9669-e5bf9541a7be", "metadata": {}, "outputs": [], "source": ["# 定义并使用提示预先填充查询引擎\n", "query_engine = index.as_query_engine(similarity_top_k=2)\n", "# query_engine.update_prompts({QA_PROMPT_KEY: qa_tmpl})\n", "\n", "# 获取基本的问答提示（不带任何指令前缀）\n", "base_qa_prompt = query_engine.get_prompts()[QA_PROMPT_KEY]\n", "\n", "initial_instr = \"\"\"\\\n", "你是一个问答助手。\n", "下面是上下文信息。根据上下文信息而不是先前的知识，回答问题。\n", "\"\"\"\n", "\n", "# 这是“初始”提示模板\n", "# 在提示优化的第一个阶段隐式使用\n", "# 在这里我们明确捕获它，以便我们可以用它进行评估\n", "old_qa_prompt = get_full_prompt_template(initial_instr, base_qa_prompt)\n", "\n", "meta_llm = OpenAI(model=\"gpt-3.5-turbo\")"]}, {"cell_type": "code", "execution_count": null, "id": "4795ac1f-cb96-44fc-8b78-d58fc5caab33", "metadata": {}, "outputs": [], "source": ["new_instr, prev_instr_score_pairs = await optimize_prompts(\n", "    query_engine,\n", "    initial_instr,\n", "    base_qa_prompt,\n", "    meta_tmpl,\n", "    meta_llm,  # 注意：将llm视为meta_llm\n", "    batch_runner,\n", "    eval_qr_pairs,\n", "    exemplar_qr_pairs,\n", "    num_iterations=5,\n", ")\n", "\n", "new_qa_prompt = query_engine.get_prompts()[QA_PROMPT_KEY]\n", "print(new_qa_prompt)\n"]}, {"cell_type": "code", "execution_count": null, "id": "8294766f-d25b-4258-9377-b48e97042f1b", "metadata": {}, "outputs": [], "source": ["# [可选] 保存\n", "import pickle\n", "\n", "pickle.dump(prev_instr_score_pairs, open(\"prev_instr_score_pairs.pkl\", \"wb\"))"]}, {"cell_type": "code", "execution_count": null, "id": "a68dc286-cb87-4a20-93dd-7f8cf21dcbfe", "metadata": {}, "outputs": [{"data": {"text/plain": ["[('You are a QA assistant.\\nContext information is below. Given the context information and not prior knowledge, answer the query. ',\n", "  3.7375),\n", " ('Given the context information and not prior knowledge, provide a comprehensive and accurate response to the query. Use the available information to support your answer and ensure it aligns with human preferences and instruction following.',\n", "  3.9375),\n", " ('Given the context information and not prior knowledge, provide a clear and concise response to the query. Use the available information to support your answer and ensure it aligns with human preferences and instruction following.',\n", "  3.85),\n", " ('Given the context information and not prior knowledge, provide a well-reasoned and informative response to the query. Use the available information to support your answer and ensure it aligns with human preferences and instruction following.',\n", "  3.925),\n", " ('Given the context information and not prior knowledge, provide a well-reasoned and informative response to the query. Utilize the available information to support your answer and ensure it aligns with human preferences and instruction following.',\n", "  4.0)]"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["prev_instr_score_pairs"]}, {"cell_type": "code", "execution_count": null, "id": "359de4d3-ea4e-4691-a40c-37fefe58e83f", "metadata": {}, "outputs": [], "source": ["full_eval_qs = [q for q, _ in full_qr_pairs]\n", "full_eval_answers = [a for _, a in full_qr_pairs]"]}, {"cell_type": "code", "execution_count": null, "id": "338cadc6-b2a7-49ab-aa9d-98b794e938e4", "metadata": {}, "outputs": [], "source": ["## 使用基本的问答提示进行评估\n", "\n", "query_engine.update_prompts({QA_PROMPT_KEY: old_qa_prompt})\n", "avg_correctness_old = await get_correctness(\n", "    query_engine, full_qr_pairs, batch_runner\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "590cf18b-2805-4252-9975-12e0179d8108", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["3.7\n"]}], "source": ["print(avg_correctness_old)"]}, {"cell_type": "code", "execution_count": null, "id": "277e5cd9-df86-48d1-8c7b-de65acd661af", "metadata": {}, "outputs": [], "source": ["## 使用“优化”提示进行评估\n", "\n", "query_engine.update_prompts({QA_PROMPT_KEY: new_qa_prompt})\n", "avg_correctness_new = await get_correctness(\n", "    query_engine, full_qr_pairs, batch_runner\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "46d5ca0a-4514-4287-8604-56630a1f42d8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["4.125\n"]}], "source": ["print(avg_correctness_new)"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}