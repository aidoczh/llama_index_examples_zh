{"cells": [{"cell_type": "markdown", "id": "dd8b51d9-b28c-44b8-a73e-926c90b018a3", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/prompts/advanced_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "c5ec4ea1-78bb-4dfe-b193-ab9ba5a10e4c", "metadata": {}, "source": ["# 高级提示技巧（变量映射，函数）\n", "\n", "在这个笔记本中，我们将展示一些高级的提示技巧。这些功能允许您定义更多定制/表达力强的提示，重用现有的提示，并且用更少的代码表达某些操作。\n", "\n", "我们将展示以下功能：\n", "1. 部分格式化\n", "2. 提示模板变量映射\n", "3. 提示函数映射\n"]}, {"cell_type": "code", "execution_count": null, "id": "0a50028b", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "e1cd3473-df8c-4b5c-abac-d663b0117fef", "metadata": {}, "outputs": [], "source": ["from llama_index.core import PromptTemplate\n", "from llama_index.llms.openai import OpenAI"]}, {"cell_type": "markdown", "id": "f435c4df-3682-4a8a-872b-a79ace3695ee", "metadata": {}, "source": ["## 1. 部分格式化\n", "\n", "部分格式化（`partial_format`）允许您部分格式化提示，填充一些变量，同时将其他变量留待以后填充。\n", "\n", "这是一个很方便的函数，这样您就不必一直保留所有必需的提示变量直到`format`，可以在它们到达时部分格式化。\n", "\n", "这将创建提示模板的副本。\n"]}, {"cell_type": "code", "execution_count": null, "id": "a19eaa7f-1e72-498f-8e29-fec9b1ef9ceb", "metadata": {}, "outputs": [], "source": ["qa_prompt_tmpl_str = \"\"\"\\\n", "下面是上下文信息。\n", "---------------------\n", "{context_str}\n", "---------------------\n", "根据上下文信息而非先验知识，回答问题。\n", "请以{tone_name}的风格写出答案。\n", "问题: {query_str}\n", "答案: \\\n", "\"\"\"\n", "\n", "prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n"]}, {"cell_type": "code", "execution_count": null, "id": "933db073-712d-4feb-b49f-6c64a20ec2fa", "metadata": {}, "outputs": [], "source": ["partial_prompt_tmpl = prompt_tmpl.partial_format(tone_name=\"Shakespeare\")"]}, {"cell_type": "code", "execution_count": null, "id": "470446e4-aeb9-40cb-9017-fcdd03af8d4c", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'tone_name': 'Shakespeare'}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["partial_prompt_tmpl.kwargs"]}, {"cell_type": "code", "execution_count": null, "id": "b9627977-5d2a-4300-a9da-91a5dfb671a3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query.\n", "Please write the answer in the style of Shakespeare\n", "Query: How many params does llama 2 have\n", "Answer: \n"]}], "source": ["fmt_prompt = partial_prompt_tmpl.format(\n", "    context_str=\"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters\",\n", "    query_str=\"How many params does llama 2 have\",\n", ")\n", "print(fmt_prompt)"]}, {"cell_type": "markdown", "id": "1cf5e3d8-a0f8-40fd-ba32-b72f01036c24", "metadata": {}, "source": ["## 2. 提示模板变量映射\n", "\n", "模板变量映射允许您指定从“预期”提示键（例如用于响应合成的`context_str`和`query_str`）到实际模板中的键的映射。\n", "\n", "这样可以让您重复使用现有的字符串模板，而无需烦人地更改模板变量。\n"]}, {"cell_type": "code", "execution_count": null, "id": "e197319a-37cc-4a3f-a623-8fffb9c3d932", "metadata": {}, "outputs": [], "source": ["\n", "# 注意：这里请注意我们使用`my_context`和`my_query`作为模板变量\n", "\n", "qa_prompt_tmpl_str = \"\"\"\\\n", "下面是上下文信息。\n", "---------------------\n", "{my_context}\n", "---------------------\n", "根据上下文信息和非先验知识，回答查询。\n", "查询：{my_query}\n", "答案：\\\n", "\"\"\"\n", "\n", "template_var_mappings = {\"context_str\": \"my_context\", \"query_str\": \"my_query\"}\n", "\n", "prompt_tmpl = PromptTemplate(\n", "    qa_prompt_tmpl_str, template_var_mappings=template_var_mappings\n", ")\n"]}, {"cell_type": "code", "execution_count": null, "id": "44936c0f-bae1-4955-b59f-4bcfb373bdc8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query.\n", "Please write the answer in the style of Shakespeare\n", "Query: How many params does llama 2 have\n", "Answer: \n"]}], "source": ["fmt_prompt = partial_prompt_tmpl.format(\n", "    context_str=\"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters\",\n", "    query_str=\"How many params does llama 2 have\",\n", ")\n", "print(fmt_prompt)"]}, {"cell_type": "markdown", "id": "4f21e05d-145a-4d0a-b36e-10e9685af32c", "metadata": {}, "source": ["### 3. 提示函数映射\n", "\n", "您还可以将函数作为模板变量传递，而不是固定值。\n", "\n", "这允许您在查询时动态注入某些值，这些值取决于其他值。\n", "\n", "以下是一些基本示例。我们在我们的RAG提示工程指南中展示了更高级的示例（例如few-shot示例）。\n"]}, {"cell_type": "code", "execution_count": null, "id": "dffd1302-ec1c-411d-b0ef-23fd40ea4ba9", "metadata": {}, "outputs": [], "source": ["qa_prompt_tmpl_str = \"\"\"\\\n", "下面是上下文信息。\n", "---------------------\n", "{context_str}\n", "---------------------\n", "根据上下文信息而非先验知识来回答问题。\n", "问题: {query_str}\n", "回答: \\\n", "\"\"\"\n", "\n", "\n", "def format_context_fn(**kwargs):\n", "    # 使用项目符号格式化上下文\n", "    context_list = kwargs[\"context_str\"].split(\"\\n\\n\")\n", "    fmtted_context = \"\\n\\n\".join([f\"- {c}\" for c in context_list])\n", "    return fmtted_context\n", "\n", "\n", "prompt_tmpl = PromptTemplate(\n", "    qa_prompt_tmpl_str, function_mappings={\"context_str\": format_context_fn}\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "6e078289-f0bc-4848-9e97-7bf7eb4abbc1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "- In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n", "\n", "- Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases.\n", "\n", "- Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models.\n", "\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query.\n", "Query: How many params does llama 2 have\n", "Answer: \n"]}], "source": ["context_str = \"\"\"\\\n", "在这项工作中，我们开发并发布了Llama 2，这是一个包含了预训练和微调的大型语言模型（LLMs），规模从70亿到700亿参数不等。\n", "\n", "我们的微调LLMs，称为Llama 2-Chat，经过优化以用于对话场景。\n", "\n", "我们的模型在我们测试的大多数基准测试中表现优于开源聊天模型，并根据我们的人工评估，对于帮助性和安全性，可能是封闭源模型的合适替代品。\n", "\"\"\"\n", "\n", "fmt_prompt = prompt_tmpl.format(\n", "    context_str=context_str, query_str=\"Llama 2有多少参数\"\n", ")\n", "print(fmt_prompt)"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}