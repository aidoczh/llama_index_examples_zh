{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "d1eb11b1", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/prompts/prompt_mixin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "4a5a641c-d90c-4401-ad59-369d70babf5b", "metadata": {}, "source": ["# 访问/定制高级模块中的提示\n", "\n", "LlamaIndex包含各种高级模块（查询引擎、响应合成器、检索器等），其中许多模块都会进行LLM调用并使用提示模板。\n", "\n", "本指南展示了如何通过`get_prompts`来访问任何模块（包括嵌套模块）的提示集，以及如何使用`update_prompts`轻松更新这些提示。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "b34cf919", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "d8c8cd3e", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "487c01b6-60ea-457f-8cc9-2a448d401d49", "metadata": {}, "outputs": [], "source": ["import os\n", "import openai"]}, {"cell_type": "code", "execution_count": null, "id": "5a94148e-a2f9-44b3-919f-4d9811ae27c1", "metadata": {}, "outputs": [], "source": ["os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"cell_type": "code", "execution_count": null, "id": "352384e6-0f31-4aa9-a351-4d3b278e2afd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n", "NumExpr defaulting to 8 threads.\n"]}], "source": ["import logging\n", "import sys\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import (\n", "    VectorStoreIndex,\n", "    SimpleDirectoryReader,\n", "    load_index_from_storage,\n", "    StorageContext,\n", ")\n", "from IPython.display import Markdown, display"]}, {"cell_type": "markdown", "id": "6edb6b21-565e-4993-98ab-bad44a2259b9", "metadata": {}, "source": ["## 设置：加载数据，构建索引并获取查询引擎\n", "\n", "在这里，我们将在一个玩具数据集（PG的文章）上构建一个向量索引，并访问查询引擎。\n", "\n", "查询引擎是一个简单的RAG流水线，包括top-k检索和LLM合成。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "0e299e93", "metadata": {}, "source": ["下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "3c036532", "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "id": "c0f1a04a-7859-4d97-810d-abde72891d1c", "metadata": {}, "outputs": [], "source": ["# 加载文档", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"]}, {"cell_type": "code", "execution_count": null, "id": "b84c7e74-c084-40b0-b94b-d454e095e746", "metadata": {}, "outputs": [], "source": ["index = VectorStoreIndex.from_documents(documents)"]}, {"cell_type": "code", "execution_count": null, "id": "75b8534b-eb16-47a7-9269-dfd4ebf97fd7", "metadata": {}, "outputs": [], "source": ["# 将日志记录级别设置为DEBUG，以获得更详细的输出", "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")"]}, {"cell_type": "code", "execution_count": null, "id": "d192cc45-f90a-4c8d-b9e0-29f70272a47e", "metadata": {}, "outputs": [], "source": ["# 定义提示查看函数", "def display_prompt_dict(prompts_dict):", "    for k, p in prompts_dict.items():", "        text_md = f\"**提示键**：{k}<br>\" f\"**文本：**<br>\"", "        display(Markdown(text_md))", "        print(p.get_template())", "        display(Markdown(\"<br><br>\"))"]}, {"cell_type": "markdown", "id": "50d48e1c-15ce-4eb3-9e3c-00e7e547bf2c", "metadata": {}, "source": ["## 访问提示\n", "\n", "在这里，我们从查询引擎中获取提示。请注意，*所有*提示都会被返回，包括在查询引擎的子模块中使用的提示。这样可以让您集中查看这些提示！\n"]}, {"cell_type": "code", "execution_count": null, "id": "7f28b46d-6115-4095-b1a2-5603a8ee709e", "metadata": {}, "outputs": [], "source": ["prompts_dict = query_engine.get_prompts()"]}, {"cell_type": "code", "execution_count": null, "id": "1413a347-817c-4648-a4ee-1ea00dcbd319", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information from multiple sources is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the information from multiple sources and not prior knowledge, answer the query.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "1a110a58-2f93-42c5-a1ec-89ecc3692f93", "metadata": {}, "source": ["#### 在响应合成器上检查 `get_prompts`\n", "\n", "您还可以在底层的响应合成器上调用 `get_prompts`，您将看到相同的列表。\n"]}, {"cell_type": "code", "execution_count": null, "id": "a2d99264-0f53-4da3-aa76-ab88f1425dc1", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: summary_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information from multiple sources is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the information from multiple sources and not prior knowledge, answer the query.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = query_engine.response_synthesizer.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "cdb003e4-e58b-46dd-abae-fb09b0c891a2", "metadata": {}, "source": ["#### 使用不同的响应合成策略检查`get_prompts`\n", "\n", "在这里，我们尝试使用默认的`compact`方法。\n", "\n", "我们会发现使用的模板集是不同的；一个是问答模板，另一个是改进模板。\n"]}, {"cell_type": "code", "execution_count": null, "id": "d523fb00-7106-4849-8ce9-4bbc13dd912c", "metadata": {}, "outputs": [], "source": ["# 将日志级别设置为DEBUG，以获得更详细的输出", "query_engine = index.as_query_engine(response_mode=\"compact\")"]}, {"cell_type": "code", "execution_count": null, "id": "27474aef-06c3-4684-8778-9b00ba4ae7ef", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["The original query is as follows: {query_str}\n", "We have provided an existing answer: {existing_answer}\n", "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n", "------------\n", "{context_msg}\n", "------------\n", "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n", "Refined Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = query_engine.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "b545710e-bb29-4dd4-885c-bd9222e67b26", "metadata": {}, "source": ["#### 放入查询引擎，获取响应\n"]}, {"cell_type": "code", "execution_count": null, "id": "4936c416-bdd8-48d4-95c3-760b5f2a2bc8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They later got a microcomputer and started programming on it, writing simple games and a word processor. They also mentioned their interest in philosophy and AI.\n"]}], "source": ["response = query_engine.query(\"What did the author do growing up?\")\n", "print(str(response))"]}, {"cell_type": "markdown", "id": "71103bb5-fe3e-4373-8cf9-b786f64f0189", "metadata": {}, "source": ["## 自定义提示符\n", "\n", "您还可以使用`update_prompts`函数更新/自定义提示符。将参数值传递给键等于提示字典中看到的键的键。\n", "\n", "在这里，我们将更改摘要提示，以使用莎士比亚。\n"]}, {"cell_type": "code", "execution_count": null, "id": "1545df36-4bd5-40d3-b4c8-a01c89774262", "metadata": {}, "outputs": [], "source": ["from llama_index.core import PromptTemplate", "", "# 重置", "query_engine = index.as_query_engine(response_mode=\"tree_summarize\")", "", "# 莎士比亚！", "new_summary_tmpl_str = (", "    \"下面是上下文信息。\\n\"", "    \"---------------------\\n\"", "    \"{context_str}\\n\"", "    \"---------------------\\n\"", "    \"根据上下文信息和非先验知识，以莎士比亚戏剧的风格回答查询。\\n\"", "    \"查询：{query_str}\\n\"", "    \"答案：\"", ")", "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)"]}, {"cell_type": "code", "execution_count": null, "id": "3bd7687b-bdbf-454e-b3b3-4438b80934a4", "metadata": {}, "outputs": [], "source": ["query_engine.update_prompts(\n", "    {\"response_synthesizer:summary_template\": new_summary_tmpl}\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "ad924fca-5074-4a04-99bc-e5dc0102d131", "metadata": {}, "outputs": [], "source": ["prompts_dict = query_engine.get_prompts()"]}, {"cell_type": "code", "execution_count": null, "id": "cc9e39a0-6db1-419f-816e-c702a8e49e04", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query in the style of a Shakespeare play.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["display_prompt_dict(prompts_dict)"]}, {"cell_type": "code", "execution_count": null, "id": "0b195c32-1913-4d1a-b6cf-3f2635e8773a", "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"What did the author do growing up?\")\n", "print(str(response))"]}, {"cell_type": "markdown", "id": "518c793c-e70a-4f69-b03d-adc0acd6fb2c", "metadata": {}, "source": ["## 从其他模块访问提示\n", "\n", "在这里，我们将看一些其他模块：查询引擎、路由器/选择器、评估器等。\n"]}, {"cell_type": "code", "execution_count": null, "id": "f3bf5ee9-ef06-46e0-bcf4-6e1ad90e3f1a", "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_engine import (\n", "    RouterQueryEngine,\n", "    FLAREInstructQueryEngine,\n", ")\n", "from llama_index.core.selectors import LLMMultiSelector\n", "from llama_index.core.evaluation import FaithfulnessEvaluator, DatasetGenerator\n", "from llama_index.core.postprocessor import LLMRerank"]}, {"cell_type": "markdown", "id": "00c69460-8361-4cef-ba67-af593f3755a1", "metadata": {}, "source": ["#### 分析提示：路由器查询引擎\n"]}, {"cell_type": "code", "execution_count": null, "id": "96090e36-314c-4758-b193-bc4608e4d3f9", "metadata": {}, "outputs": [], "source": ["# 设置样本路由查询引擎", "from llama_index.core.tools import QueryEngineTool", "", "query_tool = QueryEngineTool.from_defaults(", "    query_engine=query_engine, description=\"测试描述\"", ")", "", "router_query_engine = RouterQueryEngine.from_defaults([query_tool])"]}, {"cell_type": "code", "execution_count": null, "id": "25195335-e641-4bb5-8e46-5a950a5e0674", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: summarizer:summary_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information from multiple sources is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the information from multiple sources and not prior knowledge, answer the query.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = router_query_engine.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "3847ce81-8d1a-475d-b293-5b10458197bc", "metadata": {}, "source": ["#### 分析提示：FLARE查询引擎\n"]}, {"cell_type": "code", "execution_count": null, "id": "4d555edd-6c5e-4c68-b06c-8e1e0c365a17", "metadata": {}, "outputs": [], "source": ["flare_query_engine = FLAREInstructQueryEngine(query_engine)"]}, {"cell_type": "code", "execution_count": null, "id": "dc7d8f2d-a522-41af-bfbb-9842e03af595", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: instruct_prompt<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Skill 1. Use the Search API to look up relevant information by writing     \"[Search(query)]\" where \"query\" is the search query you want to look up.     For example:\n", "\n", "Query: But what are the risks during production of nanomaterials?\n", "Answer: [Search(What are some nanomaterial production risks?)]\n", "\n", "Query: The colors on the flag of Ghana have the following meanings.\n", "Answer: Red is for [Search(What is the meaning of Ghana's flag being red?)],     green for forests, and gold for mineral wealth.\n", "\n", "Query: What did the author do during his time in college?\n", "Answer: The author took classes in [Search(What classes did the author take in     college?)].\n", "\n", "\n", "\n", "Skill 2. Solve more complex generation tasks by thinking step by step. For example:\n", "\n", "Query: Give a summary of the author's life and career.\n", "Answer: The author was born in 1990. Growing up, he [Search(What did the     author do during his childhood?)].\n", "\n", "Query: Can you write a summary of the Great Gatsby.\n", "Answer: The Great Gatsby is a novel written by F. Scott Fitzgerald. It is about     [Search(What is the Great Gatsby about?)].\n", "\n", "\n", "Now given the following task, and the stub of an existing answer, generate the next portion of the answer. You may use the Search API \"[Search(query)]\" whenever possible.\n", "If the answer is complete and no longer contains any \"[Search(query)]\" tags, write     \"done\" to finish the task.\n", "Do not write \"done\" if the answer still contains \"[Search(query)]\" tags.\n", "Do not make up answers. It is better to generate one \"[Search(query)]\" tag and stop generation\n", "than to fill in the answer with made up information with no \"[Search(query)]\" tags\n", "or multiple \"[Search(query)]\" tags that assume a structure in the answer.\n", "Try to limit generation to one sentence if possible.\n", "\n", "\n", "Query: {query_str}\n", "Existing Answer: {existing_answer}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Prompt Key**: query_engine:response_synthesizer:summary_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query in the style of a Shakespeare play.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Prompt Key**: lookahead_answer_inserter:answer_insert_prompt<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["\n", "An existing 'lookahead response' is given below. The lookahead response\n", "contains `[Search(query)]` tags. Some queries have been executed and the\n", "response retrieved. The queries and answers are also given below.\n", "Also the previous response (the response before the lookahead response)\n", "is given below.\n", "Given the lookahead template, previous response, and also queries and answers,\n", "please 'fill in' the lookahead template with the appropriate answers.\n", "\n", "NOTE: Please make sure that the final response grammatically follows\n", "the previous response + lookahead template. For example, if the previous\n", "response is \"New York City has a population of \" and the lookahead\n", "template is \"[Search(What is the population of New York City?)]\", then\n", "the final response should be \"8.4 million\".\n", "\n", "NOTE: the lookahead template may not be a complete sentence and may\n", "contain trailing/leading commas, etc. Please preserve the original\n", "formatting of the lookahead template if possible.\n", "\n", "NOTE:\n", "\n", "NOTE: the exception to the above rule is if the answer to a query\n", "is equivalent to \"I don't know\" or \"I don't have an answer\". In this case,\n", "modify the lookahead template to indicate that the answer is not known.\n", "\n", "NOTE: the lookahead template may contain multiple `[Search(query)]` tags\n", "    and only a subset of these queries have been executed.\n", "    Do not replace the `[Search(query)]` tags that have not been executed.\n", "\n", "Previous Response:\n", "\n", "\n", "Lookahead Template:\n", "Red is for [Search(What is the meaning of Ghana's     flag being red?)], green for forests, and gold for mineral wealth.\n", "\n", "Query-Answer Pairs:\n", "Query: What is the meaning of Ghana's flag being red?\n", "Answer: The red represents the blood of those who died in the country's struggle     for independence\n", "\n", "Filled in Answers:\n", "Red is for the blood of those who died in the country's struggle for independence,     green for forests, and gold for mineral wealth.\n", "\n", "Previous Response:\n", "One of the largest cities in the world\n", "\n", "Lookahead Template:\n", ", the city contains a population of [Search(What is the population     of New York City?)]\n", "\n", "Query-Answer Pairs:\n", "Query: What is the population of New York City?\n", "Answer: The population of New York City is 8.4 million\n", "\n", "Synthesized Response:\n", ", the city contains a population of 8.4 million\n", "\n", "Previous Response:\n", "the city contains a population of\n", "\n", "Lookahead Template:\n", "[Search(What is the population of New York City?)]\n", "\n", "Query-Answer Pairs:\n", "Query: What is the population of New York City?\n", "Answer: The population of New York City is 8.4 million\n", "\n", "Synthesized Response:\n", "8.4 million\n", "\n", "Previous Response:\n", "{prev_response}\n", "\n", "Lookahead Template:\n", "{lookahead_response}\n", "\n", "Query-Answer Pairs:\n", "{query_answer_pairs}\n", "\n", "Synthesized Response:\n", "\n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = flare_query_engine.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "214a767a-3681-46b3-b073-9dc1744a1f9d", "metadata": {}, "source": ["#### 分析提示：LLMMultiSelector\n"]}, {"cell_type": "code", "execution_count": null, "id": "d8254af7-076d-4cc3-9a9f-d92686b8f307", "metadata": {}, "outputs": [], "source": ["from llama_index.core.selectors import LLMSingleSelector\n", "\n", "selector = LLMSingleSelector.from_defaults()"]}, {"cell_type": "code", "execution_count": null, "id": "3f3f0d7d-a19b-46cf-946f-6259c70c93df", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: prompt<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\n", "---------------------\n", "{context_list}\n", "---------------------\n", "Using only the choices above and not prior knowledge, return the choice that is most relevant to the question: '{query_str}'\n", "\n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = selector.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "12b47039-0b47-4de6-b408-f04d52bb66e3", "metadata": {}, "source": ["#### 分析提示：FaithfulnessEvaluator\n"]}, {"cell_type": "code", "execution_count": null, "id": "a4368750-6229-4f71-b194-deb0013ab5d7", "metadata": {}, "outputs": [], "source": ["evaluator = FaithfulnessEvaluator()"]}, {"cell_type": "code", "execution_count": null, "id": "ac4ac1f0-a071-4bd0-95bb-54d7bce07e1b", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: eval_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Please tell if a given piece of information is supported by the context.\n", "You need to answer with either YES or NO.\n", "Answer YES if any of the context supports the information, even if most of the context is unrelated. Some examples are provided below. \n", "\n", "Information: Apple pie is generally double-crusted.\n", "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n", "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n", "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n", "Answer: YES\n", "Information: Apple pies tastes bad.\n", "Context: An apple pie is a fruit pie in which the principal filling ingredient is apples. \n", "Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard or cheddar cheese.\n", "It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n", "Answer: NO\n", "Information: {query_str}\n", "Context: {context_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Prompt Key**: refine_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["We want to understand if the following information is present in the context information: {query_str}\n", "We have provided an existing YES/NO answer: {existing_answer}\n", "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n", "------------\n", "{context_msg}\n", "------------\n", "If the existing answer was already YES, still answer YES. If the information is present in the new context, answer YES. Otherwise answer NO.\n", "\n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = evaluator.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "62a8b47c-f827-4bb2-990a-661b1d729b7e", "metadata": {}, "source": ["#### 分析提示：数据集生成器\n"]}, {"cell_type": "code", "execution_count": null, "id": "1edfc3d4-5c22-4d2c-aead-1fa35a04233a", "metadata": {}, "outputs": [], "source": ["dataset_generator = DatasetGenerator.from_documents(documents)"]}, {"cell_type": "code", "execution_count": null, "id": "19dc22df-893f-4553-829c-7e31a16ee9a4", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: text_question_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the context information and not prior knowledge.\n", "generate only questions based on the below query.\n", "{query_str}\n", "\n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Prompt Key**: text_qa_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = dataset_generator.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}, {"cell_type": "markdown", "id": "5fe09fc5-108c-40e6-8343-7e662bf2f67c", "metadata": {}, "source": ["#### 分析提示：LLMRerank\n"]}, {"cell_type": "code", "execution_count": null, "id": "5e3269cc-7767-46c1-a252-92de343f2d9b", "metadata": {}, "outputs": [], "source": ["llm_rerank = LLMRerank()"]}, {"cell_type": "code", "execution_count": null, "id": "94cbc425-7276-4973-8426-ea83c693c949", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: text_question_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the context information and not prior knowledge.\n", "generate only questions based on the below query.\n", "{query_str}\n", "\n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Prompt Key**: text_qa_template<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Context information is below.\n", "---------------------\n", "{context_str}\n", "---------------------\n", "Given the context information and not prior knowledge, answer the query.\n", "Query: {query_str}\n", "Answer: \n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["prompts_dict = dataset_generator.get_prompts()\n", "display_prompt_dict(prompts_dict)"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}