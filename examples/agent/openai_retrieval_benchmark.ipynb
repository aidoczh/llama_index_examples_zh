{"cells": [{"cell_type": "markdown", "id": "99cea58c-48bc-4af6-8358-df9695659983", "metadata": {}, "source": ["# OpenAI检索API（通过助理代理）的基准测试\n", "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/openai_retrieval_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n", "\n", "本指南通过使用我们的`OpenAIAssistantAgent`对[OpenAI助理API](https://platform.openai.com/docs/assistants/overview)中的检索工具进行基准测试。我们对Llama 2论文进行了测试，并将生成质量与一个简单的RAG流水线进行了比较。\n"]}, {"cell_type": "code", "execution_count": null, "id": "f3a19550", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-readers-file pymupdf\n", "%pip install llama-index-agent-openai\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "c61c873d", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "24fce591-8b3a-4c5f-985e-2669a05595bf", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "markdown", "id": "0906a181-30ea-4f04-8307-215b988ea89b", "metadata": {}, "source": ["## 设置数据\n", "\n", "这里我们加载Llama 2论文并对其进行分块。\n"]}, {"cell_type": "code", "execution_count": null, "id": "ae2b6c4c-12db-4e82-be83-76507f4cb938", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2023-11-08 21:53:52--  https://arxiv.org/pdf/2307.09288.pdf\n", "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n", "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 13661300 (13M) [application/pdf]\n", "Saving to: ‘data/llama2.pdf’\n", "\n", "data/llama2.pdf     100%[===================>]  13.03M   141KB/s    in 1m 48s  \n", "\n", "2023-11-08 21:55:42 (123 KB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n"]}], "source": ["!mkdir -p 'data/'\n", "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""]}, {"cell_type": "code", "execution_count": null, "id": "3ce1f3a3-740c-43a2-a755-94f13a2c9762", "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "from llama_index.core import Document, VectorStoreIndex\n", "from llama_index.readers.file import PyMuPDFReader\n", "from llama_index.core.node_parser import SimpleNodeParser\n", "from llama_index.llms.openai import OpenAI"]}, {"cell_type": "code", "execution_count": null, "id": "e0fd2ddf-b13f-498d-ab80-d8d456ca955d", "metadata": {}, "outputs": [], "source": ["loader = PyMuPDFReader()\n", "docs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))\n", "\n", "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n", "docs = [Document(text=doc_text)]"]}, {"cell_type": "code", "execution_count": null, "id": "65105bdd-2d4f-4684-b7f1-a24b677b4df6", "metadata": {}, "outputs": [], "source": ["node_parser = SimpleNodeParser.from_defaults()\n", "nodes = node_parser.get_nodes_from_documents(docs)"]}, {"cell_type": "code", "execution_count": null, "id": "be838a39-2c7e-43c6-b684-e699595f63a8", "metadata": {}, "outputs": [{"data": {"text/plain": ["89"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["len(nodes)"]}, {"cell_type": "markdown", "id": "55fa821a-d459-44aa-ab38-5c74184b9cd9", "metadata": {}, "source": ["## 定义评估模块\n", "\n", "我们设置评估模块，包括数据集和评估器。\n"]}, {"cell_type": "markdown", "id": "49f83fd3-57d6-4bf2-bb28-5998e96b0e43", "metadata": {}, "source": ["### 设置“黄金数据集”\n", "\n", "这里我们加载一个“黄金”数据集。\n"]}, {"cell_type": "markdown", "id": "55ca5fba-4824-464a-b25e-8d470755e692", "metadata": {}, "source": ["#### 选项1：获取现有数据集\n", "\n", "**注意**：我们从Dropbox中获取数据集。有关如何生成数据集的详细信息，请参阅我们的`DatasetGenerator`模块。\n"]}, {"cell_type": "code", "execution_count": null, "id": "9dee93c1-9e1b-4957-83e5-d1b46ff3482d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2023-11-08 22:20:10--  https://www.dropbox.com/scl/fi/fh9vsmmm8vu0j50l3ss38/llama2_eval_qr_dataset.json?rlkey=kkoaez7aqeb4z25gzc06ak6kb&dl=1\n", "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:6057:18::a27d:d12, 162.125.13.18\n", "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:6057:18::a27d:d12|:443... connected.\n", "HTTP request sent, awaiting response... 302 Found\n", "Location: https://uc63170224c66fda29da619e304b.dl.dropboxusercontent.com/cd/0/inline/CHOj1FEf2Dd6npmREaKmwUEIJ4S5QcrgeISKh55BE27i9tqrcE94Oym_0_z0EL9mBTmF9udNCxWwnFSHlio3ib6G_f_j3xiUzn5AVvQsKDPROYjazkJz_ChUVv3xkT-Pzuk/file?dl=1# [following]\n", "--2023-11-08 22:20:11--  https://uc63170224c66fda29da619e304b.dl.dropboxusercontent.com/cd/0/inline/CHOj1FEf2Dd6npmREaKmwUEIJ4S5QcrgeISKh55BE27i9tqrcE94Oym_0_z0EL9mBTmF9udNCxWwnFSHlio3ib6G_f_j3xiUzn5AVvQsKDPROYjazkJz_ChUVv3xkT-Pzuk/file?dl=1\n", "Resolving uc63170224c66fda29da619e304b.dl.dropboxusercontent.com (uc63170224c66fda29da619e304b.dl.dropboxusercontent.com)... 2620:100:6057:15::a27d:d0f, 162.125.13.15\n", "Connecting to uc63170224c66fda29da619e304b.dl.dropboxusercontent.com (uc63170224c66fda29da619e304b.dl.dropboxusercontent.com)|2620:100:6057:15::a27d:d0f|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 60656 (59K) [application/binary]\n", "Saving to: ‘data/llama2_eval_qr_dataset.json’\n", "\n", "data/llama2_eval_qr 100%[===================>]  59.23K  --.-KB/s    in 0.02s   \n", "\n", "2023-11-08 22:20:12 (2.87 MB/s) - ‘data/llama2_eval_qr_dataset.json’ saved [60656/60656]\n"]}], "source": ["!wget \"https://www.dropbox.com/scl/fi/fh9vsmmm8vu0j50l3ss38/llama2_eval_qr_dataset.json?rlkey=kkoaez7aqeb4z25gzc06ak6kb&dl=1\" -O data/llama2_eval_qr_dataset.json"]}, {"cell_type": "code", "execution_count": null, "id": "8a256715-0b45-42c3-9218-3ca04fe07f4e", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import QueryResponseDataset", "", "# 可选", "eval_dataset = QueryResponseDataset.from_json(", "    \"data/llama2_eval_qr_dataset.json\"", ")"]}, {"cell_type": "markdown", "id": "8c5b5644-a668-4c20-af4b-cbffd8c8c6fe", "metadata": {}, "source": ["#### 选项2：生成新数据集\n", "\n", "如果选择此选项，您可以选择从头开始生成一个新的数据集。这样可以让您调整我们的`DatasetGenerator`设置，以确保它符合您的需求。\n"]}, {"cell_type": "code", "execution_count": null, "id": "5a50f429-5b8e-476d-8031-a9a6dfe5cd53", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\n", "from llama_index.llms.openai import OpenAI"]}, {"cell_type": "code", "execution_count": null, "id": "b5e09743-1781-4959-95b5-6a71d8ef676d", "metadata": {}, "outputs": [], "source": ["# 注意：如果数据集尚未保存，请运行此代码", "# 注意：我们只从前20个节点生成，因为其余的是引用", "llm = OpenAI(model=\"gpt-4-1106-preview\")", "dataset_generator = DatasetGenerator(", "    nodes[:20],", "    llm=llm,", "    show_progress=True,", "    num_questions_per_chunk=3,", ")", "eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60)", "eval_dataset.save_json(\"data/llama2_eval_qr_dataset.json\")"]}, {"cell_type": "code", "execution_count": null, "id": "63154cc9-4536-4fb4-a5ab-30bd8966e46d", "metadata": {}, "outputs": [], "source": ["# 可选", "eval_dataset = QueryResponseDataset.from_json(", "    \"data/llama2_eval_qr_dataset.json\"", ")"]}, {"cell_type": "markdown", "id": "617e4c39-2a92-455f-982d-fd565309d9e9", "metadata": {}, "source": ["### 评估模块\n", "\n", "我们定义了两个评估模块：正确性和语义相似度 - 两者都用于比较预测响应与实际响应的质量。\n"]}, {"cell_type": "code", "execution_count": null, "id": "8d6bfcc5-2ba9-4df9-9154-d0a544c0b4c3", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation.eval_utils import (\n", "    get_responses,\n", "    get_results_df,\n", ")\n", "from llama_index.core.evaluation import (\n", "    CorrectnessEvaluator,\n", "    SemanticSimilarityEvaluator,\n", "    BatchEvalRunner,\n", ")\n", "from llama_index.llms.openai import OpenAI"]}, {"cell_type": "code", "execution_count": null, "id": "cdda45f2-f97f-4083-a529-c0b924b82545", "metadata": {}, "outputs": [], "source": ["eval_llm = OpenAI(model=\"gpt-4-1106-preview\")\n", "evaluator_c = CorrectnessEvaluator(llm=eval_llm)\n", "evaluator_s = SemanticSimilarityEvaluator(llm=eval_llm)\n", "evaluator_dict = {\n", "    \"correctness\": evaluator_c,\n", "    \"semantic_similarity\": evaluator_s,\n", "}\n", "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)"]}, {"cell_type": "code", "execution_count": null, "id": "3afde820-e018-41a8-ae19-a86732f404b9", "metadata": {}, "outputs": [], "source": ["import numpy as np", "import time", "import os", "import pickle", "from tqdm import tqdm", "", "", "def get_responses_sync(", "    eval_qs, query_engine, show_progress=True, save_path=None", "):", "    if show_progress:", "        eval_qs_iter = tqdm(eval_qs)", "    else:", "        eval_qs_iter = eval_qs", "    pred_responses = []", "    start_time = time.time()", "    for eval_q in eval_qs_iter:", "        print(f\"eval q: {eval_q}\")", "        pred_response = agent.query(eval_q)", "        print(f\"predicted response: {pred_response}\")", "        pred_responses.append(pred_response)", "        if save_path is not None:", "            # save intermediate responses (to cache in case something breaks)", "            avg_time = (time.time() - start_time) / len(pred_responses)", "            pickle.dump(", "                {\"pred_responses\": pred_responses, \"avg_time\": avg_time},", "                open(save_path, \"wb\"),", "            )", "    return pred_responses", "", "", "async def run_evals(", "    query_engine,", "    eval_qa_pairs,", "    batch_runner,", "    disable_async_for_preds=False,", "    save_path=None,", "):", "    # then evaluate", "    # TODO: evaluate a sample of generated results", "    eval_qs = [q for q, _ in eval_qa_pairs]", "    eval_answers = [a for _, a in eval_qa_pairs]", "", "    if save_path is not None:", "        if not os.path.exists(save_path):", "            start_time = time.time()", "            if disable_async_for_preds:", "                pred_responses = get_responses_sync(", "                    eval_qs,", "                    query_engine,", "                    show_progress=True,", "                    save_path=save_path,", "                )", "            else:", "                pred_responses = get_responses(", "                    eval_qs, query_engine, show_progress=True", "                )", "            avg_time = (time.time() - start_time) / len(eval_qs)", "            pickle.dump(", "                {\"pred_responses\": pred_responses, \"avg_time\": avg_time},", "                open(save_path, \"wb\"),", "            )", "        else:", "            # [optional] load", "            pickled_dict = pickle.load(open(save_path, \"rb\"))", "            pred_responses = pickled_dict[\"pred_responses\"]", "            avg_time = pickled_dict[\"avg_time\"]", "    else:", "        start_time = time.time()", "        pred_responses = get_responses(", "            eval_qs, query_engine, show_progress=True", "        )", "        avg_time = (time.time() - start_time) / len(eval_qs)", "", "    eval_results = await batch_runner.aevaluate_responses(", "        eval_qs, responses=pred_responses, reference=eval_answers", "    )", "    return eval_results, {\"avg_time\": avg_time}"]}, {"cell_type": "markdown", "id": "0783a8db-5546-472a-8376-6d2774dba45a", "metadata": {}, "source": ["## 使用内置检索构建助手\n", "\n", "让我们在构建助手的同时，也将内置的OpenAI检索工具传递给它。\n", "\n", "在这里，我们在创建助手的过程中上传并传递文件。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3ac4421f-ca9e-4d9f-91e1-10e1fb1119e7", "metadata": {}, "outputs": [], "source": ["from llama_index.agent.openai import OpenAIAssistantAgent"]}, {"cell_type": "code", "execution_count": null, "id": "304c5c23-930c-4aed-8e0b-84a6e5c36138", "metadata": {}, "outputs": [], "source": ["agent = OpenAIAssistantAgent.from_new(\n", "    name=\"SEC Analyst\",\n", "    instructions=\"You are a QA assistant designed to analyze sec filings.\",\n", "    openai_tools=[{\"type\": \"retrieval\"}],\n", "    instructions_prefix=\"Please address the user as Jerry.\",\n", "    files=[\"data/llama2.pdf\"],\n", "    verbose=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "6845f8e9-ca2c-4bd1-a31a-dc58b47c585a", "metadata": {}, "outputs": [], "source": ["response = agent.query(\n", "    \"What are the key differences between Llama 2 and Llama 2-Chat?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "e1a93a9d-cc97-49c4-8a12-de710edc2fac", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The key differences between Llama 2 and Llama 2-Chat, as indicated by the document, focus on their performance in safety evaluations, particularly when tested with adversarial prompts. Here are some of the differences highlighted within the safety evaluation section of Llama 2-Chat:\n", "\n", "1. Safety Human Evaluation: Llama 2-Chat was assessed with roughly 2000 adversarial prompts, among which 1351 were single-turn and 623 were multi-turn. The responses were judged for safety violations on a five-point Likert scale, where a rating of 1 or 2 indicated a violation. The evaluation aimed to gauge the model’s safety by its rate of generating responses with safety violations and its helpfulness to users.\n", "\n", "2. Violation Percentage and Mean Rating: Llama 2-Chat exhibited a low overall violation percentage across different model sizes and a high mean rating for safety and helpfulness, which suggests a strong performance in safety evaluations.\n", "\n", "3. Inter-Rater Reliability: The reliability of the safety assessments was measured using Gwet’s AC1/2 statistic, showing a high degree of agreement among annotators with an average inter-rater reliability score of 0.92 for Llama 2-Chat annotations.\n", "\n", "4. Single-turn and Multi-turn Conversations: The evaluation revealed that multi-turn conversations generally lead to more safety violations across models, but Llama 2-Chat performed well compared to baselines, particularly in multi-turn scenarios.\n", "\n", "5. Violation Percentage Per Risk Category: Llama 2-Chat had a relatively higher number of violations in the unqualified advice category, possibly due to a lack of appropriate disclaimers in its responses.\n", "\n", "6. Improvements in Fine-Tuned Llama 2-Chat: The document also mentions that the fine-tuned Llama 2-Chat showed significant improvement over the pre-trained Llama 2 in terms of truthfulness and toxicity. The percentage of toxic generations dropped to effectively 0% for Llama 2-Chat of all sizes, which was the lowest among all compared models, indicating a notable enhancement in safety.\n", "\n", "These points detail the evaluations and improvements emphasizing safety that distinguish Llama 2-Chat from Llama 2【9†source】.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "markdown", "id": "b05e4116-1ed8-4b26-a506-8357fb4409b6", "metadata": {}, "source": ["## 基准测试\n", "\n", "我们在评估数据集上运行代理程序。我们使用gpt-4-turbo对标准的top-k RAG管道（k=2）进行基准测试。\n", "\n", "**注意**：在我们进行测试的时候（2023年11月），助手API受到严格的速率限制，生成超过60个数据点的响应可能需要大约1-2小时的时间。\n"]}, {"cell_type": "markdown", "id": "82aa9da3-5963-4809-abd1-1d0c51716250", "metadata": {}, "source": ["#### 定义基准指数 + RAG管道\n"]}, {"cell_type": "code", "execution_count": null, "id": "ec6c6c62-3922-40a8-81d7-5790f111107a", "metadata": {}, "outputs": [], "source": ["llm = OpenAI(model=\"gpt-4-1106-preview\")\n", "base_index = VectorStoreIndex(nodes)\n", "base_query_engine = base_index.as_query_engine(similarity_top_k=2, llm=llm)"]}, {"cell_type": "markdown", "id": "b7a01cd9-a956-403a-9bf7-134dcf4ec940", "metadata": {}, "source": ["#### 运行基准评估\n", "\n", "这个部分将运行基准评估，以便比较模型的性能。\n"]}, {"cell_type": "code", "execution_count": null, "id": "ede929ea-a722-467f-853e-a996af19170d", "metadata": {}, "outputs": [], "source": ["base_eval_results, base_extra_info = await run_evals(\n", "    base_query_engine,\n", "    eval_dataset.qr_pairs,\n", "    batch_runner,\n", "    save_path=\"data/llama2_preds_base.pkl\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "38c33d1c-ecf8-4a9a-a273-e785535f9c15", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>names</th>\n", "      <th>correctness</th>\n", "      <th>semantic_similarity</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Base Query Engine</td>\n", "      <td>4.05</td>\n", "      <td>0.964245</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["               names  correctness  semantic_similarity\n", "0  Base Query Engine         4.05             0.964245"]}, "metadata": {}, "output_type": "display_data"}], "source": ["results_df = get_results_df(\n", "    [base_eval_results],\n", "    [\"Base Query Engine\"],\n", "    [\"correctness\", \"semantic_similarity\"],\n", ")\n", "display(results_df)"]}, {"cell_type": "markdown", "id": "822d89be-3a39-4e9f-9c4e-0893e438d31b", "metadata": {}, "source": ["#### 运行助手API上的评估\n"]}, {"cell_type": "code", "execution_count": null, "id": "18fdc484-952e-4935-a241-b447c1440869", "metadata": {}, "outputs": [], "source": ["assistant_eval_results, assistant_extra_info = await run_evals(\n", "    agent,\n", "    eval_dataset.qr_pairs[:55],\n", "    batch_runner,\n", "    save_path=\"data/llama2_preds_assistant.pkl\",\n", "    disable_async_for_preds=True,\n", ")"]}, {"cell_type": "markdown", "id": "cb94a23e-0a9f-40d4-94f9-13dac3ff0b82", "metadata": {}, "source": ["#### 获取结果\n", "\n", "在这里我们看到……我们的基本RAG管道表现更好。\n", "\n", "对这些数字要持保留态度。这里的目标是为您提供一个脚本，以便您可以在自己的数据上运行它。\n", "\n", "也就是说，令人惊讶的是检索API并没有立即提供更好的开箱即用性能。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6d7c79bb-7f29-4c8a-9a39-53621c379d70", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>names</th>\n", "      <th>correctness</th>\n", "      <th>semantic_similarity</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Retrieval API</td>\n", "      <td>3.536364</td>\n", "      <td>0.952647</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Base Query Engine</td>\n", "      <td>4.050000</td>\n", "      <td>0.964245</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["               names  correctness  semantic_similarity\n", "0      Retrieval API     3.536364             0.952647\n", "1  Base Query Engine     4.050000             0.964245"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Base Avg Time: 0.25683316787083943\n", "Assistant Avg Time: 75.43605598536405\n"]}], "source": ["results_df = get_results_df(\n", "    [assistant_eval_results, base_eval_results],\n", "    [\"Retrieval API\", \"Base Query Engine\"],\n", "    [\"correctness\", \"semantic_similarity\"],\n", ")\n", "display(results_df)\n", "print(f\"Base Avg Time: {base_extra_info['avg_time']}\")\n", "print(f\"Assistant Avg Time: {assistant_extra_info['avg_time']}\")"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}