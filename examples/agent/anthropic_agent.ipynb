{"cells": [{"cell_type": "markdown", "id": "24103c51", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/mistral_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "99cea58c-48bc-4af6-8358-df9695659983", "metadata": {}, "source": ["# 调用函数的人类智能体\n"]}, {"cell_type": "markdown", "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e", "metadata": {}, "source": ["这个笔记本向您展示了如何使用我们的Anthropic代理，该代理由函数调用功能提供支持。\n", "\n", "**注意：** 只有claude-3模型支持使用Anthropic的API进行函数调用。\n"]}, {"cell_type": "markdown", "id": "54b7bc2e-606f-411a-9490-fcfab9236dfc", "metadata": {}, "source": ["## 初始设置\n"]}, {"cell_type": "markdown", "id": "23e80e5b-aaee-4f23-b338-7ae62b08141f", "metadata": {}, "source": ["让我们从导入一些简单的构建模块开始。\n", "\n", "我们主要需要的是：\n", "1. 人类 API（使用我们自己的 `llama_index` LLM 类）\n", "2. 一个用于保存对话历史的地方\n", "3. 我们的代理可以使用的工具定义。\n"]}, {"cell_type": "markdown", "id": "41101795", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "4985c578", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-anthropic\n", "%pip install llama-index-embeddings-openai"]}, {"cell_type": "code", "execution_count": null, "id": "c61c873d", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "9d47283b-025e-4874-88ed-76245b22f82e", "metadata": {}, "outputs": [], "source": ["from llama_index.llms.anthropic import Anthropic\n", "from llama_index.core.tools import FunctionTool\n", "\n", "import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "markdown", "id": "6fe08eb1-e638-4c00-9103-5c305bfacccf", "metadata": {}, "source": ["让我们为我们的代理定义一些非常简单的计算器工具。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3dd3c4a6-f3e0-46f9-ad3b-7ba57d1bc992", "metadata": {}, "outputs": [], "source": ["def multiply(a: int, b: int) -> int:", "    \"\"\"将两个整数相乘，并返回结果整数\"\"\"", "    return a * b", "", "", "multiply_tool = FunctionTool.from_defaults(fn=multiply)"]}, {"cell_type": "code", "execution_count": null, "id": "bfcfb78b-7d4f-48d9-8d4c-ffcded23e7ac", "metadata": {}, "outputs": [], "source": ["def add(a: int, b: int) -> int:", "    \"\"\"对两个整数进行相加，并返回结果整数\"\"\"", "    return a + b", "", "", "add_tool = FunctionTool.from_defaults(fn=add)"]}, {"cell_type": "markdown", "id": "eeac7d4c-58fd-42a5-9da9-c258375c61a0", "metadata": {}, "source": ["确保你的ANTHROPIC_API_KEY已设置。否则，请明确指定`api_key`参数。\n"]}, {"cell_type": "code", "execution_count": null, "id": "4becf171-6632-42e5-bdec-918a00934696", "metadata": {}, "outputs": [], "source": ["llm = Anthropic(model=\"claude-3-opus-20240229\", api_key=\"sk-ant-...\")"]}, {"cell_type": "markdown", "id": "707d30b8-6405-4187-a9ed-6146dcc42167", "metadata": {}, "source": ["## 初始化人类智能体\n"]}, {"cell_type": "markdown", "id": "798ca3fd-6711-4c0c-a853-d868dd14b484", "metadata": {}, "source": ["在这里，我们使用计算器函数初始化了一个简单的Mistral代理。\n"]}, {"cell_type": "code", "execution_count": null, "id": "38ab3938-1138-43ea-b085-f430b42f5377", "metadata": {}, "outputs": [], "source": ["from llama_index.core.agent import FunctionCallingAgentWorker\n", "\n", "agent_worker = FunctionCallingAgentWorker.from_tools(\n", "    [multiply_tool, add_tool],\n", "    llm=llm,\n", "    verbose=True,\n", "    allow_parallel_tool_calls=False,\n", ")\n", "agent = agent_worker.as_agent()"]}, {"cell_type": "markdown", "id": "500cbee4", "metadata": {}, "source": ["这是一个用Python编写的简单聊天程序。\n"]}, {"cell_type": "code", "execution_count": null, "id": "9450401d-769f-46e8-8bab-0f27f7362f5d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Added user message to memory: What is (121 + 2) * 5?\n", "=== Calling Function ===\n", "Calling function: add with args: {\"a\": 121, \"b\": 2}\n", "=== Calling Function ===\n", "Calling function: multiply with args: {\"a\": 123, \"b\": 5}\n", "assistant: Therefore, (121 + 2) * 5 = 615\n"]}], "source": ["response = agent.chat(\"What is (121 + 2) * 5?\")\n", "print(str(response))"]}, {"cell_type": "code", "execution_count": null, "id": "538bf32f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[ToolOutput(content='123', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 121, 'b': 2}}, raw_output=123), ToolOutput(content='615', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 123, 'b': 5}}, raw_output=615)]\n"]}], "source": ["# 检查数据源", "print(response.sources)"]}, {"cell_type": "markdown", "id": "fb33983c", "metadata": {}, "source": ["### 异步聊天\n", "\n", "同时，让我们重新启用并行函数调用，这样我们就可以同时调用两个`multiply`操作。\n"]}, {"cell_type": "code", "execution_count": null, "id": "1d1fc974", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Added user message to memory: What is (121 * 3) + (5 * 8)?\n", "=== Calling Function ===\n", "Calling function: multiply with args: {\"a\": 121, \"b\": 3}\n", "=== Calling Function ===\n", "Calling function: multiply with args: {\"a\": 5, \"b\": 8}\n", "=== Calling Function ===\n", "Calling function: add with args: {\"a\": 363, \"b\": 40}\n", "assistant: Therefore, the result of (121 * 3) + (5 * 8) is 403.\n"]}], "source": ["# 启用并行函数调用", "agent_worker = FunctionCallingAgentWorker.from_tools(", "    [multiply_tool, add_tool],", "    llm=llm,", "    verbose=True,", "    allow_parallel_tool_calls=True,", ")", "agent = agent_worker.as_agent()", "response = await agent.achat(\"What is (121 * 3) + (5 * 8)?\")", "print(str(response))"]}, {"cell_type": "markdown", "id": "cabfdf01-8d63-43ff-b06e-a3059ede2ddf", "metadata": {}, "source": ["## 基于RAG管道的Anthropic代理\n", "\n", "在一个简单的10K文档上构建一个Anthropic代理。我们使用OpenAI嵌入和claude-3-haiku-20240307来构建RAG管道，并将其传递给Anthropic Opus代理作为工具。\n"]}, {"cell_type": "code", "execution_count": null, "id": "48120dd4-7f50-426f-bc7e-a903e090d32e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2024-04-04 18:12:42--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 1880483 (1.8M) [application/octet-stream]\n", "Saving to: ‘data/10k/uber_2021.pdf’\n", "\n", "data/10k/uber_2021. 100%[===================>]   1.79M  6.09MB/s    in 0.3s    \n", "\n", "2024-04-04 18:12:43 (6.09 MB/s) - ‘data/10k/uber_2021.pdf’ saved [1880483/1880483]\n", "\n"]}], "source": ["!mkdir -p 'data/10k/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'"]}, {"cell_type": "code", "execution_count": null, "id": "48c0cf98-3f10-4599-8437-d88dc89cefad", "metadata": {}, "outputs": [], "source": ["from llama_index.core.tools import QueryEngineTool, ToolMetadata", "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex", "from llama_index.embeddings.openai import OpenAIEmbedding", "from llama_index.llms.anthropic import Anthropic", "", "embed_model = OpenAIEmbedding(api_key=\"sk-...\")", "query_llm = Anthropic(model=\"claude-3-haiku-20240307\", api_key=\"sk-ant-...\")", "", "# 加载数据", "uber_docs = SimpleDirectoryReader(", "    input_files=[\"./data/10k/uber_2021.pdf\"]", ").load_data()", "# 构建索引", "uber_index = VectorStoreIndex.from_documents(", "    uber_docs, embed_model=embed_model", ")", "uber_engine = uber_index.as_query_engine(similarity_top_k=3, llm=query_llm)", "query_engine_tool = QueryEngineTool(", "    query_engine=uber_engine,", "    metadata=ToolMetadata(", "        name=\"uber_10k\",", "        description=(", "            \"提供2021年Uber财务信息。\"", "            \"使用详细的纯文本问题作为工具的输入。\"", "        ),", "    ),", ")"]}, {"cell_type": "code", "execution_count": null, "id": "ebfdaf80-e5e1-4c60-b556-20558da3d5e3", "metadata": {}, "outputs": [], "source": ["from llama_index.core.agent import FunctionCallingAgentWorker\n", "\n", "agent_worker = FunctionCallingAgentWorker.from_tools(\n", "    [query_engine_tool], llm=llm, verbose=True\n", ")\n", "agent = agent_worker.as_agent()"]}, {"cell_type": "code", "execution_count": null, "id": "58c53f2a-0a3f-4abe-b8b6-97a974ec7546", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Added user message to memory: Tell me both the risk factors and tailwinds for Uber?\n", "=== Calling Function ===\n", "Calling function: uber_10k with args: {\"input\": \"What were some of the key risk factors and tailwinds mentioned for Uber's business in 2021?\"}\n", "assistant: In summary, some of the key risk factors Uber faced in 2021 included regulatory challenges, IP protection, staying competitive with new technologies, seasonality and forecasting challenges due to COVID-19, and risks of international expansion. However, Uber also benefited from tailwinds like accelerated growth in food delivery due to the pandemic and adapting well to new remote work arrangements.\n"]}], "source": ["response = agent.chat(\"Tell me both the risk factors and tailwinds for Uber?\")\n", "print(str(response))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}