{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "88216e84", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/openai_agent_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "99cea58c-48bc-4af6-8358-df9695659983", "metadata": {}, "source": ["# 检索增强型 OpenAI 代理\n", "\n", "这是一个用于演示 OpenAI GPT-3 模型的示例，该模型已经通过检索增强技术进行了增强。在这个示例中，我们将展示如何使用检索增强技术来改进对话系统的性能。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e", "metadata": {}, "source": ["在本教程中，我们将向您展示如何使用我们的`OpenAIAgent`实现与工具检索器，以构建一个基于OpenAI的函数API的代理，并存储/索引任意数量的工具。我们的索引/检索模块有助于消除由于函数过多而无法适应提示的复杂性。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "54b7bc2e-606f-411a-9490-fcfab9236dfc", "metadata": {}, "source": ["## 初始设置\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "23e80e5b-aaee-4f23-b338-7ae62b08141f", "metadata": {}, "source": ["让我们从导入一些简单的基本模块开始。\n", "\n", "我们主要需要的是：\n", "1. OpenAI API\n", "2. 一个用于保存对话历史记录的地方\n", "3. 代理程序可以使用的工具定义。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "4ce34f4d", "metadata": {}, "source": ["如果您在Colab上打开此笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "51441848", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-agent-openai-legacy"]}, {"cell_type": "code", "execution_count": null, "id": "346fb488", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "9d47283b-025e-4874-88ed-76245b22f82e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/suo/miniconda3/envs/llama/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.7) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n", "  warnings.warn(\n"]}], "source": ["import json\n", "from typing import Sequence\n", "\n", "from llama_index.core.tools import BaseTool, FunctionTool"]}, {"attachments": {}, "cell_type": "markdown", "id": "6fe08eb1-e638-4c00-9103-5c305bfacccf", "metadata": {}, "source": ["让我们为我们的代理人定义一些非常简单的计算器工具。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3dd3c4a6-f3e0-46f9-ad3b-7ba57d1bc992", "metadata": {}, "outputs": [], "source": ["def multiply(a: int, b: int) -> int:", "    \"\"\"将两个整数相乘，并返回结果整数\"\"\"", "    return a * b", "", "", "def add(a: int, b: int) -> int:", "    \"\"\"将两个整数相加，并返回结果整数\"\"\"", "    return a + b", "", "", "def useless(a: int, b: int) -> int:", "    \"\"\"无用的玩具函数。\"\"\"", "    pass", "", "", "multiply_tool = FunctionTool.from_defaults(fn=multiply, name=\"multiply\")", "useless_tools = [", "    FunctionTool.from_defaults(fn=useless, name=f\"useless_{str(idx)}\")", "    for idx in range(28)", "]", "add_tool = FunctionTool.from_defaults(fn=add, name=\"add\")", "", "all_tools = [multiply_tool] + [add_tool] + useless_tools", "all_tools_map = {t.metadata.name: t for t in all_tools}"]}, {"attachments": {}, "cell_type": "markdown", "id": "8170dd32-fa00-458c-aeb5-37292d0773c0", "metadata": {}, "source": ["## 构建对象索引\n", "\n", "在LlamaIndex中，我们有一个名为`ObjectIndex`的构造，允许用户在任意对象上使用我们的索引数据结构。\n", "ObjectIndex将处理对象的序列化和反序列化，并使用底层索引（例如VectorStoreIndex、SummaryIndex、KeywordTableIndex）作为存储机制。\n", "\n", "在这种情况下，我们有一个大型的Tool对象集合，并且希望在这些Tool上定义一个ObjectIndex。\n", "\n", "该索引捆绑了一个检索机制，即`ObjectRetriever`。\n", "\n", "这可以传递给我们的代理，以便在查询时执行Tool检索。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6704a755-7f05-43a3-8a56-f5f587ae4c40", "metadata": {}, "outputs": [], "source": ["# 在这些工具上定义一个“对象”索引", "from llama_index.core import VectorStoreIndex", "from llama_index.core.objects import ObjectIndex", "", "obj_index = ObjectIndex.from_objects(", "    all_tools,", "    index_cls=VectorStoreIndex,", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "707d30b8-6405-4187-a9ed-6146dcc42167", "metadata": {}, "source": ["## 使用工具检索的OpenAIAgent\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "798ca3fd-6711-4c0c-a853-d868dd14b484", "metadata": {}, "source": ["我们在LlamaIndex中提供了一个`OpenAIAgent`实现，它可以接受一组`BaseTool`对象上的`ObjectRetriever`。\n", "\n", "在查询时，我们首先会使用`ObjectRetriever`来检索一组相关的工具。然后这些工具将被传递给agent；更具体地说，它们的函数签名将被传递给OpenAI函数调用API。\n"]}, {"cell_type": "code", "execution_count": null, "id": "38ab3938-1138-43ea-b085-f430b42f5377", "metadata": {}, "outputs": [], "source": ["from llama_index.agent.openai import OpenAIAgent"]}, {"cell_type": "code", "execution_count": null, "id": "d852ece7-e5a1-4368-9d59-c7014e0b5b4d", "metadata": {}, "outputs": [], "source": ["agent = OpenAIAgent.from_tools(\n", "    tool_retriever=obj_index.as_retriever(similarity_top_k=2), verbose=True\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "33ea069f-819b-4ec1-a93c-fcbaacb362a1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["=== Calling Function ===\n", "Calling function: multiply with args: {\n", "  \"a\": 212,\n", "  \"b\": 122\n", "}\n", "Got output: 25864\n", "========================\n"]}, {"data": {"text/plain": ["Response(response='212 multiplied by 122 is 25,864.', source_nodes=[], metadata=None)"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["agent.chat(\"What's 212 multiplied by 122? Make sure to use Tools\")"]}, {"cell_type": "code", "execution_count": null, "id": "ec423b90-59cd-40ef-b497-a3842b3e7b58", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["=== Calling Function ===\n", "Calling function: add with args: {\n", "  \"a\": 212,\n", "  \"b\": 122\n", "}\n", "Got output: 334\n", "========================\n"]}, {"data": {"text/plain": ["Response(response='212 added to 122 is 334.', source_nodes=[], metadata=None)"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["agent.chat(\"What's 212 added to 122 ? Make sure to use Tools\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}