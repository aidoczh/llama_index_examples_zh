{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/EpsillaIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Epsilla向量存储\n", "在这个笔记本中，我们将展示如何使用[Epsilla](https://www.epsilla.com/)在LlamaIndex中执行向量搜索。\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["作为先决条件，您需要运行 Epsilla 向量数据库（例如，通过我们的 Docker 镜像），并安装 ``pyepsilla`` 包。\n", "可在 [文档](https://epsilla-inc.gitbook.io/epsilladb/quick-start) 中查看完整文档。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-vector-stores-epsilla"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip/pip3 install pyepsilla"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging", "import sys", "", "# 取消注释以查看调试日志", "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)", "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))", "", "from llama_index.core import SimpleDirectoryReader, Document, StorageContext", "from llama_index.core import VectorStoreIndex", "from llama_index.vector_stores.epsilla import EpsillaVectorStore", "import textwrap"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 设置OpenAI\n", "首先，让我们添加OpenAI API密钥。它将用于为加载到索引中的文档创建嵌入。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import openai\n", "import getpass\n", "\n", "OPENAI_API_KEY = getpass.getpass(\"OpenAI API Key:\")\n", "openai.api_key = OPENAI_API_KEY"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["```python\n", "# 下载数据\n", "```\n", "\n", "在这个部分，我们将下载所需的数据。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 加载文档\n", "使用SimpleDirectoryReader加载存储在`/data/paul_graham`文件夹中的文档。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Total documents: 1\n", "First document, id: ac7f23f0-ce15-4d94-a0a2-5020fa87df61\n", "First document, hash: 4c702b4df575421e1d1af4b1fd50511b226e0c9863dbfffeccb8b689b8448f35\n"]}], "source": ["# 加载文档", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()", "print(f\"总文档数：{len(documents)}\")", "print(f\"第一个文档，id：{documents[0].doc_id}\")", "print(f\"第一个文档，哈希值：{documents[0].hash}\")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 创建索引\n", "在这里，我们使用之前加载的文档创建一个由Epsilla支持的索引。EpsillaVectorStore接受一些参数。\n", "- client (Any): 用于连接的Epsilla客户端。\n", "\n", "- collection_name (str, optional): 要使用的集合。默认为\"llama_collection\"。\n", "- db_path (str, optional): 数据库将被持久化的路径。默认为\"/tmp/langchain-epsilla\"。\n", "- db_name (str, optional): 给加载的数据库一个名称。默认为\"langchain_store\"。\n", "- dimension (int, optional): 嵌入的维度。如果未提供，将在第一次插入时创建集合。默认为None。\n", "- overwrite (bool, optional): 是否覆盖同名的现有集合。默认为False。\n", "\n", "Epsilla vectordb正在使用默认主机\"localhost\"和端口\"8888\"运行。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[INFO] Connected to localhost:8888 successfully.\n"]}], "source": ["# 在文档上创建索引", "from pyepsilla import vectordb", "", "client = vectordb.Client()", "vector_store = EpsillaVectorStore(client=client, db_path=\"/tmp/llamastore\")", "", "storage_context = StorageContext.from_defaults(vector_store=vector_store)", "index = VectorStoreIndex.from_documents(", "    documents, storage_context=storage_context", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 查询数据\n", "现在我们已经将文档存储在索引中，我们可以针对索引提出问题。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author of the given context information is Paul Graham.\n"]}], "source": ["query_engine = index.as_query_engine()\n", "response = query_engine.query(\"Who is the author?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author learned about AI through various sources. One source was a novel called \"The Moon is a\n", "Harsh Mistress\" by Heinlein, which featured an intelligent computer called Mike. Another source was\n", "a PBS documentary that showed Terry Winograd using SHRDLU, a program that could understand natural\n", "language. These experiences sparked the author's interest in AI and motivated them to start learning\n", "about it, including teaching themselves Lisp, which was regarded as the language of AI at the time.\n"]}], "source": ["response = query_engine.query(\"How did the author learn about AI?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["接下来，让我们尝试覆盖之前的数据。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["There is no information provided about the author in the given context.\n"]}], "source": ["vector_store = EpsillaVectorStore(client=client, overwrite=True)\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n", "single_doc = Document(text=\"Epsilla is the vector database we are using.\")\n", "index = VectorStoreIndex.from_documents(\n", "    [single_doc],\n", "    storage_context=storage_context,\n", ")\n", "\n", "query_engine = index.as_query_engine()\n", "response = query_engine.query(\"Who is the author?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epsilla is the vector database being used.\n"]}], "source": ["response = query_engine.query(\"What vector database is being used?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["接下来，让我们向现有集合添加更多数据。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author of the given context information is Paul Graham.\n"]}], "source": ["vector_store = EpsillaVectorStore(client=client, overwrite=False)\n", "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n", "for doc in documents:\n", "    index.insert(document=doc)\n", "\n", "query_engine = index.as_query_engine()\n", "response = query_engine.query(\"Who is the author?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epsilla is the vector database being used.\n"]}], "source": ["response = query_engine.query(\"What vector database is being used?\")\n", "print(textwrap.fill(str(response), 100))"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}