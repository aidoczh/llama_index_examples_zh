{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/qdrant_hybrid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Qdrant混合搜索\n", "\n", "Qdrant支持通过结合`稀疏`和`密集`向量的搜索结果来实现混合搜索。\n", "\n", "`密集`向量可能是您已经在使用的一种 -- 来自OpenAI、BGE、SentenceTransformers等的嵌入模型通常是`密集`嵌入模型。它们会创建文本的数值表示，表示为一长串数字。这些`密集`向量可以捕捉整个文本的丰富语义信息。\n", "\n", "`稀疏`向量略有不同。它们使用专门的方法或模型（如TF-IDF、BM25、SPLADE等）来生成向量。这些向量通常大部分是零，使它们成为`稀疏`向量。这些`稀疏`向量非常擅长捕捉特定关键词和类似的细节。\n", "\n", "本笔记将介绍如何使用Qdrant和来自Huggingface的`\"prithvida/Splade_PP_en_v1\"`变种来设置和定制混合搜索。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 设置\n", "\n", "首先，我们设置环境并加载数据。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install -U llama-index llama-index-vector-stores-qdrant fastembed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/'\n", "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "documents = SimpleDirectoryReader(\"./data/\").load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 索引数据\n", "\n", "现在，我们可以对数据进行索引。\n", "\n", "使用Qdrant进行混合搜索必须从一开始就启用 -- 我们可以简单地设置 `enable_hybrid=True`。\n", "\n", "这将在本地使用 `\"prithvida/Splade_PP_en_v1\"` 和 fastembed 运行稀疏向量生成，同时使用OpenAI生成密集向量。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Both client and aclient are provided. If using `:memory:` mode, the data between clients is not synced.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c6b8fd0680504f10b5b83c8bc94a5f8c", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "06b8964d910b4adc8864b38b5f14a4cc", "version_major": 2, "version_minor": 0}, "text/plain": [".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "38a8a07edf454ef8866bb7151503e007", "version_major": 2, "version_minor": 0}, "text/plain": ["generation_config.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "94053bb2d9004ab89e1c0e9533106fd0", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "ef485a7bc74c41cf816db14d69aa5dbe", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "623b7b6461ac49d8ae55755f98bd9c3d", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b539d2c1490b43de858923f9b3e80bd6", "version_major": 2, "version_minor": 0}, "text/plain": ["README.md:   0%|          | 0.00/133 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6fdf5c72a8954a39b774c91554eb11e2", "version_major": 2, "version_minor": 0}, "text/plain": ["model.onnx:   0%|          | 0.00/532M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "e5b675b999ef4c84bc859aea78f97b01", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "33081a9b77464c8c899361ce05297372", "version_major": 2, "version_minor": 0}, "text/plain": ["special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6b88789a16b443c38ad8e5376ef9510c", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from llama_index.core import VectorStoreIndex,StorageContext\n", "from llama_index.core import Settings\n", "from llama_index.vector_stores.qdrant import QdrantVectorStore\n", "from qdrant_client import QdrantClient,AsyncQdrantClient\n", "\n", "# 创建一个持久化索引到磁盘\n", "client = QdrantClient(host=\"localhost\", port=6333)\n", "aclient = AsyncQdrantClient(host=\"localhost\", port=6333)\n", "\n", "# 创建一个启用混合索引的向量存储\n", "# batch_size控制一次编码稀疏向量的节点数量\n", "vector_store = QdrantVectorStore(\n", "    \"llama2_paper\",\n", "    client=client,\n", "    aclient=aclient,\n", "    enable_hybrid=True,\n", "    batch_size=20,\n", ")\n", "\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n", "Settings.chunk_size = 512\n", "\n", "index = VectorStoreIndex.from_documents(\n", "    documents,\n", "    storage_context=storage_context,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 混合查询\n", "\n", "在使用混合模式查询时，我们可以分别设置 `similarity_top_k` 和 `sparse_top_k`。\n", "\n", "`sparse_top_k` 表示将从每个稠密和稀疏查询中检索多少个节点。例如，如果设置 `sparse_top_k=5`，这意味着将使用稀疏向量检索 5 个节点，并使用稠密向量检索 5 个节点。\n", "\n", "`similarity_top_k` 控制最终返回的节点数量。在上述设置中，我们最终得到 10 个节点。应用融合算法来对来自不同向量空间的节点进行排名和排序（在本例中是[相对分数融合](https://weaviate.io/blog/hybrid-search-fusion-algorithms#relative-score-fusion)）。`similarity_top_k=2` 表示融合后返回前两个节点。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(\n", "    similarity_top_k=2, sparse_top_k=12, vector_store_query_mode=\"hybrid\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/markdown": ["Llama 2 was specifically trained differently from Llama 1 by making changes such as performing more robust data cleaning, updating data mixes, training on 40% more total tokens, doubling the context length, and using grouped-query attention (GQA) to improve inference scalability for larger models. Additionally, Llama 2 adopted most of the pretraining setting and model architecture from Llama 1 but included architectural enhancements like increased context length and grouped-query attention."], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from IPython.display import display, Markdown\n", "\n", "response = query_engine.query(\n", "    \"How was Llama2 specifically trained differently from Llama1?\"\n", ")\n", "\n", "display(Markdown(str(response)))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["2\n"]}], "source": ["print(len(response.source_nodes))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们来比较一下完全不使用混合搜索的情况！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/markdown": ["Llama 2 was specifically trained differently from Llama 1 by making changes to improve performance, such as performing more robust data cleaning, updating data mixes, training on 40% more total tokens, doubling the context length, and using grouped-query attention (GQA) to improve inference scalability for larger models."], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from IPython.display import display, Markdown\n", "\n", "query_engine = index.as_query_engine(\n", "    similarity_top_k=2,\n", "    # sparse_top_k=10,\n", "    # vector_store_query_mode=\"hybrid\"\n", ")\n", "\n", "response = query_engine.query(\n", "    \"How was Llama2 specifically trained differently from Llama1?\"\n", ")\n", "display(Markdown(str(response)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 异步支持\n", "\n", "当然，Qdrant也支持异步查询（请注意，内存中的Qdrant数据不会在异步和同步客户端之间共享！）\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, StorageContext\n", "from llama_index.core import Settings\n", "from llama_index.vector_stores.qdrant import QdrantVectorStore\n", "\n", "\n", "# 创建具有混合索引功能的向量存储\n", "vector_store = QdrantVectorStore(\n", "    collection_name=\"llama2_paper\",\n", "    client=client,\n", "    aclient=aclient,\n", "    enable_hybrid=True,\n", "    batch_size=20,\n", ")\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n", "Settings.chunk_size = 512\n", "\n", "index = VectorStoreIndex.from_documents(\n", "    documents,\n", "    storage_context=storage_context,\n", "    use_async=True,\n", ")\n", "\n", "query_engine = index.as_query_engine(similarity_top_k=2, sparse_top_k=10)\n", "\n", "response = await query_engine.aquery(\n", "    \"What baseline models are measured against in the paper?\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## [高级] 使用Qdrant自定义混合搜索\n", "\n", "在本节中，我们将介绍可以用来完全定制混合搜索体验的各种设置。\n", "\n", "### 自定义稀疏向量生成\n", "\n", "稀疏向量可以使用单个模型进行生成，或者有时使用不同的模型来处理查询和文档。在这里，我们使用两个模型 -- `\"naver/efficient-splade-VI-BT-large-doc\"` 和 `\"naver/efficient-splade-VI-BT-large-query\"`。\n", "\n", "以下是用于生成稀疏向量的示例代码，以及如何在构造函数中设置功能。您可以使用这个示例，并根据需要进行定制。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from typing import Any, List, Tuple\n", "import torch\n", "from transformers import AutoTokenizer, AutoModelForMaskedLM\n", "\n", "doc_tokenizer = AutoTokenizer.from_pretrained(\n", "    \"naver/efficient-splade-VI-BT-large-doc\"\n", ")\n", "doc_model = AutoModelForMaskedLM.from_pretrained(\n", "    \"naver/efficient-splade-VI-BT-large-doc\"\n", ")\n", "\n", "query_tokenizer = AutoTokenizer.from_pretrained(\n", "    \"naver/efficient-splade-VI-BT-large-query\"\n", ")\n", "query_model = AutoModelForMaskedLM.from_pretrained(\n", "    \"naver/efficient-splade-VI-BT-large-query\"\n", ")\n", "\n", "\n", "def sparse_doc_vectors(\n", "    texts: List[str],\n", ") -> Tuple[List[List[int]], List[List[float]]]:\n", "    \"\"\"\n", "    使用ReLU、log和max操作从logits和attention mask计算向量。\n", "    \"\"\"\n", "    tokens = doc_tokenizer(\n", "        texts, truncation=True, padding=True, return_tensors=\"pt\"\n", "    )\n", "    if torch.cuda.is_available():\n", "        tokens = tokens.to(\"cuda\")\n", "\n", "    output = doc_model(**tokens)\n", "    logits, attention_mask = output.logits, tokens.attention_mask\n", "    relu_log = torch.log(1 + torch.relu(logits))\n", "    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n", "    tvecs, _ = torch.max(weighted_log, dim=1)\n", "\n", "    # 提取非零向量及其索引\n", "    indices = []\n", "    vecs = []\n", "    for batch in tvecs:\n", "        indices.append(batch.nonzero(as_tuple=True)[0].tolist())\n", "        vecs.append(batch[indices[-1]].tolist())\n", "\n", "    return indices, vecs\n", "\n", "\n", "def sparse_query_vectors(\n", "    texts: List[str],\n", ") -> Tuple[List[List[int]], List[List[float]]]:\n", "    \"\"\"\n", "    使用ReLU、log和max操作从logits和attention mask计算向量。\n", "    \"\"\"\n", "    # TODO: 如果超出最大长度，则分批计算稀疏向量\n", "    tokens = query_tokenizer(\n", "        texts, truncation=True, padding=True, return_tensors=\"pt\"\n", "    )\n", "    if torch.cuda.is_available():\n", "        tokens = tokens.to(\"cuda\")\n", "\n", "    output = query_model(**tokens)\n", "    logits, attention_mask = output.logits, tokens.attention_mask\n", "    relu_log = torch.log(1 + torch.relu(logits))\n", "    weighted_log = relu_log * attention_mask.unsqueeze(-1)\n", "    tvecs, _ = torch.max(weighted_log, dim=1)\n", "\n", "    # 提取非零向量及其索引\n", "    indices = []\n", "    vecs = []\n", "    for batch in tvecs:\n", "        indices.append(batch.nonzero(as_tuple=True)[0].tolist())\n", "        vecs.append(batch[indices[-1]].tolist())\n", "\n", "    return indices, vecs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vector_store = QdrantVectorStore(\n", "    \"llama2_paper\",\n", "    client=client,\n", "    enable_hybrid=True,\n", "    sparse_doc_fn=sparse_doc_vectors,\n", "    sparse_query_fn=sparse_query_vectors,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 自定义 `hybrid_fusion_fn()`\n", "\n", "默认情况下，在使用 Qdrant 运行混合查询时，会使用相对分数融合来合并从稀疏查询和密集查询中检索到的节点。\n", "\n", "您可以自定义此函数为任何其他方法（如简单去重、倒数排名融合等）。\n", "\n", "以下是我们相对分数融合方法的默认代码以及如何将其传递给构造函数。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.vector_stores import VectorStoreQueryResult\n", "\n", "\n", "def relative_score_fusion(\n", "    dense_result: VectorStoreQueryResult,\n", "    sparse_result: VectorStoreQueryResult,\n", "    alpha: float = 0.5,  # 从查询引擎传入\n", "    top_k: int = 2,  # 从查询引擎传入，例如 similarity_top_k\n", ") -> VectorStoreQueryResult:\n", "    \"\"\"\n", "    使用相对分数融合方法融合稠密和稀疏结果。\n", "    \"\"\"\n", "    # 检查结果是否合理\n", "    assert dense_result.nodes is not None\n", "    assert dense_result.similarities is not None\n", "    assert sparse_result.nodes is not None\n", "    assert sparse_result.similarities is not None\n", "\n", "    # 拆解结果\n", "    sparse_result_tuples = list(\n", "        zip(sparse_result.similarities, sparse_result.nodes)\n", "    )\n", "    sparse_result_tuples.sort(key=lambda x: x[0], reverse=True)\n", "\n", "    dense_result_tuples = list(\n", "        zip(dense_result.similarities, dense_result.nodes)\n", "    )\n", "    dense_result_tuples.sort(key=lambda x: x[0], reverse=True)\n", "\n", "    # 跟踪两个结果中的节点\n", "    all_nodes_dict = {x.node_id: x for x in dense_result.nodes}\n", "    for node in sparse_result.nodes:\n", "        if node.node_id not in all_nodes_dict:\n", "            all_nodes_dict[node.node_id] = node\n", "\n", "    # 将稀疏相似度归一化到0到1\n", "    sparse_similarities = [x[0] for x in sparse_result_tuples]\n", "    max_sparse_sim = max(sparse_similarities)\n", "    min_sparse_sim = min(sparse_similarities)\n", "    sparse_similarities = [\n", "        (x - min_sparse_sim) / (max_sparse_sim - min_sparse_sim)\n", "        for x in sparse_similarities\n", "    ]\n", "    sparse_per_node = {\n", "        sparse_result_tuples[i][1].node_id: x\n", "        for i, x in enumerate(sparse_similarities)\n", "    }\n", "\n", "    # 将稠密相似度归一化到0到1\n", "    dense_similarities = [x[0] for x in dense_result_tuples]\n", "    max_dense_sim = max(dense_similarities)\n", "    min_dense_sim = min(dense_similarities)\n", "    dense_similarities = [\n", "        (x - min_dense_sim) / (max_dense_sim - min_dense_sim)\n", "        for x in dense_similarities\n", "    ]\n", "    dense_per_node = {\n", "        dense_result_tuples[i][1].node_id: x\n", "        for i, x in enumerate(dense_similarities)\n", "    }\n", "\n", "    # 融合分数\n", "    fused_similarities = []\n", "    for node_id in all_nodes_dict:\n", "        sparse_sim = sparse_per_node.get(node_id, 0)\n", "        dense_sim = dense_per_node.get(node_id, 0)\n", "        fused_sim = alpha * (sparse_sim + dense_sim)\n", "        fused_similarities.append((fused_sim, all_nodes_dict[node_id]))\n", "\n", "    fused_similarities.sort(key=lambda x: x[0], reverse=True)\n", "    fused_similarities = fused_similarities[:top_k]\n", "\n", "    # 创建最终的响应对象\n", "    return VectorStoreQueryResult(\n", "        nodes=[x[1] for x in fused_similarities],\n", "        similarities=[x[0] for x in fused_similarities],\n", "        ids=[x[1].node_id for x in fused_similarities],\n", "    )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vector_store = QdrantVectorStore(\n", "    \"llama2_paper\",\n", "    client=client,\n", "    enable_hybrid=True,\n", "    hybrid_fusion_fn=relative_score_fusion,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["您可能已经注意到了上面函数中的alpha参数。这可以直接在`as_query_engine()`调用中设置，这将在向量索引检索器中进行设置。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["index.as_query_engine(alpha=0.5, similarity_top_k=2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 自定义混合Qdrant集合\n", "\n", "除了让llama-index来做，您也可以预先配置您的Qdrant混合集合。\n", "\n", "**注意：** 如果创建混合索引，向量配置的名称必须是`text-dense`和`text-sparse`。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from qdrant_client import models\n", "\n", "client.recreate_collection(\n", "    collection_name=\"llama2_paper\",\n", "    vectors_config={\n", "        \"text-dense\": models.VectorParams(\n", "            size=1536,  # openai向量大小\n", "            distance=models.Distance.COSINE,\n", "        )\n", "    },\n", "    sparse_vectors_config={\n", "        \"text-sparse\": models.SparseVectorParams(\n", "            index=models.SparseIndexParams()\n", "        )\n", "    },\n", ")\n", "\n", "# 由于我们创建了一个稀疏集合，因此启用混合模式\n", "vector_store = QdrantVectorStore(\n", "    collection_name=\"llama2_paper\", client=client, enable_hybrid=True\n", ")"]}], "metadata": {"kernelspec": {"display_name": "llama-index-4a-wkI5X-py3.11", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}