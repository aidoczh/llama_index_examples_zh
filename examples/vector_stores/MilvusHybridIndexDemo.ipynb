{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "1496f9de", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/MilvusIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "0b692c73", "metadata": {}, "source": ["# 使用混合检索的Milvus向量存储\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "1e7787c2", "metadata": {}, "source": ["在这个笔记本中，我们将展示如何快速使用MilvusVectorStore进行混合检索的演示。（Milvus版本应该高于2.4.0）\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "f81e2c81", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3e0c18ca", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-vector-stores-milvus"]}, {"cell_type": "markdown", "id": "df3ddfc5", "metadata": {}, "source": ["BGE-M3是FlagEmbedding中默认的稀疏嵌入方法，因此在安装llama-index时需要一并安装。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6b80700a", "metadata": {}, "outputs": [], "source": ["! pip install llama-index\n", "! pip install FlagEmbedding"]}, {"cell_type": "code", "execution_count": null, "id": "47264e32", "metadata": {}, "outputs": [], "source": ["import logging\n", "import sys\n", "\n", "# 取消注释以查看调试日志\n", "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n", "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n", "from llama_index.vector_stores.milvus import MilvusVectorStore\n", "from IPython.display import Markdown, display\n", "import textwrap"]}, {"attachments": {}, "cell_type": "markdown", "id": "f9b97a89", "metadata": {}, "source": ["### 设置OpenAI\n", "让我们首先添加OpenAI API密钥。这将允许我们访问OpenAI以获取嵌入和使用ChatGPT。\n"]}, {"cell_type": "code", "execution_count": null, "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84", "metadata": {}, "outputs": [], "source": ["import openai\n", "\n", "openai.api_key = \"sk-\""]}, {"attachments": {}, "cell_type": "markdown", "id": "a3d4e638", "metadata": {}, "source": ["### 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "2a2e24d1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2024-04-25 17:44:59--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 75042 (73K) [text/plain]\n", "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n", "\n", "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.07s   \n", "\n", "2024-04-25 17:45:00 (994 KB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n", "\n"]}], "source": ["! mkdir -p 'data/paul_graham/'\n", "! wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "id": "59ff935d", "metadata": {}, "source": ["### 生成我们的数据\n", "有了我们的LLM集合，让我们开始使用Milvus索引。作为第一个例子，让我们从`data/paul_graham/`文件夹中的文件生成一个文档。在这个文件夹中，有一篇来自Paul Graham的单篇文章，标题为`What I Worked On`。为了生成这些文档，我们将使用SimpleDirectoryReader。\n"]}, {"cell_type": "code", "execution_count": null, "id": "68cbd239-880e-41a3-98d8-dbb3fab55431", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Document ID: ca3f5dbc-f772-41da-9a4f-bb4884691793\n"]}], "source": ["# 加载文档\n", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n", "\n", "print(\"文档ID:\", documents[0].doc_id)"]}, {"attachments": {}, "cell_type": "markdown", "id": "dd270925", "metadata": {}, "source": ["### 在数据中创建索引\n", "现在我们有了一个文档，我们可以创建一个索引并插入文档。对于索引，我们将使用MilvusVectorStore。MilvusVectorStore接受一些参数：\n", "\n", "- `uri (str, optional)`: 连接的URI，格式为\"http://address:port\"。默认为\"http://localhost:19530\"。\n", "- `token (str, optional)`: 登录的令牌。如果不使用rbac，则为空，如果使用rbac，则可能是\"username:password\"。默认为空字符串。\n", "- `collection_name (str, optional)`: 数据将被存储的集合的名称。默认为\"llamalection\"。\n", "- `dim (int, optional)`: 嵌入的维度。如果未提供，将在第一次插入时创建集合。默认为None。\n", "- `embedding_field (str, optional)`: 集合的嵌入字段的名称，默认为DEFAULT_EMBEDDING_KEY。\n", "- `doc_id_field (str, optional)`: 集合的doc_id字段的名称，默认为DEFAULT_DOC_ID_KEY。\n", "- `similarity_metric (str, optional)`: 要使用的相似度度量，目前支持IP和L2。默认为\"IP\"。\n", "- `consistency_level (str, optional)`: 用于新创建的集合的一致性级别。默认为\"Strong\"。\n", "- `overwrite (bool, optional)`: 是否覆盖同名的现有集合。默认为False。\n", "- `text_key (str, optional)`: 在传递的集合中存储文本的键。在带有自己的集合时使用。默认为None。\n", "- `index_config (dict, optional)`: 用于构建Milvus索引的配置。默认为None。\n", "- `search_config (dict, optional)`: 用于搜索Milvus索引的配置。注意，这必须与index_config指定的索引类型兼容。默认为None。\n", "- `batch_size (int)`: 在将数据插入Milvus时，配置在一个批处理中处理的文档数量。默认为DEFAULT_BATCH_SIZE。\n", "- `enable_sparse (bool)`: 一个布尔标志，指示是否启用对混合检索的稀疏嵌入的支持。默认为False。\n", "- `sparse_embedding_function (BaseSparseEmbeddingFunction, optional)`: 如果enable_sparse为True，则应提供此对象以将文本转换为稀疏嵌入。\n", "- `hybrid_ranker (str)`: 指定在混合搜索查询中使用的排名器类型。目前仅支持['RRFRanker'，'WeightedRanker']。默认为\"RRFRanker\"。\n", "- `hybrid_ranker_params (dict)`: 混合排名器的配置参数。\n", "    - 对于\"RRFRanker\"，它应包括：\n", "        - 'k' (int): 用于Reciprocal Rank Fusion (RRF)的参数。该值用于计算排名分数，作为RRF算法的一部分，该算法将多个排名策略组合成单个分数，以提高搜索相关性。\n", "    - 对于\"WeightedRanker\"，它应包括：\n", "        - 'weights' (float列表): 由两个权重组成的列表：\n", "             - 密集嵌入组件的权重。\n", "             - 稀疏嵌入组件的权重。\n", "             \n", "        这些权重用于调整嵌入的密集和稀疏组件在混合检索过程中的重要性。\n", "\n", "    默认为空字典，表示排名器将使用其预定义的默认设置。\n"]}, {"cell_type": "markdown", "id": "5fe075f5", "metadata": {}, "source": ["现在，让我们开始创建一个用于混合检索的MilvusVectorStore。我们需要将`enable_sparse`设置为True以启用稀疏嵌入生成，还需要配置RRFRanker进行重新排序。更多详情，请参考[Milvus重新排序](https://milvus.io/docs/reranking.md)。\n"]}, {"cell_type": "code", "execution_count": null, "id": "ba1558b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Sparse embedding function is not provided, using default.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "5a60f3c94f3d456b9c15876d021511bf", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["----------using 2*GPUs----------\n"]}], "source": ["# 在文档上创建索引\n", "from llama_index.core import StorageContext\n", "import os\n", "\n", "\n", "vector_store = MilvusVectorStore(\n", "    dim=1536,\n", "    overwrite=True,\n", "    enable_sparse=True,\n", "    hybrid_ranker=\"RRFRanker\",\n", "    hybrid_ranker_params={\"k\": 60},\n", ")\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n", "index = VectorStoreIndex.from_documents(\n", "    documents, storage_context=storage_context\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "04304299-fc3e-40a0-8600-f50c3292767e", "metadata": {}, "source": ["### 查询数据\n", "现在我们已经将文档存储在索引中，我们可以通过指定 `vector_store_query_mode` 来针对索引提出问题。索引将使用自身存储的数据作为chatgpt的知识库。\n"]}, {"cell_type": "code", "execution_count": null, "id": "35369eda", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author learned that the field of AI, as practiced at the time, was not as promising as initially\n", "believed. The author realized that the approach of using explicit data structures to represent\n", "concepts in AI was not effective in truly understanding natural language. This led the author to\n", "shift focus from traditional AI to exploring Lisp for its own merits, ultimately deciding to write a\n", "book about Lisp hacking.\n"]}], "source": ["query_engine = index.as_query_engine(vector_store_query_mode=\"hybrid\")\n", "response = query_engine.query(\"What did the author learn?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"cell_type": "code", "execution_count": null, "id": "99212d33", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Dealing with the stress and pressure related to managing Hacker News was a challenging moment for\n", "the author.\n"]}], "source": ["response = query_engine.query(\"What was a hard moment for the author?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"cell_type": "markdown", "id": "a41c8b12", "metadata": {}, "source": ["### 自定义稀疏嵌入函数\n", "\n", "在这里，我们使用默认的稀疏嵌入函数，它利用了[BGE-M3](https://arxiv.org/abs/2402.03216)模型。下面，我们将描述如何准备一个自定义的稀疏嵌入函数。\n", "\n", "您需要创建一个类，类似于ExampleEmbeddingFunction。这个类应该包括以下方法：\n", "- encode_queries: 这个方法将文本转换为查询的稀疏嵌入列表。\n", "- encode_documents: 这个方法将文本转换为文档的稀疏嵌入列表。\n", "\n", "稀疏嵌入的格式是一个字典，其中键（整数）表示维度，其对应的值（浮点数）表示该维度上的嵌入大小。（例如，{1: 0.5, 2: 0.3}）。\n"]}, {"cell_type": "code", "execution_count": null, "id": "c1a07def", "metadata": {}, "outputs": [], "source": ["! pip install FlagEmbedding"]}, {"cell_type": "code", "execution_count": null, "id": "2a2d7e0f", "metadata": {}, "outputs": [], "source": ["from FlagEmbedding import BGEM3FlagModel\n", "from typing import List\n", "from llama_index.vector_stores.milvus.utils import BaseSparseEmbeddingFunction\n", "\n", "\n", "class ExampleEmbeddingFunction(BaseSparseEmbeddingFunction):\n", "    def __init__(self):\n", "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)\n", "\n", "    def encode_queries(self, queries: List[str]):\n", "        outputs = self.model.encode(\n", "            queries,\n", "            return_dense=False,\n", "            return_sparse=True,\n", "            return_colbert_vecs=False,\n", "        )[\"lexical_weights\"]\n", "        return [self._to_standard_dict(output) for output in outputs]\n", "\n", "    def encode_documents(self, documents: List[str]):\n", "        outputs = self.model.encode(\n", "            documents,\n", "            return_dense=False,\n", "            return_sparse=True,\n", "            return_colbert_vecs=False,\n", "        )[\"lexical_weights\"]\n", "        return [self._to_standard_dict(output) for output in outputs]\n", "\n", "    def _to_standard_dict(self, raw_output):\n", "        result = {}\n", "        for k in raw_output:\n", "            result[int(k)] = raw_output[k]\n", "        return result"]}, {"cell_type": "markdown", "id": "b52723bf", "metadata": {}, "source": ["现在我们可以在混合检索中使用这个。\n"]}, {"cell_type": "code", "execution_count": null, "id": "e5c465a7", "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c4e5ec8bd9a14dceb9ee4b4d1c66f38d", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["----------using 2*GPUs----------\n"]}], "source": ["vector_store = MilvusVectorStore(\n", "    dim=1536,\n", "    overwrite=True,\n", "    enable_sparse=True,\n", "    sparse_embedding_function=ExampleEmbeddingFunction(),\n", "    hybrid_ranker=\"RRFRanker\",\n", "    hybrid_ranker_params={\"k\": 60},\n", ")"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}