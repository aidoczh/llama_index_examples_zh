{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "1496f9de", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/MilvusIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "0b692c73", "metadata": {}, "source": ["# ä½¿ç”¨æ··åˆæ£€ç´¢çš„Milvuså‘é‡å­˜å‚¨\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "1e7787c2", "metadata": {}, "source": ["åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•å¿«é€Ÿä½¿ç”¨MilvusVectorStoreè¿›è¡Œæ··åˆæ£€ç´¢çš„æ¼”ç¤ºã€‚ï¼ˆMilvusç‰ˆæœ¬åº”è¯¥é«˜äº2.4.0ï¼‰\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "f81e2c81", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "3e0c18ca", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-vector-stores-milvus"]}, {"cell_type": "markdown", "id": "df3ddfc5", "metadata": {}, "source": ["BGE-M3æ˜¯FlagEmbeddingä¸­é»˜è®¤çš„ç¨€ç–åµŒå…¥æ–¹æ³•ï¼Œå› æ­¤åœ¨å®‰è£…llama-indexæ—¶éœ€è¦ä¸€å¹¶å®‰è£…ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "6b80700a", "metadata": {}, "outputs": [], "source": ["! pip install llama-index\n", "! pip install FlagEmbedding"]}, {"cell_type": "code", "execution_count": null, "id": "47264e32", "metadata": {}, "outputs": [], "source": ["import logging\n", "import sys\n", "\n", "# å–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹è°ƒè¯•æ—¥å¿—\n", "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n", "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document\n", "from llama_index.vector_stores.milvus import MilvusVectorStore\n", "from IPython.display import Markdown, display\n", "import textwrap"]}, {"attachments": {}, "cell_type": "markdown", "id": "f9b97a89", "metadata": {}, "source": ["### è®¾ç½®OpenAI\n", "è®©æˆ‘ä»¬é¦–å…ˆæ·»åŠ OpenAI APIå¯†é’¥ã€‚è¿™å°†å…è®¸æˆ‘ä»¬è®¿é—®OpenAIä»¥è·å–åµŒå…¥å’Œä½¿ç”¨ChatGPTã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "0c9f4d21-145a-401e-95ff-ccb259e8ef84", "metadata": {}, "outputs": [], "source": ["import openai\n", "\n", "openai.api_key = \"sk-\""]}, {"attachments": {}, "cell_type": "markdown", "id": "a3d4e638", "metadata": {}, "source": ["### ä¸‹è½½æ•°æ®\n"]}, {"cell_type": "code", "execution_count": null, "id": "2a2e24d1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2024-04-25 17:44:59--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 75042 (73K) [text/plain]\n", "Saving to: â€˜data/paul_graham/paul_graham_essay.txtâ€™\n", "\n", "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.07s   \n", "\n", "2024-04-25 17:45:00 (994 KB/s) - â€˜data/paul_graham/paul_graham_essay.txtâ€™ saved [75042/75042]\n", "\n"]}], "source": ["! mkdir -p 'data/paul_graham/'\n", "! wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "id": "59ff935d", "metadata": {}, "source": ["### ç”Ÿæˆæˆ‘ä»¬çš„æ•°æ®\n", "æœ‰äº†æˆ‘ä»¬çš„LLMé›†åˆï¼Œè®©æˆ‘ä»¬å¼€å§‹ä½¿ç”¨Milvusç´¢å¼•ã€‚ä½œä¸ºç¬¬ä¸€ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬ä»`data/paul_graham/`æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶ç”Ÿæˆä¸€ä¸ªæ–‡æ¡£ã€‚åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæœ‰ä¸€ç¯‡æ¥è‡ªPaul Grahamçš„å•ç¯‡æ–‡ç« ï¼Œæ ‡é¢˜ä¸º`What I Worked On`ã€‚ä¸ºäº†ç”Ÿæˆè¿™äº›æ–‡æ¡£ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨SimpleDirectoryReaderã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "68cbd239-880e-41a3-98d8-dbb3fab55431", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Document ID: ca3f5dbc-f772-41da-9a4f-bb4884691793\n"]}], "source": ["# åŠ è½½æ–‡æ¡£\n", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n", "\n", "print(\"æ–‡æ¡£ID:\", documents[0].doc_id)"]}, {"attachments": {}, "cell_type": "markdown", "id": "dd270925", "metadata": {}, "source": ["### åœ¨æ•°æ®ä¸­åˆ›å»ºç´¢å¼•\n", "ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ–‡æ¡£ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªç´¢å¼•å¹¶æ’å…¥æ–‡æ¡£ã€‚å¯¹äºç´¢å¼•ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨MilvusVectorStoreã€‚MilvusVectorStoreæ¥å—ä¸€äº›å‚æ•°ï¼š\n", "\n", "- `uri (str, optional)`: è¿æ¥çš„URIï¼Œæ ¼å¼ä¸º\"http://address:port\"ã€‚é»˜è®¤ä¸º\"http://localhost:19530\"ã€‚\n", "- `token (str, optional)`: ç™»å½•çš„ä»¤ç‰Œã€‚å¦‚æœä¸ä½¿ç”¨rbacï¼Œåˆ™ä¸ºç©ºï¼Œå¦‚æœä½¿ç”¨rbacï¼Œåˆ™å¯èƒ½æ˜¯\"username:password\"ã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚\n", "- `collection_name (str, optional)`: æ•°æ®å°†è¢«å­˜å‚¨çš„é›†åˆçš„åç§°ã€‚é»˜è®¤ä¸º\"llamalection\"ã€‚\n", "- `dim (int, optional)`: åµŒå…¥çš„ç»´åº¦ã€‚å¦‚æœæœªæä¾›ï¼Œå°†åœ¨ç¬¬ä¸€æ¬¡æ’å…¥æ—¶åˆ›å»ºé›†åˆã€‚é»˜è®¤ä¸ºNoneã€‚\n", "- `embedding_field (str, optional)`: é›†åˆçš„åµŒå…¥å­—æ®µçš„åç§°ï¼Œé»˜è®¤ä¸ºDEFAULT_EMBEDDING_KEYã€‚\n", "- `doc_id_field (str, optional)`: é›†åˆçš„doc_idå­—æ®µçš„åç§°ï¼Œé»˜è®¤ä¸ºDEFAULT_DOC_ID_KEYã€‚\n", "- `similarity_metric (str, optional)`: è¦ä½¿ç”¨çš„ç›¸ä¼¼åº¦åº¦é‡ï¼Œç›®å‰æ”¯æŒIPå’ŒL2ã€‚é»˜è®¤ä¸º\"IP\"ã€‚\n", "- `consistency_level (str, optional)`: ç”¨äºæ–°åˆ›å»ºçš„é›†åˆçš„ä¸€è‡´æ€§çº§åˆ«ã€‚é»˜è®¤ä¸º\"Strong\"ã€‚\n", "- `overwrite (bool, optional)`: æ˜¯å¦è¦†ç›–åŒåçš„ç°æœ‰é›†åˆã€‚é»˜è®¤ä¸ºFalseã€‚\n", "- `text_key (str, optional)`: åœ¨ä¼ é€’çš„é›†åˆä¸­å­˜å‚¨æ–‡æœ¬çš„é”®ã€‚åœ¨å¸¦æœ‰è‡ªå·±çš„é›†åˆæ—¶ä½¿ç”¨ã€‚é»˜è®¤ä¸ºNoneã€‚\n", "- `index_config (dict, optional)`: ç”¨äºæ„å»ºMilvusç´¢å¼•çš„é…ç½®ã€‚é»˜è®¤ä¸ºNoneã€‚\n", "- `search_config (dict, optional)`: ç”¨äºæœç´¢Milvusç´¢å¼•çš„é…ç½®ã€‚æ³¨æ„ï¼Œè¿™å¿…é¡»ä¸index_configæŒ‡å®šçš„ç´¢å¼•ç±»å‹å…¼å®¹ã€‚é»˜è®¤ä¸ºNoneã€‚\n", "- `batch_size (int)`: åœ¨å°†æ•°æ®æ’å…¥Milvusæ—¶ï¼Œé…ç½®åœ¨ä¸€ä¸ªæ‰¹å¤„ç†ä¸­å¤„ç†çš„æ–‡æ¡£æ•°é‡ã€‚é»˜è®¤ä¸ºDEFAULT_BATCH_SIZEã€‚\n", "- `enable_sparse (bool)`: ä¸€ä¸ªå¸ƒå°”æ ‡å¿—ï¼ŒæŒ‡ç¤ºæ˜¯å¦å¯ç”¨å¯¹æ··åˆæ£€ç´¢çš„ç¨€ç–åµŒå…¥çš„æ”¯æŒã€‚é»˜è®¤ä¸ºFalseã€‚\n", "- `sparse_embedding_function (BaseSparseEmbeddingFunction, optional)`: å¦‚æœenable_sparseä¸ºTrueï¼Œåˆ™åº”æä¾›æ­¤å¯¹è±¡ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºç¨€ç–åµŒå…¥ã€‚\n", "- `hybrid_ranker (str)`: æŒ‡å®šåœ¨æ··åˆæœç´¢æŸ¥è¯¢ä¸­ä½¿ç”¨çš„æ’åå™¨ç±»å‹ã€‚ç›®å‰ä»…æ”¯æŒ['RRFRanker'ï¼Œ'WeightedRanker']ã€‚é»˜è®¤ä¸º\"RRFRanker\"ã€‚\n", "- `hybrid_ranker_params (dict)`: æ··åˆæ’åå™¨çš„é…ç½®å‚æ•°ã€‚\n", "    - å¯¹äº\"RRFRanker\"ï¼Œå®ƒåº”åŒ…æ‹¬ï¼š\n", "        - 'k' (int): ç”¨äºReciprocal Rank Fusion (RRF)çš„å‚æ•°ã€‚è¯¥å€¼ç”¨äºè®¡ç®—æ’ååˆ†æ•°ï¼Œä½œä¸ºRRFç®—æ³•çš„ä¸€éƒ¨åˆ†ï¼Œè¯¥ç®—æ³•å°†å¤šä¸ªæ’åç­–ç•¥ç»„åˆæˆå•ä¸ªåˆ†æ•°ï¼Œä»¥æé«˜æœç´¢ç›¸å…³æ€§ã€‚\n", "    - å¯¹äº\"WeightedRanker\"ï¼Œå®ƒåº”åŒ…æ‹¬ï¼š\n", "        - 'weights' (floatåˆ—è¡¨): ç”±ä¸¤ä¸ªæƒé‡ç»„æˆçš„åˆ—è¡¨ï¼š\n", "             - å¯†é›†åµŒå…¥ç»„ä»¶çš„æƒé‡ã€‚\n", "             - ç¨€ç–åµŒå…¥ç»„ä»¶çš„æƒé‡ã€‚\n", "             \n", "        è¿™äº›æƒé‡ç”¨äºè°ƒæ•´åµŒå…¥çš„å¯†é›†å’Œç¨€ç–ç»„ä»¶åœ¨æ··åˆæ£€ç´¢è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚\n", "\n", "    é»˜è®¤ä¸ºç©ºå­—å…¸ï¼Œè¡¨ç¤ºæ’åå™¨å°†ä½¿ç”¨å…¶é¢„å®šä¹‰çš„é»˜è®¤è®¾ç½®ã€‚\n"]}, {"cell_type": "markdown", "id": "5fe075f5", "metadata": {}, "source": ["ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¼€å§‹åˆ›å»ºä¸€ä¸ªç”¨äºæ··åˆæ£€ç´¢çš„MilvusVectorStoreã€‚æˆ‘ä»¬éœ€è¦å°†`enable_sparse`è®¾ç½®ä¸ºTrueä»¥å¯ç”¨ç¨€ç–åµŒå…¥ç”Ÿæˆï¼Œè¿˜éœ€è¦é…ç½®RRFRankerè¿›è¡Œé‡æ–°æ’åºã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒ[Milvusé‡æ–°æ’åº](https://milvus.io/docs/reranking.md)ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "ba1558b3", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Sparse embedding function is not provided, using default.\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "5a60f3c94f3d456b9c15876d021511bf", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["----------using 2*GPUs----------\n"]}], "source": ["# åœ¨æ–‡æ¡£ä¸Šåˆ›å»ºç´¢å¼•\n", "from llama_index.core import StorageContext\n", "import os\n", "\n", "\n", "vector_store = MilvusVectorStore(\n", "    dim=1536,\n", "    overwrite=True,\n", "    enable_sparse=True,\n", "    hybrid_ranker=\"RRFRanker\",\n", "    hybrid_ranker_params={\"k\": 60},\n", ")\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n", "index = VectorStoreIndex.from_documents(\n", "    documents, storage_context=storage_context\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "04304299-fc3e-40a0-8600-f50c3292767e", "metadata": {}, "source": ["### æŸ¥è¯¢æ•°æ®\n", "ç°åœ¨æˆ‘ä»¬å·²ç»å°†æ–‡æ¡£å­˜å‚¨åœ¨ç´¢å¼•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŒ‡å®š `vector_store_query_mode` æ¥é’ˆå¯¹ç´¢å¼•æå‡ºé—®é¢˜ã€‚ç´¢å¼•å°†ä½¿ç”¨è‡ªèº«å­˜å‚¨çš„æ•°æ®ä½œä¸ºchatgptçš„çŸ¥è¯†åº“ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "35369eda", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author learned that the field of AI, as practiced at the time, was not as promising as initially\n", "believed. The author realized that the approach of using explicit data structures to represent\n", "concepts in AI was not effective in truly understanding natural language. This led the author to\n", "shift focus from traditional AI to exploring Lisp for its own merits, ultimately deciding to write a\n", "book about Lisp hacking.\n"]}], "source": ["query_engine = index.as_query_engine(vector_store_query_mode=\"hybrid\")\n", "response = query_engine.query(\"What did the author learn?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"cell_type": "code", "execution_count": null, "id": "99212d33", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Dealing with the stress and pressure related to managing Hacker News was a challenging moment for\n", "the author.\n"]}], "source": ["response = query_engine.query(\"What was a hard moment for the author?\")\n", "print(textwrap.fill(str(response), 100))"]}, {"cell_type": "markdown", "id": "a41c8b12", "metadata": {}, "source": ["### è‡ªå®šä¹‰ç¨€ç–åµŒå…¥å‡½æ•°\n", "\n", "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨é»˜è®¤çš„ç¨€ç–åµŒå…¥å‡½æ•°ï¼Œå®ƒåˆ©ç”¨äº†[BGE-M3](https://arxiv.org/abs/2402.03216)æ¨¡å‹ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°†æè¿°å¦‚ä½•å‡†å¤‡ä¸€ä¸ªè‡ªå®šä¹‰çš„ç¨€ç–åµŒå…¥å‡½æ•°ã€‚\n", "\n", "æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ªç±»ï¼Œç±»ä¼¼äºExampleEmbeddingFunctionã€‚è¿™ä¸ªç±»åº”è¯¥åŒ…æ‹¬ä»¥ä¸‹æ–¹æ³•ï¼š\n", "- encode_queries: è¿™ä¸ªæ–¹æ³•å°†æ–‡æœ¬è½¬æ¢ä¸ºæŸ¥è¯¢çš„ç¨€ç–åµŒå…¥åˆ—è¡¨ã€‚\n", "- encode_documents: è¿™ä¸ªæ–¹æ³•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ–‡æ¡£çš„ç¨€ç–åµŒå…¥åˆ—è¡¨ã€‚\n", "\n", "ç¨€ç–åµŒå…¥çš„æ ¼å¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®ï¼ˆæ•´æ•°ï¼‰è¡¨ç¤ºç»´åº¦ï¼Œå…¶å¯¹åº”çš„å€¼ï¼ˆæµ®ç‚¹æ•°ï¼‰è¡¨ç¤ºè¯¥ç»´åº¦ä¸Šçš„åµŒå…¥å¤§å°ã€‚ï¼ˆä¾‹å¦‚ï¼Œ{1: 0.5, 2: 0.3}ï¼‰ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "c1a07def", "metadata": {}, "outputs": [], "source": ["! pip install FlagEmbedding"]}, {"cell_type": "code", "execution_count": null, "id": "2a2d7e0f", "metadata": {}, "outputs": [], "source": ["from FlagEmbedding import BGEM3FlagModel\n", "from typing import List\n", "from llama_index.vector_stores.milvus.utils import BaseSparseEmbeddingFunction\n", "\n", "\n", "class ExampleEmbeddingFunction(BaseSparseEmbeddingFunction):\n", "    def __init__(self):\n", "        self.model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)\n", "\n", "    def encode_queries(self, queries: List[str]):\n", "        outputs = self.model.encode(\n", "            queries,\n", "            return_dense=False,\n", "            return_sparse=True,\n", "            return_colbert_vecs=False,\n", "        )[\"lexical_weights\"]\n", "        return [self._to_standard_dict(output) for output in outputs]\n", "\n", "    def encode_documents(self, documents: List[str]):\n", "        outputs = self.model.encode(\n", "            documents,\n", "            return_dense=False,\n", "            return_sparse=True,\n", "            return_colbert_vecs=False,\n", "        )[\"lexical_weights\"]\n", "        return [self._to_standard_dict(output) for output in outputs]\n", "\n", "    def _to_standard_dict(self, raw_output):\n", "        result = {}\n", "        for k in raw_output:\n", "            result[int(k)] = raw_output[k]\n", "        return result"]}, {"cell_type": "markdown", "id": "b52723bf", "metadata": {}, "source": ["ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æ··åˆæ£€ç´¢ä¸­ä½¿ç”¨è¿™ä¸ªã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "e5c465a7", "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c4e5ec8bd9a14dceb9ee4b4d1c66f38d", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["----------using 2*GPUs----------\n"]}], "source": ["vector_store = MilvusVectorStore(\n", "    dim=1536,\n", "    overwrite=True,\n", "    enable_sparse=True,\n", "    sparse_embedding_function=ExampleEmbeddingFunction(),\n", "    hybrid_ranker=\"RRFRanker\",\n", "    hybrid_ranker_params={\"k\": 60},\n", ")"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}