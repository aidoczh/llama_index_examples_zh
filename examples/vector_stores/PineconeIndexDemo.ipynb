{"cells": [{"cell_type": "markdown", "id": "714eb664", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/PineconeIndexDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "307804a3-c02b-4a57-ac0d-172c30ddc851", "metadata": {}, "source": ["# Pinecone向量存储\n", "\n", "Pinecone是一个高性能的向量数据库，专门用于存储和检索大规模向量数据。它提供了快速的相似向量搜索和高效的向量索引功能，适用于各种应用场景，如推荐系统、搜索引擎、自然语言处理等。Pinecone支持多种编程语言，并提供了简单易用的API，使开发人员能够轻松地集成和使用该服务。\n"]}, {"cell_type": "markdown", "id": "36be66bf", "metadata": {}, "source": ["如果您在Colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "9ddff1e4", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-vector-stores-pinecone"]}, {"cell_type": "code", "execution_count": null, "id": "6807106d", "metadata": {}, "outputs": [], "source": ["!pip install llama-index>=0.9.31 pinecone-client>=3.0.0"]}, {"cell_type": "code", "execution_count": null, "id": "d48af8e1", "metadata": {}, "outputs": [], "source": ["import logging\n", "import sys\n", "import os\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"]}, {"cell_type": "markdown", "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396", "metadata": {}, "source": ["#### 创建一个Pinecone索引\n"]}, {"cell_type": "code", "execution_count": null, "id": "0ce3143d-198c-4dd2-8e5a-c5cdf94f017a", "metadata": {}, "outputs": [], "source": ["from pinecone import Pinecone, ServerlessSpec"]}, {"cell_type": "code", "execution_count": null, "id": "4ad14111-0bbb-4c62-906d-6d6253e0cdee", "metadata": {}, "outputs": [], "source": ["os.environ[\n", "    \"PINECONE_API_KEY\"\n", "] = \"<Your Pinecone API key, from app.pinecone.io>\"\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n", "\n", "api_key = os.environ[\"PINECONE_API_KEY\"]\n", "\n", "pc = Pinecone(api_key=api_key)"]}, {"cell_type": "code", "execution_count": null, "id": "233a080f", "metadata": {}, "outputs": [], "source": ["# 如果需要，删除\n", "# pc.delete_index(\"quickstart\")"]}, {"cell_type": "code", "execution_count": null, "id": "c2c90087-bdd9-4ca4-b06b-2af883559f88", "metadata": {}, "outputs": [], "source": ["# dimensions are for text-embedding-ada-002\n", "\n", "pc.create_index(\n", "    name=\"quickstart\",\n", "    dimension=1536,\n", "    metric=\"euclidean\",\n", "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-west-2\"),\n", ")\n", "\n", "# 如果您需要创建基于Pod的Pinecone索引，您也可以这样做：\n", "\n", "# from pinecone import Pinecone, PodSpec\n", "#\n", "# pc = Pinecone(api_key='xxx')\n", "#\n", "# pc.create_index(\n", "# \t name='my-index',\n", "# \t dimension=1536,\n", "# \t metric='cosine',\n", "# \t spec=PodSpec(\n", "# \t\t environment='us-east1-gcp',\n", "# \t\t pod_type='p1.x1',\n", "# \t\t pods=1\n", "# \t )\n", "# )\n", "#"]}, {"cell_type": "code", "execution_count": null, "id": "667f3cb3-ce18-48d5-b9aa-bfc1a1f0f0f6", "metadata": {}, "outputs": [], "source": ["pinecone_index = pc.Index(\"quickstart\")"]}, {"cell_type": "markdown", "id": "8ee4473a-094f-4d0a-a825-e1213db07240", "metadata": {}, "source": ["#### 加载文档，构建PineconeVectorStore和VectorStoreIndex\n"]}, {"cell_type": "code", "execution_count": null, "id": "0a2bcc07", "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "from llama_index.vector_stores.pinecone import PineconeVectorStore\n", "from IPython.display import Markdown, display"]}, {"cell_type": "markdown", "id": "7d782f76", "metadata": {}, "source": ["### 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "5104674e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n", "ERROR: could not open HSTS store at '/home/loganm/.wget-hsts'. HSTS will be disabled.\n", "--2024-01-16 11:56:25--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 75042 (73K) [text/plain]\n", "Saving to: ‘data/paul_graham/paul_graham_essay.txt’\n", "\n", "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.04s   \n", "\n", "2024-01-16 11:56:25 (1.79 MB/s) - ‘data/paul_graham/paul_graham_essay.txt’ saved [75042/75042]\n", "\n"]}], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "id": "68cbd239-880e-41a3-98d8-dbb3fab55431", "metadata": {}, "outputs": [], "source": ["# 加载文档\n", "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"]}, {"cell_type": "code", "execution_count": null, "id": "ba1558b3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n", "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b9ce8a4615fe4162be2eaa2a5573f948", "version_major": 2, "version_minor": 0}, "text/plain": ["Upserted vectors:   0%|          | 0/22 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# 初始化，不带元数据过滤器\n", "from llama_index.core import StorageContext\n", "\n", "if \"OPENAI_API_KEY\" not in os.environ:\n", "    raise EnvironmentError(f\"环境变量 OPENAI_API_KEY 未设置\")\n", "\n", "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n", "index = VectorStoreIndex.from_documents(\n", "    documents, storage_context=storage_context\n", ")"]}, {"cell_type": "markdown", "id": "04304299-fc3e-40a0-8600-f50c3292767e", "metadata": {}, "source": ["#### 查询索引\n", "\n", "可能需要一分钟左右来准备索引！\n"]}, {"cell_type": "code", "execution_count": null, "id": "35369eda", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n", "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n", "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n", "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"]}], "source": ["# 将日志级别设置为DEBUG，以获得更详细的输出\n", "query_engine = index.as_query_engine()\n", "response = query_engine.query(\"作者在成长过程中做了什么？\")"]}, {"cell_type": "code", "execution_count": null, "id": "bedbb693-725f-478f-be26-fa7180ea38b2", "metadata": {}, "outputs": [{"data": {"text/markdown": ["<b>The author, growing up, worked on writing and programming. They wrote short stories and tried writing programs on an IBM 1401 computer. They later got a microcomputer and started programming more extensively, writing simple games and a word processor.</b>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["display(Markdown(f\"<b>{response}</b>\"))"]}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}