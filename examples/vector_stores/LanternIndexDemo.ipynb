{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "bccd47fc", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/vector_stores/Lantern.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "db0855d0", "metadata": {}, "source": ["# 灯笼向量存储\n", "在这个笔记本中，我们将展示如何使用[Postgresql](https://www.postgresql.org)和[Lantern](https://github.com/lanterndata/lantern)在LlamaIndex中执行向量搜索。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "e4f33fc9", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "59632875", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-vector-stores-lantern\n", "%pip install llama-index-embeddings-openai"]}, {"cell_type": "code", "execution_count": null, "id": "712daea5", "metadata": {}, "outputs": [], "source": ["\n", "!pip install psycopg2-binary llama-index asyncpg \n"]}, {"cell_type": "code", "execution_count": null, "id": "c2d1c538", "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader, StorageContext\n", "from llama_index.core import VectorStoreIndex\n", "from llama_index.vector_stores.lantern import LanternVectorStore\n", "import textwrap\n", "import openai"]}, {"cell_type": "markdown", "id": "26c71b6d", "metadata": {}, "source": ["### 设置OpenAI\n", "第一步是配置OpenAI密钥。它将用于为加载到索引中的文档创建嵌入。\n"]}, {"cell_type": "code", "execution_count": null, "id": "67b86621", "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"<your_key>\"\n", "openai.api_key = \"<your_key>\""]}, {"attachments": {}, "cell_type": "markdown", "id": "eecf4bd5", "metadata": {}, "source": ["下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "6df9fa89", "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396", "metadata": {}, "source": ["### 加载文档\n", "使用SimpleDirectoryReader加载存储在`data/paul_graham/`中的文档。\n"]}, {"cell_type": "code", "execution_count": null, "id": "c154dd4b", "metadata": {}, "outputs": [], "source": ["documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n", "print(\"Document ID:\", documents[0].doc_id)"]}, {"cell_type": "markdown", "id": "7bd24f0a", "metadata": {}, "source": ["### 创建数据库\n", "使用已经在本地运行的postgres，创建我们将要使用的数据库。\n"]}, {"cell_type": "code", "execution_count": null, "id": "e6d61e73", "metadata": {}, "outputs": [], "source": ["import psycopg2\n", "\n", "connection_string = \"postgresql://postgres:postgres@localhost:5432\"\n", "db_name = \"postgres\"\n", "conn = psycopg2.connect(connection_string)\n", "conn.autocommit = True\n", "\n", "with conn.cursor() as c:\n", "    c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n", "    c.execute(f\"CREATE DATABASE {db_name}\")"]}, {"cell_type": "code", "execution_count": null, "id": "8883b6b0-8a1e-42ca-9134-ade42285e7dc", "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.openai import OpenAIEmbedding", "from llama_index.core import Settings", "", "# 使用嵌入模型设置全局设置", "# 因此查询字符串将被转换为嵌入，并且将使用HNSW索引", "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"]}, {"cell_type": "markdown", "id": "c0232fd1", "metadata": {}, "source": ["### 创建索引\n", "在这里，我们使用之前加载的文档创建一个由Postgres支持的索引。LanternVectorStore需要一些参数。\n"]}, {"cell_type": "code", "execution_count": null, "id": "8731da62", "metadata": {}, "outputs": [], "source": ["from sqlalchemy import make_url", "", "url = make_url(connection_string)", "vector_store = LanternVectorStore.from_params(", "    database=db_name,", "    host=url.host,", "    password=url.password,", "    port=url.port,", "    user=url.username,", "    table_name=\"paul_graham_essay\",", "    embed_dim=1536,  # openai embedding dimension", ")", "", "storage_context = StorageContext.from_defaults(vector_store=vector_store)", "index = VectorStoreIndex.from_documents(", "    documents, storage_context=storage_context, show_progress=True", ")", "query_engine = index.as_query_engine()"]}, {"cell_type": "markdown", "id": "8ee4473a-094f-4d0a-a825-e1213db07240", "metadata": {}, "source": ["### 查询索引\n", "现在我们可以使用我们的索引来提出问题。\n"]}, {"cell_type": "code", "execution_count": null, "id": "0a2bcc07", "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"What did the author do?\")"]}, {"cell_type": "code", "execution_count": null, "id": "8cf55bf7", "metadata": {}, "outputs": [], "source": ["print(textwrap.fill(str(response), 100))"]}, {"cell_type": "code", "execution_count": null, "id": "68cbd239-880e-41a3-98d8-dbb3fab55431", "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"What happened in the mid 1980s?\")"]}, {"cell_type": "code", "execution_count": null, "id": "fdf5287f", "metadata": {}, "outputs": [], "source": ["print(textwrap.fill(str(response), 100))"]}, {"cell_type": "markdown", "id": "b3bed9e1", "metadata": {}, "source": ["### 查询现有索引\n"]}, {"cell_type": "code", "execution_count": null, "id": "e6b2634b", "metadata": {}, "outputs": [], "source": ["vector_store = LanternVectorStore.from_params(", "    database=db_name,  # 数据库名称", "    host=url.host,  # 主机地址", "    password=url.password,  # 密码", "    port=url.port,  # 端口", "    user=url.username,  # 用户名", "    table_name=\"paul_graham_essay\",  # 表名称", "    embed_dim=1536,  # openai嵌入维度", "    m=16,  # HNSW M参数", "    ef_construction=128,  # HNSW ef构建参数", "    ef=64,  # HNSW ef搜索参数", ")", "", "# 了解有关HNSW参数的更多信息，请访问：https://github.com/nmslib/hnswlib/blob/master/ALGO_PARAMS.md", "", "index = VectorStoreIndex.from_vector_store(vector_store=vector_store)", "query_engine = index.as_query_engine()"]}, {"cell_type": "code", "execution_count": null, "id": "e7075af3-156e-4bde-8f76-6d9dee86861f", "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"What did the author do?\")"]}, {"cell_type": "code", "execution_count": null, "id": "b088c090", "metadata": {}, "outputs": [], "source": ["print(textwrap.fill(str(response), 100))"]}, {"cell_type": "markdown", "id": "55745895-8f01-4275-abaa-b2ebef2cb4c7", "metadata": {}, "source": ["### 混合搜索\n"]}, {"cell_type": "markdown", "id": "91cae40f-3cd4-4403-8af4-aca2705e96a2", "metadata": {}, "source": ["要启用混合搜索，您需要：\n", "1. 在构建`LanternVectorStore`时传入`hybrid_search=True`（并可选择使用所需的语言配置`text_search_config`）\n", "2. 在构建查询引擎时传入`vector_store_query_mode=\"hybrid\"`（此配置会在幕后传递给检索器）。您还可以选择设置`sparse_top_k`来配置从稀疏文本搜索中获取多少结果（默认值与`similarity_top_k`相同）。\n"]}, {"cell_type": "code", "execution_count": null, "id": "65a7e133-39da-40c5-b2c5-7af2c0a3a792", "metadata": {}, "outputs": [], "source": ["from sqlalchemy import make_url", "", "url = make_url(connection_string)", "hybrid_vector_store = LanternVectorStore.from_params(", "    database=db_name,", "    host=url.host,", "    password=url.password,", "    port=url.port,", "    user=url.username,", "    table_name=\"paul_graham_essay_hybrid_search\",", "    embed_dim=1536,  # openai embedding dimension", "    hybrid_search=True,", "    text_search_config=\"english\",", ")", "", "storage_context = StorageContext.from_defaults(", "    vector_store=hybrid_vector_store", ")", "hybrid_index = VectorStoreIndex.from_documents(", "    documents, storage_context=storage_context", ")"]}, {"cell_type": "code", "execution_count": null, "id": "6f8edee4-6c19-4d99-b602-110bdc5708e5", "metadata": {}, "outputs": [], "source": ["hybrid_query_engine = hybrid_index.as_query_engine(\n", "    vector_store_query_mode=\"hybrid\", sparse_top_k=2\n", ")\n", "hybrid_response = hybrid_query_engine.query(\n", "    \"Who does Paul Graham think of with the word schtick\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "bd454b25-b66c-4733-8ff4-24fb2ee84cec", "metadata": {}, "outputs": [], "source": ["print(hybrid_response)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}