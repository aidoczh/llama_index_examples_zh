{"cells": [{"cell_type": "markdown", "id": "3ac9adb4", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/llm/llama_2_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "id": "368686b4-f487-4dd4-aeff-37823976529d", "metadata": {}, "source": ["# LlamaCPP \n", "\n", "åœ¨è¿™ä¸ªç®€çŸ­çš„ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) åº“ä¸ LlamaIndexã€‚\n", "\n", "åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) æ¨¡å‹ï¼Œä»¥åŠé€‚å½“çš„æç¤ºæ ¼å¼ã€‚\n", "\n", "è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ `llama-cpp-python` ç‰ˆæœ¬ `0.1.79` ä¹‹åçš„ç‰ˆæœ¬ï¼Œæ¨¡å‹æ ¼å¼å·²ä» `ggmlv3` æ›´æ”¹ä¸º `gguf`ã€‚åƒæœ¬ç¬”è®°æœ¬ä¸­ä½¿ç”¨çš„æ—§æ¨¡å‹æ–‡ä»¶å¯ä»¥ä½¿ç”¨ [`llama.cpp`](https://github.com/ggerganov/llama.cpp) ä»“åº“ä¸­çš„è„šæœ¬è¿›è¡Œè½¬æ¢ã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä» [huggingface](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF) ä¸‹è½½ä¸Šè¿°æ¨¡å‹çš„ GGUF ç‰ˆæœ¬ã€‚\n", "\n", "é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœ model_path å’Œ model_url ä¸ºç©ºï¼Œ`LlamaCPP` æ¨¡å—å°†æ ¹æ®æ‚¨çš„ç‰ˆæœ¬ä»¥ä»»ä¸€æ ¼å¼åŠ è½½ llama2-chat-13Bã€‚\n", "\n", "## å®‰è£…\n", "\n", "ä¸ºäº†è·å¾— `LlamaCPP` çš„æœ€ä½³æ€§èƒ½ï¼Œå»ºè®®å®‰è£…è¯¥è½¯ä»¶åŒ…ï¼Œä»¥ä¾¿ä½¿ç”¨ GPU æ”¯æŒè¿›è¡Œç¼–è¯‘ã€‚å®‰è£…æ–¹å¼çš„å®Œæ•´æŒ‡å—åœ¨[è¿™é‡Œ](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal)ã€‚\n", "\n", "å®Œæ•´çš„ MACOS æŒ‡ä»¤ä¹Ÿåœ¨[è¿™é‡Œ](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/)ã€‚\n", "\n", "ä¸€èˆ¬è€Œè¨€ï¼š\n", "- å¦‚æœæ‚¨æœ‰ CUDA å’Œ NVIDIA GPUï¼Œè¯·ä½¿ç”¨ `CuBLAS`\n", "- å¦‚æœæ‚¨åœ¨ M1/M2 MacBook ä¸Šè¿è¡Œï¼Œè¯·ä½¿ç”¨ `METAL`\n", "- å¦‚æœæ‚¨åœ¨ AMD/Intel GPU ä¸Šè¿è¡Œï¼Œè¯·ä½¿ç”¨ `CLBLAST`\n"]}, {"cell_type": "code", "execution_count": null, "id": "aff273be", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-huggingface\n", "%pip install llama-index-llms-llama-cpp"]}, {"cell_type": "code", "execution_count": null, "id": "40a33749", "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n", "from llama_index.llms.llama_cpp import LlamaCPP\n", "from llama_index.llms.llama_cpp.llama_utils import (\n", "    messages_to_prompt,\n", "    completion_to_prompt,\n", ")"]}, {"cell_type": "markdown", "id": "e7927630-0044-41fb-a8a6-8dc3d2adb608", "metadata": {}, "source": ["## è®¾ç½®LLM\n", "\n", "LlamaCPP llm é«˜åº¦å¯é…ç½®ã€‚æ ¹æ®æ‰€ä½¿ç”¨çš„æ¨¡å‹ï¼Œæ‚¨å°†éœ€è¦ä¼ å…¥ `messages_to_prompt` å’Œ `completion_to_prompt` å‡½æ•°æ¥å¸®åŠ©æ ¼å¼åŒ–æ¨¡å‹è¾“å…¥ã€‚\n", "\n", "ç”±äºé»˜è®¤æ¨¡å‹æ˜¯ llama2-chatï¼Œæˆ‘ä»¬ä½¿ç”¨åœ¨ [`llama_index.llms.llama_utils`](https://github.com/jerryjliu/llama_index/blob/main/llama_index/llms/llama_utils.py) ä¸­æ‰¾åˆ°çš„å®ç”¨å‡½æ•°ã€‚\n", "\n", "å¯¹äºä»»ä½•éœ€è¦åœ¨åˆå§‹åŒ–æœŸé—´ä¼ å…¥çš„ kwargsï¼Œè¯·å°†å®ƒä»¬è®¾ç½®åœ¨ `model_kwargs` ä¸­ã€‚å¯ç”¨æ¨¡å‹ kwargs çš„å®Œæ•´åˆ—è¡¨å¯åœ¨ [LlamaCPP æ–‡æ¡£](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__init__) ä¸­æ‰¾åˆ°ã€‚\n", "\n", "å¯¹äºéœ€è¦åœ¨æ¨æ–­æœŸé—´ä¼ å…¥çš„ä»»ä½• kwargsï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬è®¾ç½®åœ¨ `generate_kwargs` ä¸­ã€‚åœ¨è¿™é‡ŒæŸ¥çœ‹å®Œæ•´çš„ [generate kwargs åˆ—è¡¨](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__call__)ã€‚\n", "\n", "ä¸€èˆ¬æ¥è¯´ï¼Œé»˜è®¤å€¼æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ä¸‹é¢çš„ç¤ºä¾‹æ˜¾ç¤ºäº†æ‰€æœ‰é»˜è®¤é…ç½®ã€‚\n", "\n", "å¦‚ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬åœ¨æœ¬ç¬”è®°æœ¬ä¸­ä½¿ç”¨çš„æ˜¯ [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) æ¨¡å‹ï¼Œå®ƒä½¿ç”¨ `ggmlv3` æ¨¡å‹æ ¼å¼ã€‚å¦‚æœæ‚¨æ­£åœ¨è¿è¡Œçš„ `llama-cpp-python` ç‰ˆæœ¬å¤§äº `0.1.79`ï¼Œæ‚¨å¯ä»¥å°†ä¸‹é¢çš„ `model_url` æ›¿æ¢ä¸º `\"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"`ã€‚\n"]}, {"cell_type": "markdown", "id": "59b27895", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨Colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "439960c5", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "2640c7a4", "metadata": {}, "outputs": [], "source": ["model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\""]}, {"cell_type": "code", "execution_count": null, "id": "6fa0ec4f-03ff-4e28-957f-b4b99a0faa20", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["llama.cpp: loading model from /Users/rchan/Library/Caches/llama_index/models/llama-2-13b-chat.ggmlv3.q4_0.bin\n", "llama_model_load_internal: format     = ggjt v3 (latest)\n", "llama_model_load_internal: n_vocab    = 32000\n", "llama_model_load_internal: n_ctx      = 3900\n", "llama_model_load_internal: n_embd     = 5120\n", "llama_model_load_internal: n_mult     = 256\n", "llama_model_load_internal: n_head     = 40\n", "llama_model_load_internal: n_head_kv  = 40\n", "llama_model_load_internal: n_layer    = 40\n", "llama_model_load_internal: n_rot      = 128\n", "llama_model_load_internal: n_gqa      = 1\n", "llama_model_load_internal: rnorm_eps  = 5.0e-06\n", "llama_model_load_internal: n_ff       = 13824\n", "llama_model_load_internal: freq_base  = 10000.0\n", "llama_model_load_internal: freq_scale = 1\n", "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n", "llama_model_load_internal: model size = 13B\n", "llama_model_load_internal: ggml ctx size =    0.11 MB\n", "llama_model_load_internal: mem required  = 6983.72 MB (+ 3046.88 MB per state)\n", "llama_new_context_with_model: kv self size  = 3046.88 MB\n", "ggml_metal_init: allocating\n", "ggml_metal_init: loading '/Users/rchan/opt/miniconda3/envs/llama-index/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n", "ggml_metal_init: loaded kernel_add                            0x14ff4f060\n", "ggml_metal_init: loaded kernel_add_row                        0x14ff4f2c0\n", "ggml_metal_init: loaded kernel_mul                            0x14ff4f520\n", "ggml_metal_init: loaded kernel_mul_row                        0x14ff4f780\n", "ggml_metal_init: loaded kernel_scale                          0x14ff4f9e0\n", "ggml_metal_init: loaded kernel_silu                           0x14ff4fc40\n", "ggml_metal_init: loaded kernel_relu                           0x14ff4fea0\n", "ggml_metal_init: loaded kernel_gelu                           0x11f7aef50\n", "ggml_metal_init: loaded kernel_soft_max                       0x11f7af380\n", "ggml_metal_init: loaded kernel_diag_mask_inf                  0x11f7af5e0\n", "ggml_metal_init: loaded kernel_get_rows_f16                   0x11f7af840\n", "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x11f7afaa0\n", "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x13ffba0c0\n", "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x13ffba320\n", "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x13ffba580\n", "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x13ffbaab0\n", "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x13ffbaea0\n", "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x13ffbb290\n", "ggml_metal_init: loaded kernel_rms_norm                       0x13ffbb690\n", "ggml_metal_init: loaded kernel_norm                           0x13ffbba80\n", "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x13ffbc070\n", "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x13ffbc510\n", "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x11f7aff40\n", "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x11f7b03e0\n", "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x11f7b0880\n", "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x11f7b0d20\n", "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x11f7b11c0\n", "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x11f7b1860\n", "ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x11f7b1d40\n", "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x11f7b2220\n", "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x11f7b2700\n", "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x11f7b2be0\n", "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x11f7b30c0\n", "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x11f7b35a0\n", "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x11f7b3a80\n", "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x11f7b3f60\n", "ggml_metal_init: loaded kernel_rope                           0x11f7b41c0\n", "ggml_metal_init: loaded kernel_alibi_f32                      0x11f7b47c0\n", "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x11f7b4d90\n", "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x11f7b5360\n", "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x11f7b5930\n", "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n", "ggml_metal_init: hasUnifiedMemory             = true\n", "ggml_metal_init: maxTransferRate              = built-in GPU\n", "llama_new_context_with_model: compute buffer total size =  356.03 MB\n", "llama_new_context_with_model: max tensor size =    87.89 MB\n", "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6984.50 / 21845.34)\n", "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.36 MB, ( 6985.86 / 21845.34)\n", "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3048.88 MB, (10034.73 / 21845.34)\n", "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   354.70 MB, (10389.44 / 21845.34)\n", "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 | \n"]}], "source": ["llm = LlamaCPP(", "    # æ‚¨å¯ä»¥ä¼ å…¥GGMLæ¨¡å‹çš„URLä»¥è‡ªåŠ¨ä¸‹è½½", "    model_url=model_url,", "    # å¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥è®¾ç½®é¢„å…ˆä¸‹è½½çš„æ¨¡å‹çš„è·¯å¾„ï¼Œè€Œä¸æ˜¯model_url", "    model_path=None,", "    temperature=0.1,", "    max_new_tokens=256,", "    # llama2å…·æœ‰4096ä¸ªæ ‡è®°çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œä½†æˆ‘ä»¬å°†å…¶è®¾ç½®å¾—è¾ƒä½ä»¥ç•™æœ‰ä¸€äº›ä½™åœ°", "    context_window=3900,", "    # ä¼ é€’ç»™__call__()çš„kwargs", "    generate_kwargs={},", "    # ä¼ é€’ç»™__init__()çš„kwargs", "    # è®¾ç½®ä¸ºè‡³å°‘1ä»¥ä½¿ç”¨GPU", "    model_kwargs={\"n_gpu_layers\": 1},", "    # å°†è¾“å…¥è½¬æ¢ä¸ºLlama2æ ¼å¼", "    messages_to_prompt=messages_to_prompt,", "    completion_to_prompt=completion_to_prompt,", "    verbose=True,", ")"]}, {"cell_type": "markdown", "id": "445453b1", "metadata": {}, "source": ["æˆ‘ä»¬å¯ä»¥é€šè¿‡æ—¥å¿—è®°å½•æ¥åˆ¤æ–­æ¨¡å‹æ­£åœ¨ä½¿ç”¨`metal`ï¼\n"]}, {"cell_type": "markdown", "id": "5e2e6a78-7e5d-4915-bcbf-6087edb30276", "metadata": {}, "source": ["## å¼€å§‹ä½¿ç”¨æˆ‘ä»¬çš„ `LlamaCPP` LLM æŠ½è±¡ï¼\n", "\n", "æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨æˆ‘ä»¬çš„ `LlamaCPP` LLM æŠ½è±¡çš„ `complete` æ–¹æ³•æ¥ç”Ÿæˆç»™å®šæç¤ºçš„è¡¥å…¨ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "5cfaf34c-0348-415e-98bb-83f782d64fe9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["  Of course, I'd be happy to help! Here's a short poem about cats and dogs:\n", "\n", "Cats and dogs, so different yet the same,\n", "Both furry friends, with their own special game.\n", "\n", "Cats purr and curl up tight,\n", "Dogs wag their tails with delight.\n", "\n", "Cats hunt mice with stealthy grace,\n", "Dogs chase after balls with joyful pace.\n", "\n", "But despite their differences, they share,\n", "A love for play and a love so fair.\n", "\n", "So here's to our feline and canine friends,\n", "Both equally dear, and both equally grand.\n"]}, {"name": "stderr", "output_type": "stream", "text": ["\n", "llama_print_timings:        load time =  1204.19 ms\n", "llama_print_timings:      sample time =   106.79 ms /   146 runs   (    0.73 ms per token,  1367.14 tokens per second)\n", "llama_print_timings: prompt eval time =  1204.14 ms /    81 tokens (   14.87 ms per token,    67.27 tokens per second)\n", "llama_print_timings:        eval time =  7468.88 ms /   145 runs   (   51.51 ms per token,    19.41 tokens per second)\n", "llama_print_timings:       total time =  8993.90 ms\n"]}], "source": ["response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n", "print(response.text)"]}, {"cell_type": "markdown", "id": "9038f7d7", "metadata": {}, "source": ["æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`stream_complete`ç«¯ç‚¹æ¥æµå¼ä¼ è¾“å“åº”ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ•´ä¸ªå“åº”ç”Ÿæˆå®Œæ¯•ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "7b059409-cd9d-4651-979c-03b3943e94af", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Llama.generate: prefix-match hit\n"]}, {"name": "stdout", "output_type": "stream", "text": ["  Sure! Here's a poem about fast cars:\n", "\n", "Fast cars, sleek and strong\n", "Racing down the highway all day long\n", "Their engines purring smooth and sweet\n", "As they speed through the streets\n", "\n", "Their wheels grip the road with might\n", "As they take off like a shot in flight\n", "The wind rushes past with a roar\n", "As they leave all else behind\n", "\n", "With paint that shines like the sun\n", "And lines that curve like a dream\n", "They're a sight to behold, my son\n", "These fast cars, so sleek and serene\n", "\n", "So if you ever see one pass\n", "Don't be afraid to give a cheer\n", "For these machines of speed and grace\n", "Are truly something to admire and revere."]}, {"name": "stderr", "output_type": "stream", "text": ["\n", "llama_print_timings:        load time =  1204.19 ms\n", "llama_print_timings:      sample time =   123.72 ms /   169 runs   (    0.73 ms per token,  1365.97 tokens per second)\n", "llama_print_timings: prompt eval time =   267.03 ms /    14 tokens (   19.07 ms per token,    52.43 tokens per second)\n", "llama_print_timings:        eval time =  8794.21 ms /   168 runs   (   52.35 ms per token,    19.10 tokens per second)\n", "llama_print_timings:       total time =  9485.38 ms\n"]}], "source": ["response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n", "for response in response_iter:\n", "    print(response.delta, end=\"\", flush=True)"]}, {"cell_type": "markdown", "id": "f7617600", "metadata": {}, "source": ["## ä½¿ç”¨LlamaCPPè®¾ç½®æŸ¥è¯¢å¼•æ“\n", "\n", "æˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ï¼Œç®€å•åœ°å°†`LlamaCPP` LLMæŠ½è±¡ä¼ é€’ç»™`LlamaIndex`æŸ¥è¯¢å¼•æ“ã€‚\n", "\n", "ä½†é¦–å…ˆï¼Œè®©æˆ‘ä»¬å°†å…¨å±€åˆ†è¯å™¨æ›´æ”¹ä¸ºä¸æˆ‘ä»¬çš„LLMåŒ¹é…ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "d8ff0c0b", "metadata": {}, "outputs": [], "source": ["from llama_index.core import set_global_tokenizer\n", "from transformers import AutoTokenizer\n", "\n", "set_global_tokenizer(\n", "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "d4c6f564", "metadata": {}, "outputs": [], "source": ["# ä½¿ç”¨HuggingfaceåµŒå…¥", "from llama_index.embeddings.huggingface import HuggingFaceEmbedding", "", "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"]}, {"cell_type": "code", "execution_count": null, "id": "5d485f1e", "metadata": {}, "outputs": [], "source": ["# åŠ è½½æ–‡æ¡£", "documents = SimpleDirectoryReader(", "    \"../../../examples/paul_graham_essay/data\"", ").load_data()"]}, {"cell_type": "code", "execution_count": null, "id": "c55c33cd", "metadata": {}, "outputs": [], "source": ["# åˆ›å»ºå‘é‡å­˜å‚¨ç´¢å¼•", "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"]}, {"cell_type": "code", "execution_count": null, "id": "e07659c8", "metadata": {}, "outputs": [], "source": ["# è®¾ç½®æŸ¥è¯¢å¼•æ“", "query_engine = index.as_query_engine(llm=llm)"]}, {"cell_type": "code", "execution_count": null, "id": "64e095c5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Llama.generate: prefix-match hit\n"]}, {"name": "stdout", "output_type": "stream", "text": ["  Based on the given context information, the author's childhood activities were writing short stories and programming. They wrote programs on punch cards using an early version of Fortran and later used a TRS-80 microcomputer to write simple games, a program to predict the height of model rockets, and a word processor that their father used to write at least one book.\n"]}, {"name": "stderr", "output_type": "stream", "text": ["\n", "llama_print_timings:        load time =  1204.19 ms\n", "llama_print_timings:      sample time =    56.13 ms /    80 runs   (    0.70 ms per token,  1425.21 tokens per second)\n", "llama_print_timings: prompt eval time = 65280.71 ms /  2272 tokens (   28.73 ms per token,    34.80 tokens per second)\n", "llama_print_timings:        eval time =  6877.38 ms /    79 runs   (   87.06 ms per token,    11.49 tokens per second)\n", "llama_print_timings:       total time = 72315.85 ms\n"]}], "source": ["response = query_engine.query(\"What did the author do growing up?\")\n", "print(response)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}