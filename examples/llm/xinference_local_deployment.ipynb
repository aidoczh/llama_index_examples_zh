{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "c6218e91", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/XinferenceLocalDeployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "id": "7096589b-daaf-440a-b89d-b4956f2db4b2", "metadata": {}, "source": ["# Xorbitsæ¨æ–­\n", "\n", "åœ¨è¿™ä¸ªæ¼”ç¤ºç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨Xorbitsæ¨æ–­ï¼ˆç®€ç§°Xinferenceï¼‰æ¥åœ¨ä¸‰ä¸ªæ­¥éª¤ä¸­éƒ¨ç½²æœ¬åœ°LLMã€‚\n", "\n", "åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Llama 2èŠå¤©æ¨¡å‹çš„GGMLæ ¼å¼ï¼Œä½†æ˜¯ä»£ç åº”è¯¥å¾ˆå®¹æ˜“åœ°è½¬ç§»åˆ°Xinferenceæ”¯æŒçš„æ‰€æœ‰LLMèŠå¤©æ¨¡å‹ä¸Šã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š\n", "\n", "| åç§°          | ç±»å‹             | è¯­è¨€ | æ ¼å¼   | å¤§å°ï¼ˆåäº¿ï¼‰ | é‡åŒ–                                |\n", "|---------------|------------------|------|--------|--------------|-------------------------------------|\n", "| llama-2-chat  | RLHFæ¨¡å‹         | en   | ggmlv3 | 7, 13, 70    | 'q2_K', 'q3_K_L', ... , 'q6_K', 'q8_0' |\n", "| chatglm       | SFTæ¨¡å‹          | en, zh | ggmlv3 | 6            | 'q4_0', 'q4_1', 'q5_0', 'q5_1', 'q8_0' |\n", "| chatglm2      | SFTæ¨¡å‹          | en, zh | ggmlv3 | 6            | 'q4_0', 'q4_1', 'q5_0', 'q5_1', 'q8_0' |\n", "| wizardlm-v1.0 | SFTæ¨¡å‹          | en   | ggmlv3 | 7, 13, 33    | 'q2_K', 'q3_K_L', ... , 'q6_K', 'q8_0' |\n", "| wizardlm-v1.1 | SFTæ¨¡å‹          | en   | ggmlv3 | 13           | 'q2_K', 'q3_K_L', ... , 'q6_K', 'q8_0' |\n", "| vicuna-v1.3   | SFTæ¨¡å‹          | en   | ggmlv3 | 7, 13        | 'q2_K', 'q3_K_L', ... , 'q6_K', 'q8_0' |\n", "\n", "æœ€æ–°çš„æ”¯æŒæ¨¡å‹å®Œæ•´åˆ—è¡¨å¯ä»¥åœ¨Xorbitsæ¨æ–­çš„[å®˜æ–¹GitHubé¡µé¢](https://github.com/xorbitsai/inference/blob/main/README.md)ä¸­æ‰¾åˆ°ã€‚\n"]}, {"cell_type": "markdown", "id": "d8cfbe6f-4c50-4c4f-90f9-03bb91201ef5", "metadata": {}, "source": ["## <span style=\"font-size: xx-large;;\">ğŸ¤–  </span> å®‰è£… Xinference\n", "\n", "i. åœ¨ç»ˆç«¯çª—å£ä¸­è¿è¡Œ `pip install \"xinference[all]\"`ã€‚\n", "\n", "ii. å®‰è£…å®Œæˆåï¼Œé‡æ–°å¯åŠ¨è¿™ä¸ªjupyterç¬”è®°æœ¬ã€‚\n", "\n", "iii. åœ¨æ–°çš„ç»ˆç«¯çª—å£ä¸­è¿è¡Œ `xinference`ã€‚\n", "\n", "iv. ä½ åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹è¾“å‡ºï¼š\n", "\n", "```\n", "INFO:xinference:Xinference successfully started. Endpoint: http://127.0.0.1:9997\n", "INFO:xinference.core.service:Worker 127.0.0.1:21561 has been added successfully\n", "INFO:xinference.deploy.worker:Xinference worker successfully started.\n", "```\n", "\n", "v. åœ¨ç«¯ç‚¹æè¿°ä¸­ï¼Œæ‰¾åˆ°å†’å·åçš„ç«¯å£å·ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç«¯å£å·æ˜¯ `9997`ã€‚\n", "\n", "vi. ä½¿ç”¨ä»¥ä¸‹å•å…ƒæ ¼è®¾ç½®ç«¯å£å·ï¼š\n"]}, {"cell_type": "code", "execution_count": null, "id": "790947f0", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-xinference"]}, {"cell_type": "code", "execution_count": null, "id": "5d520d56", "metadata": {}, "outputs": [], "source": ["ç«¯å£ = 9997  # æ›¿æ¢ä¸ºæ‚¨çš„ç«¯ç‚¹ç«¯å£å·"]}, {"cell_type": "markdown", "id": "93139076", "metadata": {}, "source": ["## <span style=\"font-size: xx-large;;\">ğŸš€  </span> å¯åŠ¨æœ¬åœ°æ¨¡å‹\n", "\n", "åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†ä»`llama_index`å¯¼å…¥ç›¸å…³çš„åº“ã€‚\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "1a6c1642", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "97577a07", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "fd1d259c", "metadata": {}, "outputs": [], "source": ["# å¦‚æœæ— æ³•å¯¼å…¥Xinferenceï¼Œåˆ™å¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨jupyterç¬”è®°æœ¬", "from llama_index.core import SummaryIndex", "from llama_index.core import (", "    TreeIndex,", "    VectorStoreIndex,", "    KeywordTableIndex,", "    KnowledgeGraphIndex,", "    SimpleDirectoryReader,", ")", "from llama_index.llms.xinference import Xinference", "from xinference.client import RESTfulClient", "from IPython.display import Markdown, display"]}, {"cell_type": "markdown", "id": "7a2dce47", "metadata": {}, "source": ["ç„¶åï¼Œæˆ‘ä»¬å¯åŠ¨ä¸€ä¸ªæ¨¡å‹å¹¶ä½¿ç”¨å®ƒã€‚è¿™æ ·å¯ä»¥è®©æˆ‘ä»¬åœ¨åç»­æ­¥éª¤ä¸­å°†æ¨¡å‹è¿æ¥åˆ°æ–‡æ¡£å’ŒæŸ¥è¯¢ã€‚\n", "\n", "è¯·éšæ„æ›´æ”¹å‚æ•°ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼ä¸ºäº†è¾¾åˆ°æœ€ä½³ç»“æœï¼Œå»ºè®®ä½¿ç”¨å¤§å°è¶…è¿‡13Bçš„æ¨¡å‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ7Bæ¨¡å‹å¯¹äºè¿™ä¸ªç®€çŸ­çš„æ¼”ç¤ºå·²ç»è¶³å¤Ÿäº†ã€‚\n", "\n", "ä»¥ä¸‹æ˜¯GGMLæ ¼å¼ä¸­Llama 2èŠå¤©æ¨¡å‹çš„ä¸€äº›æ›´å¤šå‚æ•°é€‰é¡¹ï¼ŒæŒ‰ç…§å ç”¨ç©ºé—´æœ€å°‘åˆ°èµ„æºæ¶ˆè€—æœ€å¤šä½†æ€§èƒ½é«˜çš„é¡ºåºåˆ—å‡ºã€‚\n", "\n", "<span style=\"font-weight: bold; ;\">æ¨¡å‹å¤§å°ï¼ˆä»¥åäº¿ä¸ºå•ä½ï¼‰ï¼š</span>\n", "\n", "`7`, `13`, `70`\n", "\n", "<span style=\"font-weight: bold; ;\">7Bå’Œ13Bæ¨¡å‹çš„é‡åŒ–é€‰é¡¹ï¼š</span>\n", "\n", "`q2_K`, `q3_K_L`, `q3_K_M`, `q3_K_S`, `q4_0`, `q4_1`, `q4_K_M`, `q4_K_S`, `q5_0`, `q5_1`, `q5_K_M`, `q5_K_S`, `q6_K`, `q8_0`\n", "\n", "<span style=\"font-weight: bold; ;\">70Bæ¨¡å‹çš„é‡åŒ–é€‰é¡¹ï¼š</span>\n", "\n", "`q4_0`\n"]}, {"cell_type": "code", "execution_count": null, "id": "b48c6d7a-7a38-440b-8ecb-f43f9050ee54", "metadata": {}, "outputs": [], "source": ["# å®šä¹‰ä¸€ä¸ªå®¢æˆ·ç«¯ï¼Œç”¨äºå‘xinferenceå‘é€å‘½ä»¤", "client = RESTfulClient(f\"http://localhost:{port}\")", "", "# ä¸‹è½½å¹¶å¯åŠ¨ä¸€ä¸ªæ¨¡å‹ï¼Œç¬¬ä¸€æ¬¡å¯èƒ½éœ€è¦ä¸€æ®µæ—¶é—´", "model_uid = client.launch_model(", "    model_name=\"llama-2-chat\",", "    model_size_in_billions=7,", "    model_format=\"ggmlv3\",", "    quantization=\"q2_K\",", ")", "", "# åˆå§‹åŒ–Xinferenceå¯¹è±¡ä»¥ä½¿ç”¨LLM", "llm = Xinference(", "    endpoint=f\"http://localhost:{port}\",", "    model_uid=model_uid,", "    temperature=0.0,", "    max_tokens=512,", ")"]}, {"cell_type": "markdown", "id": "094a02b7", "metadata": {}, "source": ["## <span style=\"font-size: xx-large;;\">ğŸ•º  </span> ç´¢å¼•æ•°æ®...å¹¶èŠå¤©ï¼\n", "\n", "åœ¨è¿™ä¸€æ­¥ä¸­ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å’Œæ•°æ®ç»“åˆèµ·æ¥åˆ›å»ºä¸€ä¸ªæŸ¥è¯¢å¼•æ“ã€‚æŸ¥è¯¢å¼•æ“å¯ä»¥ç”¨ä½œèŠå¤©æœºå™¨äººï¼Œæ ¹æ®ç»™å®šçš„æ•°æ®å›ç­”æˆ‘ä»¬çš„æŸ¥è¯¢ã€‚\n", "\n", "æˆ‘ä»¬å°†ä½¿ç”¨`VetorStoreIndex`ï¼Œå› ä¸ºå®ƒç›¸å¯¹è¾ƒå¿«ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¯ä»¥éšæ„æ›´æ”¹ç´¢å¼•ä»¥è·å¾—ä¸åŒçš„ä½“éªŒã€‚ä»¥ä¸‹æ˜¯ä»ä¸Šä¸€æ­¥å·²ç»å¯¼å…¥çš„ä¸€äº›å¯ç”¨ç´¢å¼•ï¼š\n", "\n", "`ListIndex`ã€`TreeIndex`ã€`VetorStoreIndex`ã€`KeywordTableIndex`ã€`KnowledgeGraphIndex`\n", "\n", "è¦æ›´æ”¹ç´¢å¼•ï¼Œåªéœ€åœ¨ä¸‹é¢çš„ä»£ç ä¸­å°†`VetorStoreIndex`æ›¿æ¢ä¸ºå¦ä¸€ä¸ªç´¢å¼•å³å¯ã€‚\n", "\n", "æ‰€æœ‰å¯ç”¨ç´¢å¼•çš„æœ€æ–°å®Œæ•´åˆ—è¡¨å¯ä»¥åœ¨ Llama Index çš„[å®˜æ–¹æ–‡æ¡£](https://gpt-index.readthedocs.io/en/latest/core_modules/data_modules/index/modules.html)ä¸­æ‰¾åˆ°ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "708b323e-d314-4b83-864b-22a1ead60de9", "metadata": {}, "outputs": [], "source": ["# ä»æ•°æ®ä¸­åˆ›å»ºç´¢å¼•", "documents = SimpleDirectoryReader(\"../data/paul_graham\").load_data()", "", "# åœ¨ä¸‹é¢çš„ä»£ç ä¸­æ›´æ”¹ç´¢å¼•åç§°", "index = VectorStoreIndex.from_documents(documents=documents)", "", "# åˆ›å»ºæŸ¥è¯¢å¼•æ“", "query_engine = index.as_query_engine(llm=llm)"]}, {"cell_type": "markdown", "id": "548174a2", "metadata": {}, "source": ["æˆ‘ä»¬å¯ä»¥åœ¨è¯¢é—®é—®é¢˜ä¹‹å‰é€šè¿‡`Xinference`å¯¹è±¡ç›´æ¥è®¾ç½®æ¸©åº¦å’Œæœ€å¤§ç­”æ¡ˆé•¿åº¦ï¼ˆä»¥æ ‡è®°ä¸ºå•ä½ï¼‰ã€‚è¿™æ ·å¯ä»¥åœ¨ä¸éœ€è¦æ¯æ¬¡é‡å»ºæŸ¥è¯¢å¼•æ“çš„æƒ…å†µä¸‹ä¸ºä¸åŒçš„é—®é¢˜æ›´æ”¹å‚æ•°ã€‚\n", "\n", "`temperature`æ˜¯ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„æ•°å­—ï¼Œç”¨äºæ§åˆ¶å“åº”çš„éšæœºæ€§ã€‚è¾ƒé«˜çš„å€¼ä¼šå¢åŠ åˆ›é€ åŠ›ï¼Œä½†å¯èƒ½å¯¼è‡´è·‘é¢˜çš„å›å¤ã€‚å°†å…¶è®¾ç½®ä¸ºé›¶å¯ä»¥ç¡®ä¿æ¯æ¬¡éƒ½å¾—åˆ°ç›¸åŒçš„å›å¤ã€‚\n", "\n", "`max_tokens`æ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œç”¨äºè®¾ç½®å“åº”é•¿åº¦çš„ä¸Šé™ã€‚å¦‚æœç­”æ¡ˆä¼¼ä¹è¢«æˆªæ–­ï¼Œå¯ä»¥å¢åŠ è¿™ä¸ªå€¼ï¼Œä½†è¦æ³¨æ„å¤ªé•¿çš„å›å¤å¯èƒ½ä¼šè¶…å‡ºä¸Šä¸‹æ–‡çª—å£å¹¶å¯¼è‡´é”™è¯¯ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "e0b32ddb", "metadata": {}, "outputs": [], "source": ["# å¯é€‰åœ°ï¼Œæ›´æ–°æ¸©åº¦å’Œæœ€å¤§ç­”æ¡ˆé•¿åº¦ï¼ˆä»¥æ ‡è®°ä¸ºå•ä½ï¼‰", "llm.__dict__.update({\"temperature\": 0.0})", "llm.__dict__.update({\"max_tokens\": 2048})", "", "# æå‡ºä¸€ä¸ªé—®é¢˜å¹¶æ˜¾ç¤ºç­”æ¡ˆ", "question = \"ä½œè€…åœ¨Y Combinatorç»“æŸååšäº†ä»€ä¹ˆï¼Ÿ\"", "", "response = query_engine.query(question)", "display(Markdown(f\"<b>{response}</b>\"))"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}