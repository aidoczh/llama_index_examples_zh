{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/maritalk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n", "\n", "# Maritalk\n", "\n", "## 简介\n", "\n", "MariTalk是由巴西公司[Maritaca AI](https://www.maritaca.ai)开发的助手。\n", "MariTalk基于经过特别训练以很好地理解葡萄牙语的语言模型。\n", "\n", "本笔记本演示了如何通过两个示例与Llama Index一起使用MariTalk：\n", "\n", "1. 使用聊天方法获取宠物名字建议;\n", "2. 使用完整方法将电影评论分类为负面或正面，带有少量示例。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 安装\n", "如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index\n", "!pip install llama-index-llms-maritalk\n", "!pip install asyncio"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## API密钥\n", "您需要一个API密钥，可以从chat.maritaca.ai（“API密钥”部分）获取。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 示例1 - 宠物名建议与聊天\n", "\n", "这个示例演示了如何使用聊天来为宠物提供名字建议。我们将使用一个简单的聊天界面来与用户交互，然后根据用户输入的信息提供宠物名字的建议。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage", "from llama_index.llms.maritalk import Maritalk", "", "import asyncio", "", "# 要自定义您的API密钥，请执行以下操作", "# 否则，它将查找您的环境变量中的MARITALK_API_KEY", "llm = Maritalk(api_key=\"<your_maritalk_api_key>\", model=\"sabia-2-medium\")", "", "# 使用消息列表调用聊天", "messages = [", "    ChatMessage(", "        role=\"system\",", "        content=\"You are an assistant specialized in suggesting pet names. Given the animal, you must suggest 4 names.\",", "    ),", "    ChatMessage(role=\"user\", content=\"I have a dog.\"),", "]", "", "# 同步聊天", "response = llm.chat(messages)", "print(response)", "", "", "# 异步聊天", "async def get_dog_name(llm, messages):", "    response = await llm.achat(messages)", "    print(response)", "", "", "asyncio.run(get_dog_name(llm, messages))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 流式生成\n", "\n", "对于涉及生成长文本的任务，比如创建一篇详尽的文章或翻译大型文档，逐步接收响应可能比等待完整文本更有优势。这样做可以使应用程序更具响应性和效率，特别是当生成的文本非常庞大时。我们提供两种满足这种需求的方法：一种是同步的，另一种是异步的。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 同步流式聊天", "response = llm.stream_chat(messages)", "for chunk in response:", "    print(chunk.delta, end=\"\", flush=True)", "", "", "# 异步流式聊天", "async def get_dog_name_streaming(llm, messages):", "    async for chunk in await llm.astream_chat(messages):", "        print(chunk.delta, end=\"\", flush=True)", "", "", "asyncio.run(get_dog_name_streaming(llm, messages))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 示例2 - 使用完整的少样本示例\n", "\n", "我们建议在使用少样本示例时使用`llm.complete()`方法。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["prompt = \"\"\"将电影评论分类为“积极”或“消极”。", "", "评论：我非常喜欢这部电影，是今年最好的！", "类别：积极", "", "评论：这部电影令人失望。", "类别：消极", "", "评论：虽然很长，但票价还是值得的。", "类别：\"\"\"", " ", "# 同步完成", "response = llm.complete(prompt)", "print(response)", "", "", "# 异步完成", "async def classify_review(llm, prompt):", "    response = await llm.acomplete(prompt)", "    print(response)", "", "", "asyncio.run(classify_review(llm, prompt))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 同步流式传输完成", "response = llm.stream_complete(prompt)", "for chunk in response:", "    print(chunk.delta, end=\"\", flush=True)", "", "", "# 异步流式传输完成", "async def classify_review_streaming(llm, prompt):", "    async for chunk in await llm.astream_complete(prompt):", "        print(chunk.delta, end=\"\", flush=True)", "", "", "asyncio.run(classify_review_streaming(llm, prompt))"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}