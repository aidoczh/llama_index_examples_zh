{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "978146e2", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "id": "f717d3d4-942b-4d86-9435-fc44b3ac6d39", "metadata": {}, "source": ["# Hugging Face è¯­è¨€æ¨¡å‹\n", "\n", "æœ‰è®¸å¤šç§æ–¹æ³•å¯ä»¥ä¸[Hugging Face](https://huggingface.co/)çš„è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚\n", "Hugging Faceæœ¬èº«æä¾›äº†å‡ ä¸ªPythonåŒ…æ¥å®ç°è®¿é—®ï¼Œ\n", "LlamaIndexå°†è¿™äº›åŒ…è£…æˆäº†`LLM`å®ä½“ï¼š\n", "\n", "- [`transformers`](https://github.com/huggingface/transformers) åŒ…ï¼š\n", "  ä½¿ç”¨ `llama_index.llms.HuggingFaceLLM`\n", "- [Hugging Face æ¨ç† API](https://huggingface.co/inference-api),\n", "  [ç”± `huggingface_hub[inference]` åŒ…è£…](https://github.com/huggingface/huggingface_hub)ï¼š\n", "  ä½¿ç”¨ `llama_index.llms.HuggingFaceInferenceAPI`\n", "\n", "è¿™ä¸¤è€…æœ‰éå¸¸å¤šçš„å¯èƒ½ç»„åˆæ–¹å¼ï¼Œå› æ­¤æœ¬ç¬”è®°æœ¬ä»…è¯¦ç»†ä»‹ç»äº†ä¸€äº›ã€‚\n", "è®©æˆ‘ä»¬ä»¥Hugging Faceçš„[æ–‡æœ¬ç”Ÿæˆä»»åŠ¡](https://huggingface.co/tasks/text-generation)ä½œä¸ºç¤ºä¾‹ã€‚\n"]}, {"cell_type": "markdown", "id": "90cf0f2e-8d8d-4e42-81bf-866c759221e1", "metadata": {}, "source": ["åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å®‰è£…äº†è¿™ä¸ªæ¼”ç¤ºæ‰€éœ€çš„åŒ…ï¼š\n", "\n", "- `transformers[torch]` æ˜¯ä¸ºäº† `HuggingFaceLLM`\n", "- `huggingface_hub[inference]` æ˜¯ä¸ºäº† `HuggingFaceInferenceAPI`\n", "- å¼•å·æ˜¯ä¸ºäº† Z shell (`zsh`)\n"]}, {"cell_type": "code", "execution_count": null, "id": "f413f179", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-huggingface"]}, {"cell_type": "code", "execution_count": null, "id": "3b04b4a5-6fce-4188-a538-9a5ce2fa56f6", "metadata": {}, "outputs": [], "source": ["!pip install \"transformers[torch]\" \"huggingface_hub[inference]\""]}, {"cell_type": "markdown", "id": "3dac8f9f-7136-43f7-9e9f-de679e74d66e", "metadata": {}, "source": ["ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç©ä¸€ä¸‹å§ï¼š\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "2c577674", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "86028752", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "0465029c-fe69-454a-9561-55f7a382b2e2", "metadata": {}, "outputs": [], "source": ["import os", "from typing import List, Optional", "", "from llama_index.llms.huggingface import (", "    HuggingFaceInferenceAPI,", "    HuggingFaceLLM,", ")", "", "# å‚è€ƒï¼šhttps://huggingface.co/docs/hub/security-tokens", "# æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªå…·æœ‰è¯»å–æƒé™çš„ä»¤ç‰Œæ¥è¿›è¡Œæ¼”ç¤º", "HF_TOKEN: Optional[str] = os.getenv(\"HUGGING_FACE_TOKEN\")", "# æ³¨æ„ï¼šå½“è¿™ä¸ªä»¤ç‰Œåœ¨HuggingFaceInferenceAPIä¸­è¢«ä½¿ç”¨æ—¶ï¼ŒNoneé»˜è®¤å°†å›é€€åˆ°Hugging Faceçš„ä»¤ç‰Œå­˜å‚¨ä¸­ã€‚"]}, {"cell_type": "code", "execution_count": null, "id": "a27feba3-d027-4d10-b1af-1e130e764a67", "metadata": {}, "outputs": [], "source": ["# è¿™é‡Œä½¿ç”¨äº† https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha", "# å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œåˆ™ä¼šä¸‹è½½åˆ°æœ¬åœ°çš„ Hugging Face æ¨¡å‹ç¼“å­˜ä¸­ï¼Œ", "# ç„¶ååœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œæ¨¡å‹", "locally_run = HuggingFaceLLM(model_name=\"HuggingFaceH4/zephyr-7b-alpha\")", "", "# è¿™å°†ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ï¼Œä½†åœ¨ Hugging Face çš„æœåŠ¡å™¨ä¸Šè¿œç¨‹è¿è¡Œï¼Œ", "# é€šè¿‡ Hugging Face æ¨ç† API è®¿é—®", "# è¯·æ³¨æ„ï¼Œä½¿ç”¨æ‚¨çš„ä»¤ç‰Œä¸ä¼šäº§ç”Ÿè´¹ç”¨ï¼Œ", "# æ¨ç† API æ˜¯å…è´¹çš„ï¼Œåªæ˜¯æœ‰é€Ÿç‡é™åˆ¶", "remotely_run = HuggingFaceInferenceAPI(", "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\", token=HF_TOKEN", ")", "", "# æˆ–è€…æ‚¨å¯ä»¥è·³è¿‡æä¾›ä»¤ç‰Œï¼ŒåŒ¿åä½¿ç”¨ Hugging Face æ¨ç† API", "remotely_run_anon = HuggingFaceInferenceAPI(", "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\"", ")", "", "# å¦‚æœæ‚¨æ²¡æœ‰å‘ HuggingFaceInferenceAPI æä¾› model_nameï¼Œ", "# åˆ™ä¼šä½¿ç”¨ Hugging Face æ¨èçš„æ¨¡å‹ï¼ˆæ„Ÿè°¢ huggingface_hubï¼‰", "remotely_run_recommended = HuggingFaceInferenceAPI(token=HF_TOKEN)", ""]}, {"cell_type": "markdown", "id": "b801bef7-2593-49e2-a550-721e6b796486", "metadata": {}, "source": ["ä½¿ç”¨`HuggingFaceInferenceAPI`å®Œæˆçš„åŸºç¡€æ˜¯Hugging Faceçš„[æ–‡æœ¬ç”Ÿæˆä»»åŠ¡](https://huggingface.co/tasks/text-generation)ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "631269c9-38ca-49d2-a7f0-f88e21adef6e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": [" beyond!\n", "The Infinity Wall Clock is a unique and stylish way to keep track of time. The clock is made of a durable, high-quality plastic and features a bright LED display. The Infinity Wall Clock is powered by batteries and can be mounted on any wall. It is a great addition to any home or office.\n"]}], "source": ["completion_response = remotely_run_recommended.complete(\"To infinity, and\")\n", "print(completion_response)"]}, {"cell_type": "markdown", "id": "dda1be10", "metadata": {}, "source": ["å¦‚æœæ‚¨ä¿®æ”¹äº†LLMï¼Œè¿˜åº”è¯¥ç›¸åº”åœ°ä¿®æ”¹å…¨å±€çš„åˆ†è¯å™¨ï¼\n"]}, {"cell_type": "code", "execution_count": null, "id": "12e0f3c0", "metadata": {}, "outputs": [], "source": ["from llama_index.core import set_global_tokenizer\n", "from transformers import AutoTokenizer\n", "\n", "set_global_tokenizer(\n", "    AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\").encode\n", ")"]}, {"cell_type": "markdown", "id": "3fa723d6-4308-4d94-9609-8c51ce8184c3", "metadata": {}, "source": ["å¦‚æœä½ æ„Ÿå…´è¶£ï¼Œå…¶ä»–Hugging Faceæ¨ç†APIä»»åŠ¡åŒ…æ‹¬ï¼š\n", "\n", "- `llama_index.llms.HuggingFaceInferenceAPI.chat`ï¼š[å¯¹è¯ä»»åŠ¡](https://huggingface.co/tasks/conversational)\n", "- `llama_index.embeddings.HuggingFaceInferenceAPIEmbedding`ï¼š[ç‰¹å¾æå–ä»»åŠ¡](https://huggingface.co/tasks/feature-extraction)\n", "\n", "æ˜¯çš„ï¼ŒHugging FaceåµŒå…¥æ¨¡å‹æ”¯æŒä»¥ä¸‹å†…å®¹ï¼š\n", "\n", "- `transformers[torch]`ï¼šç”±`HuggingFaceEmbedding`åŒ…è£…\n", "- `huggingface_hub[inference]`ï¼šç”±`HuggingFaceInferenceAPIEmbedding`åŒ…è£…\n", "\n", "ä¸Šè¿°ä¸¤ä¸ªéƒ½æ˜¯`llama_index.embeddings.base.BaseEmbedding`çš„å­ç±»ã€‚\n"]}, {"cell_type": "markdown", "id": "92c09b9f", "metadata": {}, "source": ["### ä½¿ç”¨Hugging Faceçš„`text-generation-inference`\n"]}, {"cell_type": "markdown", "id": "752520ec", "metadata": {}, "source": ["æ–°çš„`TextGenerationInference`ç±»å…è®¸ä¸è¿è¡Œ[`text-generation-inference`, TGI](https://huggingface.co/docs/text-generation-inference/index)çš„ç«¯ç‚¹è¿›è¡Œäº¤äº’ã€‚é™¤äº†å¿«é€Ÿçš„æ¨ç†ä¹‹å¤–ï¼Œå®ƒè¿˜æ”¯æŒä»ç‰ˆæœ¬`2.0.1`å¼€å§‹çš„`tool`ä½¿ç”¨ã€‚\n"]}, {"cell_type": "markdown", "id": "055ddcb1", "metadata": {}, "source": ["è¦åˆå§‹åŒ–`TextGenerationInference`çš„å®ä¾‹ï¼Œæ‚¨éœ€è¦æä¾›ç«¯ç‚¹URLï¼ˆTGIçš„è‡ªæ‰˜ç®¡å®ä¾‹æˆ–åœ¨Hugging Faceä¸Šåˆ›å»ºçš„å…¬å…±æ¨ç†ç«¯ç‚¹ï¼‰ã€‚å¯¹äºç§æœ‰æ¨ç†ç«¯ç‚¹ï¼Œéœ€è¦æä¾›æ‚¨çš„HFä»¤ç‰Œï¼ˆå¯ä»¥ä½œä¸ºåˆå§‹åŒ–å‚æ•°æˆ–ç¯å¢ƒå˜é‡ï¼‰ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "c02f350f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": [" beyond! This phrase is a reference to the famous line from the movie \"Toy Story\" when Buzz Lightyear, a toy astronaut, exclaims \"To infinity and beyond!\" as he soars through space. It has since become a catchphrase for reaching for the stars and striving for greatness. However, if you meant to ask a mathematical question, \"To infinity\" refers to a very large, infinite number, and \"and beyond\" could be interpreted as continuing infinitely in a certain direction. For example, \"2 to the power of infinity\" would represent a very large, infinite number.\n"]}], "source": ["", "# å¯¼å…¥å¿…è¦çš„åº“", "import os", "from typing import List, Optional", "", "from llama_index.llms.huggingface import (", "    TextGenerationInference,", ")", "", "# å®šä¹‰URLåœ°å€", "URL = \"your_tgi_endpoint\"", "model = TextGenerationInference(", "    model_url=URL, token=False", ")  # å¦‚æœæ˜¯å…¬å…±ç«¯ç‚¹ï¼Œè¯·å°†tokenè®¾ç½®ä¸ºFalse", "", "# è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬", "completion_response = model.complete(\"To infinity, and\")", "print(completion_response)"]}, {"cell_type": "markdown", "id": "e9270b99", "metadata": {}, "source": ["è¦ä½¿ç”¨`TextGenerationInference`å·¥å…·ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å·²ç»å­˜åœ¨çš„å·¥å…·ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªï¼š\n"]}, {"cell_type": "code", "execution_count": null, "id": "90a041cc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'tool_calls': [{'id': 0, 'type': 'function', 'function': {'description': None, 'name': 'get_current_weather_n_days', 'arguments': {'format': 'celsius', 'location': 'Paris, Ile-de-France', 'num_days': 7}}}]}\n"]}], "source": ["from typing import List, Literal", "from llama_index.core.bridge.pydantic import BaseModel, Field", "from llama_index.core.tools import FunctionTool", "from llama_index.core.base.llms.types import (", "    ChatMessage,", "    MessageRole,", ")", "", "", "def get_current_weather(location: str, format: str):", "    \"\"\"è·å–å½“å‰å¤©æ°”", "", "    Args:", "    location (str): åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚ï¼šæ—§é‡‘å±±ï¼ŒåŠ åˆ©ç¦å°¼äºš", "    format (str): è¦ä½¿ç”¨çš„æ¸©åº¦å•ä½ï¼ˆ'celsius' æˆ– 'fahrenheit'ï¼‰ã€‚ä»ç”¨æˆ·ä½ç½®æ¨æ–­å‡ºæ¥ã€‚", "    \"\"\"", "    ...", "", "", "class WeatherArgs(BaseModel):", "    location: str = Field(", "        description=\"åŸå¸‚å’Œåœ°åŒºï¼Œä¾‹å¦‚ï¼šå·´é»ï¼Œæ³•å…°è¥¿å²›\"", "    )", "    format: Literal[\"fahrenheit\", \"celsius\"] = Field(", "        description=\"è¦ä½¿ç”¨çš„æ¸©åº¦å•ä½ï¼ˆ'fahrenheit' æˆ– 'celsius'ï¼‰ã€‚ä»ä½ç½®æ¨æ–­å‡ºæ¥ã€‚\",", "    )", "", "", "weather_tool = FunctionTool.from_defaults(", "    fn=get_current_weather,", "    name=\"get_current_weather\",", "    description=\"è·å–å½“å‰å¤©æ°”\",", "    fn_schema=WeatherArgs,", ")", "", "", "def get_current_weather_n_days(location: str, format: str, num_days: int):", "    \"\"\"è·å–æœªæ¥Nå¤©çš„å¤©æ°”é¢„æŠ¥", "", "    Args:", "    location (str): åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚ï¼šæ—§é‡‘å±±ï¼ŒåŠ åˆ©ç¦å°¼äºš", "    format (str): è¦ä½¿ç”¨çš„æ¸©åº¦å•ä½ï¼ˆ'celsius' æˆ– 'fahrenheit'ï¼‰ã€‚ä»ç”¨æˆ·ä½ç½®æ¨æ–­å‡ºæ¥ã€‚", "    num_days (int): å¤©æ°”é¢„æŠ¥çš„å¤©æ•°ã€‚", "    \"\"\"", "    ...", "", "", "class ForecastArgs(BaseModel):", "    location: str = Field(", "        description=\"åŸå¸‚å’Œåœ°åŒºï¼Œä¾‹å¦‚ï¼šå·´é»ï¼Œæ³•å…°è¥¿å²›\"", "    )", "    format: Literal[\"fahrenheit\", \"celsius\"] = Field(", "        description=\"è¦ä½¿ç”¨çš„æ¸©åº¦å•ä½ï¼ˆ'fahrenheit' æˆ– 'celsius'ï¼‰ã€‚ä»ä½ç½®æ¨æ–­å‡ºæ¥ã€‚\",", "    )", "    num_days: int = Field(", "        description=\"å¤©æ°”é¢„æŠ¥çš„æŒç»­æ—¶é—´ï¼ˆå¤©ï¼‰ã€‚\",", "    )", "", "", "forecast_tool = FunctionTool.from_defaults(", "    fn=get_current_weather_n_days,", "    name=\"get_current_weather_n_days\",", "    description=\"è·å–æœªæ¥Nå¤©çš„å½“å‰å¤©æ°”\",", "    fn_schema=ForecastArgs,", ")", "", "usr_msg = ChatMessage(", "    role=MessageRole.USER,", "    content=\"å·´é»æœªæ¥ä¸€å‘¨çš„å¤©æ°”å¦‚ä½•ï¼Ÿ\",", ")", "", "response = model.chat_with_tools(", "    user_msg=usr_msg,", "    tools=[", "        weather_tool,", "        forecast_tool,", "    ],", "    tool_choice=\"get_current_weather_n_days\",", ")", "", "print(response.message.additional_kwargs)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}