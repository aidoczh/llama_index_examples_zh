{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "978146e2", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/openvino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "f717d3d4-942b-4d86-9435-fc44b3ac6d39", "metadata": {}, "source": ["# OpenVINO 语言模型\n", "\n", "[OpenVINO™](https://github.com/openvinotoolkit/openvino) 是一个用于优化和部署AI推断的开源工具包。OpenVINO™ Runtime 可以在各种硬件[设备](https://github.com/openvinotoolkit/openvino?tab=readme-ov-file#supported-hardware-matrix)上运行经过优化的相同模型。加速您在语言模型 + LLMs、计算机视觉、自动语音识别等用例中的深度学习性能。\n", "\n", "通过 `OpenVINOLLM` 实体封装的 LlamaIndex，可以在本地运行 OpenVINO 模型：\n"]}, {"cell_type": "markdown", "id": "90cf0f2e-8d8d-4e42-81bf-866c759221e1", "metadata": {}, "source": ["在下面的代码中，我们安装了这个演示所需的包：\n"]}, {"cell_type": "code", "execution_count": null, "id": "f413f179", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openvino transformers huggingface_hub"]}, {"cell_type": "markdown", "id": "3dac8f9f-7136-43f7-9e9f-de679e74d66e", "metadata": {}, "source": ["现在我们已经准备好了，让我们开始玩一下吧：\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "2c577674", "metadata": {}, "source": ["如果您在Colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "86028752", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "0465029c-fe69-454a-9561-55f7a382b2e2", "metadata": {}, "outputs": [], "source": ["from llama_index.llms.openvino import OpenVINOLLM"]}, {"cell_type": "code", "execution_count": null, "id": "49122583", "metadata": {}, "outputs": [], "source": ["def messages_to_prompt(messages):", "    prompt = \"\"", "    for message in messages:", "        if message.role == \"system\":", "            prompt += f\"<|system|>\\n{message.content}</s>\\n\"", "        elif message.role == \"user\":", "            prompt += f\"<|user|>\\n{message.content}</s>\\n\"", "        elif message.role == \"assistant\":", "            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"", "", "    # 确保我们以系统提示开始，如果需要则插入空白", "    if not prompt.startswith(\"<|system|>\\n\"):", "        prompt = \"<|system|>\\n</s>\\n\" + prompt", "", "    # 添加最终的助手提示", "    prompt = prompt + \"<|assistant|>\\n\"", "", "    return prompt", "", "", "def completion_to_prompt(completion):", "    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\""]}, {"cell_type": "markdown", "id": "d3e21cef-b3c3-4ddd-a70c-728de440648e", "metadata": {}, "source": ["### 模型加载\n", "\n", "可以通过使用 `OpenVINOLLM` 方法来指定模型参数来加载模型。\n", "\n", "如果你有英特尔GPU，可以指定 `device_map=\"gpu\"` 来在其上运行推断。\n"]}, {"cell_type": "code", "execution_count": null, "id": "a27feba3-d027-4d10-b1af-1e130e764a67", "metadata": {}, "outputs": [], "source": ["ov_config = {\n", "    \"PERFORMANCE_HINT\": \"LATENCY\",\n", "    \"NUM_STREAMS\": \"1\",\n", "    \"CACHE_DIR\": \"\",\n", "}\n", "\n", "ov_llm = OpenVINOLLM(\n", "    model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n", "    tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n", "    context_window=3900,\n", "    max_new_tokens=256,\n", "    model_kwargs={\"ov_config\": ov_config},\n", "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n", "    messages_to_prompt=messages_to_prompt,\n", "    completion_to_prompt=completion_to_prompt,\n", "    device_map=\"cpu\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "e25c7162", "metadata": {}, "outputs": [], "source": ["response = ov_llm.complete(\"What is the meaning of life?\")\n", "print(str(response))"]}, {"cell_type": "markdown", "id": "072dd59e-e3e7-41b9-b6fb-07bb41a82d2c", "metadata": {}, "source": ["### 使用本地OpenVINO模型进行推理\n", "\n", "可以使用命令行界面将您的模型[导出](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#export)为OpenVINO IR格式，并从本地文件夹加载模型。\n"]}, {"cell_type": "code", "execution_count": null, "id": "41dfc1a1-0aea-4136-a194-0428b89dc3cc", "metadata": {}, "outputs": [], "source": ["!optimum-cli export openvino --model HuggingFaceH4/zephyr-7b-beta ov_model_dir"]}, {"cell_type": "markdown", "id": "9e7683ab-66ae-4fbc-af20-6e3ec524d28a", "metadata": {}, "source": ["建议使用`--weight-format`对权重进行8位或4位量化，以减少推理延迟和模型占用空间。\n"]}, {"cell_type": "code", "execution_count": null, "id": "92d69e87", "metadata": {}, "outputs": [], "source": ["!optimum-cli export openvino --model HuggingFaceH4/zephyr-7b-beta --weight-format int8 ov_model_dir"]}, {"cell_type": "code", "execution_count": null, "id": "b9e96c9d", "metadata": {}, "outputs": [], "source": ["!optimum-cli export openvino --model HuggingFaceH4/zephyr-7b-beta --weight-format int4 ov_model_dir"]}, {"cell_type": "code", "execution_count": null, "id": "a6982d98", "metadata": {}, "outputs": [], "source": ["ov_llm = OpenVINOLLM(\n", "    model_name=\"ov_model_dir\",\n", "    tokenizer_name=\"ov_model_dir\",\n", "    context_window=3900,\n", "    max_new_tokens=256,\n", "    model_kwargs={\"ov_config\": ov_config},\n", "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n", "    messages_to_prompt=messages_to_prompt,\n", "    completion_to_prompt=completion_to_prompt,\n", "    device_map=\"gpu\",\n", ")"]}, {"cell_type": "markdown", "id": "8e26a478-2974-4ced-89e8-c13a64a409b2", "metadata": {}, "source": ["您可以通过激活的动态量化和KV-cache量化来获得额外的推理速度提升。可以通过以下方式在`ov_config`中启用这些选项：\n"]}, {"cell_type": "code", "execution_count": null, "id": "01c89828-a94b-4242-baf0-204eb0f1c87a", "metadata": {}, "outputs": [], "source": ["ov_config = {\n", "    \"KV_CACHE_PRECISION\": \"u8\",\n", "    \"DYNAMIC_QUANTIZATION_GROUP_SIZE\": \"32\",\n", "    \"PERFORMANCE_HINT\": \"LATENCY\",\n", "    \"NUM_STREAMS\": \"1\",\n", "    \"CACHE_DIR\": \"\",\n", "}"]}, {"cell_type": "markdown", "id": "dda1be10", "metadata": {}, "source": ["### 数据流\n", "\n", "使用 `stream_complete` 终端点\n"]}, {"cell_type": "code", "execution_count": null, "id": "12e0f3c0", "metadata": {}, "outputs": [], "source": ["response = ov_llm.stream_complete(\"Who is Paul Graham?\")\n", "for r in response:\n", "    print(r.delta, end=\"\")"]}, {"cell_type": "markdown", "id": "2c87c383", "metadata": {}, "source": ["使用 `stream_chat` 端点\n"]}, {"cell_type": "code", "execution_count": null, "id": "2db801a8", "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage\n", "\n", "messages = [\n", "    ChatMessage(\n", "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n", "    ),\n", "    ChatMessage(role=\"user\", content=\"What is your name\"),\n", "]\n", "resp = ov_llm.stream_chat(messages)\n", "\n", "for r in resp:\n", "    print(r.delta, end=\"\")"]}, {"cell_type": "markdown", "id": "3fa723d6-4308-4d94-9609-8c51ce8184c3", "metadata": {}, "source": ["有关更多信息，请参考：\n", "\n", "* [OpenVINO LLM指南](https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide.html)。\n", "\n", "* [OpenVINO文档](https://docs.openvino.ai/2024/home.html)。\n", "\n", "* [OpenVINO入门指南](https://www.intel.com/content/www/us/en/content-details/819067/openvino-get-started-guide.html)。\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}