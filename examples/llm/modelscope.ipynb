{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/llm/modelscope.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ModelScope LLMS\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["在这个笔记本中，我们将展示如何在LlamaIndex中使用ModelScope LLM模型。请查看[ModelScope网站](https://www.modelscope.cn/)。\n", "\n", "如果您在colab上打开这个笔记本，您需要安装LlamaIndex 🦙和modelscope。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index-llms-modelscope"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 基本用法\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import sys\n", "from llama_index.llms.modelscope import ModelScopeLLM\n", "\n", "llm = ModelScopeLLM(model_name=\"qwen/Qwen1.5-7B-Chat\", model_revision=\"master\")\n", "\n", "rsp = llm.complete(\"Hello, who are you?\")\n", "print(rsp)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 使用消息请求\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.base.llms.types import MessageRole, ChatMessage\n", "\n", "messages = [\n", "    ChatMessage(\n", "        role=MessageRole.SYSTEM, content=\"You are a helpful assistant.\"\n", "    ),\n", "    ChatMessage(role=MessageRole.USER, content=\"How to make cake?\"),\n", "]\n", "resp = llm.chat(messages)\n", "print(resp)"]}], "metadata": {"colab": {"name": "gemini.ipynb", "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 0}