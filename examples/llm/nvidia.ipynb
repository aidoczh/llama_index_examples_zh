{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 使用NVIDIA的LLM API目录连接器\n", "\n", "本笔记本将指导您了解`NVIDIA`连接器的基本用法。\n", "\n", "通过这个连接器，您将能够连接到NVIDIA的[API目录](https://build.nvidia.com/explore/discover)中可用的兼容模型，并生成模型，例如：\n", "\n", "- Google的[gemma-7b](https://build.nvidia.com/google/gemma-7b)\n", "- Mistal AI的[mistral-7b-instruct-v0.2](https://build.nvidia.com/mistralai/mistral-7b-instruct-v2)\n", "- 等等！\n", "\n", "我们将首先确保安装了`llama-index`和相关软件包。\n", "\n", "> 注意：目前，只有基本URL为`https://integrate.api.nvidia.com/v1`的模型与此连接器兼容。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Collecting llama-index-embeddings-openai\n", "  Using cached llama_index_embeddings_openai-0.1.7-py3-none-any.whl.metadata (603 bytes)\n", "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-embeddings-openai) (0.10.30)\n", "Requirement already satisfied: PyYAML>=6.0.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (6.0.1)\n", "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.0.29)\n", "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.9.5)\n", "Requirement already satisfied: dataclasses-json in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.6.4)\n", "Requirement already satisfied: deprecated>=1.2.9.3 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.2.14)\n", "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.8)\n", "Requirement already satisfied: fsspec>=2023.5.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.3.1)\n", "Requirement already satisfied: httpx in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.27.0)\n", "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.1.18)\n", "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.6.0)\n", "Requirement already satisfied: networkx>=3.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.1)\n", "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.8.1)\n", "Requirement already satisfied: numpy in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.24.4)\n", "Requirement already satisfied: openai>=1.1.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.22.0)\n", "Requirement already satisfied: pandas in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.0.3)\n", "Requirement already satisfied: pillow>=9.0.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (10.3.0)\n", "Requirement already satisfied: requests>=2.31.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.31.0)\n", "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (8.2.3)\n", "Requirement already satisfied: tiktoken>=0.3.3 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.6.0)\n", "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.66.2)\n", "Requirement already satisfied: typing-extensions>=4.5.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.11.0)\n", "Requirement already satisfied: typing-inspect>=0.8.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.9.0)\n", "Requirement already satisfied: wrapt in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.16.0)\n", "Requirement already satisfied: aiosignal>=1.1.2 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.3.1)\n", "Requirement already satisfied: attrs>=17.3.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (23.2.0)\n", "Requirement already satisfied: frozenlist>=1.1.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.4.1)\n", "Requirement already satisfied: multidict<7.0,>=4.5 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (6.0.5)\n", "Requirement already satisfied: yarl<2.0,>=1.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.9.4)\n", "Requirement already satisfied: pydantic>=1.10 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.7.0)\n", "Requirement already satisfied: anyio in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (4.3.0)\n", "Requirement already satisfied: certifi in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.2.2)\n", "Requirement already satisfied: httpcore==1.* in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.5)\n", "Requirement already satisfied: idna in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.7)\n", "Requirement already satisfied: sniffio in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.3.1)\n", "Requirement already satisfied: h11<0.15,>=0.13 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.14.0)\n", "Requirement already satisfied: click in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (8.1.7)\n", "Requirement already satisfied: joblib in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.4.0)\n", "Requirement already satisfied: regex>=2021.8.3 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.4.16)\n", "Requirement already satisfied: distro<2,>=1.7.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.9.0)\n", "Requirement already satisfied: charset-normalizer<4,>=2 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.3.2)\n", "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.2.1)\n", "Requirement already satisfied: greenlet!=0.4.17 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.0.3)\n", "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.0.0)\n", "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (3.21.1)\n", "Requirement already satisfied: python-dateutil>=2.8.2 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.9.0.post0)\n", "Requirement already satisfied: pytz>=2020.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.1)\n", "Requirement already satisfied: tzdata>=2022.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2024.1)\n", "Requirement already satisfied: packaging>=17.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (24.0)\n", "Requirement already satisfied: annotated-types>=0.4.0 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (0.6.0)\n", "Requirement already satisfied: pydantic-core==2.18.1 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (2.18.1)\n", "Requirement already satisfied: six>=1.5 in /home/chris/anaconda3/envs/nvidia-llama-index-api/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-openai) (1.16.0)\n", "Using cached llama_index_embeddings_openai-0.1.7-py3-none-any.whl (6.0 kB)\n", "Installing collected packages: llama-index-embeddings-openai\n", "Successfully installed llama-index-embeddings-openai-0.1.7\n"]}], "source": ["!pip install llama-index-embeddings-openai llama-index-readers-file"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## API密钥和样板文件\n", "\n", "在下一个单元格中，我们将运行一些样板文件，以便在笔记本环境中顺利执行示例。\n", "\n", "我们还将提供我们的API密钥。\n", "\n", "> 注意：您可以使用代码示例窗口中的“获取API密钥”按钮创建您自己的NVIDIA API密钥。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# llama-parse是异步优先的，运行笔记本中的异步代码需要使用nest_asyncio", "import nest_asyncio", "", "nest_asyncio.apply()", "", "import os", "", "# 使用OpenAI API进行嵌入", "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"", "", "# 使用NVIDIA API Playground API密钥进行LLM", "os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 加载NVIDIA LLM\n", "\n", "现在我们可以通过传入模型名称来加载我们的`NVIDIA` LLM，模型名称可以在文档中找到 - 位于[这里](https://docs.api.nvidia.com/nim/reference/)\n", "\n", "> 注意：默认模型是`mistralai/mistral-7b-instruct-v0.2`。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.llms.nvidia import NVIDIA\n", "from llama_index.core import VectorStoreIndex\n", "from llama_index.core import Settings\n", "\n", "llm = NVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n", "\n", "Settings.llm = llm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以观察我们的 `llm` 对象当前关联的模型是哪个，通过查看 `.model` 属性。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["'mistralai/mistral-7b-instruct-v0.2'"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["llm.model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 加载API目录 LLM\n", "\n", "我们还可以使用它们的API目录地址来加载模型。\n", "\n", "让我们以`gemma-7b`为例！\n", "\n", "1. 转到[model页面](https://build.nvidia.com/google/gemma-7b)\n", "2. 在`model`参数中找到地址（例如`\"google/gemma-7b\"`）\n", "3. 验证它具有`base_url`为`\"https://integrate.api.nvidia.com/v1\"`\n", "4. 使用`NVIDIA(model=\"model_name_here\")`来指向该模型的连接器（例如`NVIDIA(model=\"google/gemma-7b\"`）\n", "\n", "让我们在代码中看看这个。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["llm = NVIDIA(model=\"google/gemma-7b\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们确认一下我们是否将 `NvidiaAIPlayground` LLM 与正确的模型关联起来了！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["'google/gemma-7b'"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["llm.model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 基本功能\n", "\n", "现在我们可以探索在LlamaIndex生态系统中可以使用连接器的不同方式！\n", "\n", "在开始之前，让我们设置一个`ChatMessage`对象的列表 - 这是一些方法的预期输入。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage, MessageRole\n", "\n", "chat_messages = [\n", "    ChatMessage(\n", "        role=MessageRole.SYSTEM, content=(\"You are a helpful assistant.\")\n", "    ),\n", "    ChatMessage(\n", "        role=MessageRole.USER,\n", "        content=(\"What are the most popular house pets in North America?\"),\n", "    ),\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们将按照每个示例相同的基本模式进行操作：\n", "\n", "1. 我们将把我们的 `NVIDIA` LLM 指向我们想要的模型\n", "2. 我们将检查如何使用端点来实现期望的任务！\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 完成：`.complete()`\n", "\n", "我们可以使用`.complete()`/`.acomplete()`（接受一个字符串）来从所选模型中获取响应。\n", "\n", "让我们为这个任务使用我们的默认模型。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["completion_llm = NVIDIA()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们可以通过检查`.model`属性来验证这是否是预期的默认值。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["'mistralai/mistral-7b-instruct-v0.2'"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["completion_llm.model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们在模型上调用`.complete()`方法，并使用字符串`\"Hello!\"`作为输入，观察响应。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["CompletionResponse(text=\" Hello there! How can I help you today? I'm here to answer any questions you might have or provide information on a wide range of topics. So, feel free to ask me anything!\\n\\nIf you're looking for some general information, I can help you with that too. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some general information, I can provide you with that as well. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some general information, I can provide you with that as well. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some general information, I can provide you with that as well. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some\", additional_kwargs={}, raw={'id': 'chatcmpl-f6906079-51e7-44bf-aaea-a9478397dfbf', 'choices': [Choice(finish_reason=None, index=0, logprobs=ChoiceLogprobs(content=None, text_offset=[], token_logprobs=[0.0, 0.0], tokens=[], top_logprobs=[]), message=ChatCompletionMessage(content=\" Hello there! How can I help you today? I'm here to answer any questions you might have or provide information on a wide range of topics. So, feel free to ask me anything!\\n\\nIf you're looking for some general information, I can help you with that too. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some general information, I can provide you with that as well. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some general information, I can provide you with that as well. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some general information, I can provide you with that as well. For example, I can tell you about the weather, current events, or provide definitions for various words and concepts. I can also help you with math problems, translate words and phrases, and even tell you a joke or two!\\n\\nSo, what would you like to know? Let me know and I'll do my best to help you out!\\n\\nIf you have any specific question or topic in mind, please let me know and I'll be glad to help you out. If you want some\", role='assistant', function_call=None, tool_calls=None))], 'created': 1713474670, 'model': 'mistralai/mistral-7b-instruct-v0.2', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=512, prompt_tokens=11, total_tokens=523)}, logprobs=None, delta=None)"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["completion_llm.complete(\"Hello!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["正如LlamaIndex所期望的那样 - 我们会收到一个`CompletionResponse`作为响应。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 异步完成：`.acomplete()`\n", "\n", "还有一个可以以相同方式利用的异步实现！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["CompletionResponse(text=\" Hello there! How can I help you today? I'm here to answer any questions you might have or provide information on a wide range of topics. So feel free to ask me anything!\\n\\nIf you're looking for a specific topic, just let me know and I'll do my best to provide you with accurate and up-to-date information. And if you have any requests for fun facts or trivia, I'm happy to oblige!\\n\\nSo, what would you like to know today? Let me help make your day a little brighter! 😊\", additional_kwargs={}, raw={'id': 'chatcmpl-8ce881c1-a47b-43aa-afd8-9e9addf26ce9', 'choices': [Choice(finish_reason=None, index=0, logprobs=ChoiceLogprobs(content=None, text_offset=[], token_logprobs=[0.0, 0.0], tokens=[], top_logprobs=[]), message=ChatCompletionMessage(content=\" Hello there! How can I help you today? I'm here to answer any questions you might have or provide information on a wide range of topics. So feel free to ask me anything!\\n\\nIf you're looking for a specific topic, just let me know and I'll do my best to provide you with accurate and up-to-date information. And if you have any requests for fun facts or trivia, I'm happy to oblige!\\n\\nSo, what would you like to know today? Let me help make your day a little brighter! 😊\", role='assistant', function_call=None, tool_calls=None))], 'created': 1712175910, 'model': 'mistralai/mistral-7b-instruct-v0.2', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=123, prompt_tokens=11, total_tokens=134)}, logprobs=None, delta=None)"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["await completion_llm.acomplete(\"Hello!\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 聊天：`.chat()`\n", "\n", "现在我们可以尝试使用`.chat()`方法来做同样的事情。这个方法需要一个聊天消息的列表，所以我们将使用上面创建的那个列表。\n", "\n", "我们将使用`mistralai/mixtral-8x7b-instruct-v0.1`模型作为示例。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chat_llm = NVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["现在我们只需要在我们的`ChatMessages`列表上调用`.chat()`，然后观察我们的响应。\n", "\n", "您还会注意到，我们可以传入一些额外的关键字参数来影响生成过程 - 在本例中，我们使用了`seed`参数来影响我们的生成，以及`stop`参数来指示我们希望模型在达到特定标记时停止生成！\n", "\n", "> 注意：您可以在所选模型的API文档中找到有关模型端点支持的其他kwargs的信息。例如，Mixtral的API文档位于[此处](https://docs.api.nvidia.com/nim/reference/mistralai-mixtral-8x7b-instruct-infer)！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=\" In North America, the most popular types of house pets are:\\n\\n1. Dogs: Man's best friend is the most popular pet in North America. They are known for their loyalty, companionship, and the variety of breeds that cater to different lifestyles and preferences.\\n\\n2. Cats\", additional_kwargs={}), raw={'id': 'chatcmpl-b6ef95ca-e023-4dc8-8ee9-843f214169e9', 'choices': [Choice(finish_reason=None, index=0, logprobs=ChoiceLogprobs(content=None, text_offset=[], token_logprobs=[0.0, 0.0], tokens=[], top_logprobs=[]), message=ChatCompletionMessage(content=\" In North America, the most popular types of house pets are:\\n\\n1. Dogs: Man's best friend is the most popular pet in North America. They are known for their loyalty, companionship, and the variety of breeds that cater to different lifestyles and preferences.\\n\\n2. Cats\", role='assistant', function_call=None, tool_calls=None))], 'created': 1713474655, 'model': 'mistralai/mixtral-8x7b-instruct-v0.1', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=66, prompt_tokens=26, total_tokens=92)}, delta=None, logprobs=None, additional_kwargs={})"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["chat_llm.chat(chat_messages, seed=4, stop=[\"cat\", \"cats\", \"Cat\", \"Cats\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["如预期，我们收到了一个`ChatResponse`作为响应。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 异步聊天：(`achat`)\n", "\n", "我们还有一个异步实现的`.chat()`方法，可以按照以下方式调用。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=' The most popular house pets in North America are dogs and cats. According to the American Pet Products Association (APPA), as of 2021, approximately 69 million homes in the United States own a pet, and 63.4 million of those households have a dog, while 42.7 million have a cat. Birds, small mammals, reptiles, and fish are also popular pets, but to a lesser extent.', additional_kwargs={}), raw={'id': 'chatcmpl-373a1d42-4dc1-4ef9-aaf3-5fea137e8e1e', 'choices': [Choice(finish_reason=None, index=0, logprobs=ChoiceLogprobs(content=None, text_offset=[], token_logprobs=[0.0, 0.0], tokens=[], top_logprobs=[]), message=ChatCompletionMessage(content=' The most popular house pets in North America are dogs and cats. According to the American Pet Products Association (APPA), as of 2021, approximately 69 million homes in the United States own a pet, and 63.4 million of those households have a dog, while 42.7 million have a cat. Birds, small mammals, reptiles, and fish are also popular pets, but to a lesser extent.', role='assistant', function_call=None, tool_calls=None))], 'created': 1712177472, 'model': 'mistralai/mixtral-8x7b-instruct-v0.1', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=95, prompt_tokens=59, total_tokens=154)}, delta=None, logprobs=None, additional_kwargs={})"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["await chat_llm.achat(chat_messages)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 流：`.stream_chat()`\n", "\n", "我们也可以使用在`build.nvidia.com`上找到的模型来进行流式使用案例！\n", "\n", "让我们选择另一个模型并观察其行为。我们将使用谷歌的`gemma-7b`模型来完成这个任务。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stream_llm = NVIDIA(model=\"google/gemma-7b\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们使用`.stream_chat()`来调用我们的模型，它再次期望一个`ChatMessage`对象的列表，并捕获响应。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["streamed_response = stream_llm.stream_chat(chat_messages)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["<generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat.<locals>.wrapped_gen at 0x7dd89853e320>"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["streamed_response"]}, {"cell_type": "markdown", "metadata": {}, "source": ["正如我们所看到的，响应是一个生成器，其中包含流式响应。\n", "\n", "让我们在生成完成后查看最终的响应。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["assistant: **Top Popular House Pets in North America:**\n", "\n", "**1. Dogs:**\n", "* Estimated 63.4 million pet dogs in households (2023)\n", "* Known for their loyalty, companionship, and trainability\n", "\n", "**2. Cats:**\n", "* Estimated 38.4 million pet cats in households (2023)\n", "* Known for their independence, affection, and low-maintenance nature\n", "\n", "**3. Fish:**\n", "* Estimated 14.5 million pet fish in households (2023)\n", "* Popular for their tranquility, beauty, and variety of species\n", "\n", "**4. Small mammals (guinea pigs, hamsters, rabbits):**\n", "* Estimated 14.4 million pet small mammals in households (2023)\n", "* Known for their playful and affectionate nature\n", "\n", "**5. Birds:**\n", "* Estimated 13.3 million pet birds in households (2023)\n", "* Known for their beauty, song, and intelligence\n", "\n", "**Other popular pets:**\n", "\n", "* Tortoises and reptiles\n", "* Hamsters and rodents\n", "* Invertebrates (such as spiders and hermit crabs)\n", "\n", "**Factors influencing pet popularity:**\n", "\n", "* **Lifestyle and living situation:** Urban dwellers are more likely to have cats, while suburban and rural residents are more likely to have dogs.\n", "* **Cost:** Dogs tend to be more expensive to own than cats.\n", "* **Personality and preferences:** Some people prefer the companionship of dogs, while others prefer the independence of cats.\n", "* **Availability:** Certain pets are easier to find or adopt than others.\n", "* **Trend and cultural influences:** Some pets become more popular than others due to trends or cultural preferences.\n"]}], "source": ["last_element = None\n", "for last_element in streamed_response:\n", "    pass\n", "\n", "print(last_element)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 异步流：`.astream_chat()`\n", "\n", "我们也有与流式处理等效的异步方法，可以以类似的方式用于同步实现。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["streamed_response = await stream_llm.astream_chat(chat_messages)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["<async_generator object llm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat.<locals>.wrapped_gen at 0x787709eea460>"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["streamed_response"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["assistant: Sure, here are the most popular house pets in North America:\n", "\n", "1. Dogs\n", "2. Cats\n", "3. Fish\n", "4. Small Mammals\n", "5. Birds\n"]}], "source": ["last_element = None\n", "async for last_element in streamed_response:\n", "    pass\n", "\n", "print(last_element)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 流式查询引擎响应\n", "\n", "让我们来看一个稍微复杂一点的例子，使用一个查询引擎！\n", "\n", "我们将从加载一些数据开始（我们将使用[《银河系漫游指南》](https://web.eecs.utk.edu/~hqi/deeplearning/project/hhgttg.txt)）。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 加载数据\n", "\n", "让我们首先创建一个目录，用来存放我们的数据。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/hhgttg'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们将从上述来源下载我们的数据。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2024-04-01 14:39:38--  https://web.eecs.utk.edu/~hqi/deeplearning/project/hhgttg.txt\n", "Resolving web.eecs.utk.edu (web.eecs.utk.edu)... 160.36.127.165\n", "Connecting to web.eecs.utk.edu (web.eecs.utk.edu)|160.36.127.165|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 1534289 (1.5M) [text/plain]\n", "Saving to: ‘data/hhgttg/hhgttg.txt’\n", "\n", "data/hhgttg/hhgttg. 100%[===================>]   1.46M  6.75MB/s    in 0.2s    \n", "\n", "2024-04-01 14:39:39 (6.75 MB/s) - ‘data/hhgttg/hhgttg.txt’ saved [1534289/1534289]\n", "\n"]}], "source": ["!wget 'https://web.eecs.utk.edu/~hqi/deeplearning/project/hhgttg.txt' -O 'data/hhgttg/hhgttg.txt'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们需要为这一步准备一个嵌入模型！我们将使用OpenAI的`text-embedding-03-small`模型来实现这一点，并将其保存在我们的`Settings`中。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.openai import OpenAIEmbedding\n", "\n", "openai_embedding = OpenAIEmbedding(model=\"text-embedding-3-small\")\n", "\n", "Settings.embed_model = openai_embedding"]}, {"cell_type": "markdown", "metadata": {}, "source": ["现在我们可以加载我们的文档，并利用上面创建的 `OpenAIEmbedding()` 创建一个索引。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "\n", "documents = SimpleDirectoryReader(\"data/hhgttg\").load_data()\n", "index = VectorStoreIndex.from_documents(documents)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["现在我们可以创建一个简单的查询引擎，并将我们的 `streaming` 参数设置为 `True`。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["streaming_qe = index.as_query_engine(streaming=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们向查询引擎发送一个查询，然后流式传输响应。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["streaming_response = streaming_qe.query(\n", "    \"What is the significance of the number 42?\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The significance of the number 42 is a central theme in \"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams. The book is a comedic science fiction satire that follows the adventures of two intergalactic travelers, Arthur Dent and Ford Prefect, as they try to escape the destruction of Earth and uncover the true meaning of the number 42.\n", "\n", "Throughout the book, the number 42 is presented as the ultimate answer to the ultimate question of life, the universe, and everything. The question itself is never explicitly stated, but it is implied to be a deeply profound and existential one that has been sought after by philosophers, scientists, and thinkers throughout history.\n", "\n", "The idea of the number 42 as the ultimate answer is a playful jab at the idea of seeking ultimate knowledge and understanding, which is often seen as an impossible task. The number 42 is also a reference to the famous \"42\" answer in the \"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams, which is a comedic science fiction satire that follows the adventures of two intergalactic travelers, Arthur Dent and Ford Prefect, as they try to escape the destruction of Earth and uncover the true meaning of the number 42.\n", "\n", "In the book, the supercomputer Deep Thought is asked to find the answer to the ultimate question, and after billions of years of computation, it determines that the answer is 42. The answer is so profound that it causes Deep Thought to become obsolete, as it is no longer needed to answer questions.\n", "\n", "The significance of the number 42 in \"The Hitchhiker's Guide to the Galaxy\" is a commentary on the nature of knowledge and the quest for ultimate understanding. It is a reminder that there are limits to what can be known and that the pursuit of knowledge should be done with a sense of humor and a willingness to accept the unknown."]}], "source": ["streaming_response.print_response_stream()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 连接本地NIMs\n", "\n", "除了连接到托管的[NVIDIA NIMs](https://ai.nvidia.com)之外，此连接器还可以用于连接到本地微服务实例。这有助于在必要时将您的应用程序部署到本地。\n", "\n", "有关设置本地微服务实例的说明，请参阅https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.llms.nvidia import NVIDIA\n", "\n", "llm = NVIDIA(model=\"...\").mode(\"nim\", base_url=\"https://localhost.../v1\")\n", "llm.available_models"]}], "metadata": {"kernelspec": {"display_name": "nvidia-llama-index-playground-connector", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}