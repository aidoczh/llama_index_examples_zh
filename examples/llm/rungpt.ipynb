{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/llm/rungpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# RunGPT\n", "RunGPTæ˜¯ä¸€ä¸ªå¼€æºçš„äº‘åŸç”Ÿå¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰æœåŠ¡æ¡†æ¶ã€‚å®ƒæ—¨åœ¨ç®€åŒ–å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆ†å¸ƒå¼GPUé›†ç¾¤ä¸Šçš„éƒ¨ç½²å’Œç®¡ç†ã€‚RunGPTçš„ç›®æ ‡æ˜¯å°†å…¶æ‰“é€ æˆä¸€ä¸ªé›†ä¸­ä¸”æ˜“äºè®¿é—®çš„åœ°æ–¹ï¼Œæ±‡é›†ä¼˜åŒ–å¤§è§„æ¨¡å¤šæ¨¡æ€æ¨¡å‹çš„æŠ€æœ¯ï¼Œå¹¶ä½¿å…¶æ˜“äºä¸ºæ‰€æœ‰äººä½¿ç”¨çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆã€‚åœ¨RunGPTä¸­ï¼Œæˆ‘ä»¬å·²ç»æ”¯æŒäº†è®¸å¤šLLMsï¼Œå¦‚LLaMAã€Pythiaã€StableLMã€Vicunaã€MOSSï¼Œä»¥åŠåƒMiniGPT-4å’ŒOpenFlamingoè¿™æ ·çš„å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```python\n", "# å¯¼å…¥æ‰€éœ€çš„åº“\n", "import numpy as np\n", "import pandas as pd\n", "```\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-rungpt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "markdown", "metadata": {}, "source": ["æ‚¨éœ€è¦åœ¨Pythonç¯å¢ƒä¸­ä½¿ç”¨`pip install`å®‰è£…rungptåŒ…ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install rungpt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å®‰è£…æˆåŠŸåï¼ŒRunGPTæ”¯æŒçš„æ¨¡å‹å¯ä»¥é€šè¿‡ä¸€è¡Œå‘½ä»¤è¿›è¡Œéƒ¨ç½²ã€‚è¿™ä¸ªé€‰é¡¹ä¼šä»å¼€æºå¹³å°ä¸‹è½½ç›®æ ‡è¯­è¨€æ¨¡å‹ï¼Œå¹¶å°†å…¶éƒ¨ç½²ä¸ºä¸€ä¸ªæœåŠ¡åœ¨æœ¬åœ°ç«¯å£ï¼Œå¯ä»¥é€šè¿‡httpæˆ–grpcè¯·æ±‚è¿›è¡Œè®¿é—®ã€‚æˆ‘å‡è®¾ä½ ä¸ä¼šåœ¨jupyterç¬”è®°æœ¬ä¸­è¿è¡Œè¿™ä¸ªå‘½ä»¤ï¼Œè€Œæ˜¯åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!rungpt serve decapoda-research/llama-7b-hf --precision fp16 --device_map balanced"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## åŸºæœ¬ç”¨æ³•\n", "#### ä½¿ç”¨æç¤ºè°ƒç”¨`complete`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.llms.rungpt import RunGptLLM\n", "\n", "llm = RunGptLLM()\n", "promot = \"What public transportation might be available in a city?\"\n", "response = llm.complete(promot)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["I don't want to go to work, so what should I do?\n", "I have a job interview on Monday. What can I wear that will make me look professional but not too stuffy or boring?\n"]}], "source": ["print(response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨è°ƒç”¨`chat`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage, MessageRole\n", "from llama_index.llms.rungpt import RunGptLLM\n", "\n", "messages = [\n", "    ChatMessage(\n", "        role=MessageRole.USER,\n", "        content=\"Now, I want you to do some math for me.\",\n", "    ),\n", "    ChatMessage(\n", "        role=MessageRole.ASSISTANT, content=\"Sure, I would like to help you.\"\n", "    ),\n", "    ChatMessage(\n", "        role=MessageRole.USER,\n", "        content=\"How many points determine a straight line?\",\n", "    ),\n", "]\n", "llm = RunGptLLM()\n", "response = llm.chat(messages=messages, temperature=0.8, max_tokens=15)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["æµå¼å¤„ç†æ˜¯ä¸€ç§å¤„ç†æ•°æ®çš„æ–¹æ³•ï¼Œå®ƒå…è®¸æˆ‘ä»¬åœ¨æ•°æ®åˆ°è¾¾æ—¶ç«‹å³å¤„ç†å®ƒï¼Œè€Œä¸éœ€è¦ç­‰å¾…æ‰€æœ‰æ•°æ®éƒ½å¯ç”¨åå†è¿›è¡Œå¤„ç†ã€‚è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚ç”¨äºå¤„ç†å¤§é‡æ•°æ®æˆ–å®æ—¶æ•°æ®ã€‚åœ¨Pythonä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å„ç§åº“å’Œå·¥å…·æ¥å®ç°æµå¼å¤„ç†ï¼Œå¦‚`pandas`ã€`Dask`å’Œ`Spark`ç­‰ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ä½¿ç”¨ `stream_complete` ç»ˆç«¯ç‚¹\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["promot = \"What public transportation might be available in a city?\"\n", "response = RunGptLLM().stream_complete(promot)\n", "for item in response:\n", "    print(item.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ä½¿ç”¨ `stream_chat` ç«¯ç‚¹\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.llms.rungpt import RunGptLLM\n", "\n", "messages = [\n", "    ChatMessage(\n", "        role=MessageRole.USER,\n", "        content=\"Now, I want you to do some math for me.\",\n", "    ),\n", "    ChatMessage(\n", "        role=MessageRole.ASSISTANT, content=\"Sure, I would like to help you.\"\n", "    ),\n", "    ChatMessage(\n", "        role=MessageRole.USER,\n", "        content=\"How many points determine a straight line?\",\n", "    ),\n", "]\n", "response = RunGptLLM().stream_chat(messages=messages)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for item in response:\n", "    print(item.message)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}