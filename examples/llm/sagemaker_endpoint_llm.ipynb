{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/drive/104BZb4U1KLzOYArnCzhqN-CqJWZDeqhz?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 通过LlamaIndex与部署在Amazon SageMaker端点中的LLM进行交互\n", "\n", "Amazon SageMaker端点是一种完全托管的资源，可以部署机器学习模型，特别是LLM（大型语言模型），用于对新数据进行预测。\n", "\n", "本笔记演示了如何使用`SageMakerLLM`与LLM端点进行交互，解锁额外的LlamaIndex功能。\n", "因此，假设在SageMaker端点上部署了一个LLM。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 设置\n", "\n", "如果您在colab上打开此笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-sagemaker-endpoint"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["! pip install llama-index"]}, {"cell_type": "markdown", "metadata": {}, "source": ["您需要指定要交互的端点名称。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ENDPOINT_NAME = \"<-YOUR-ENDPOINT-NAME->\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["连接到终端点需要提供凭据。您可以选择：\n", "- 通过指定 `profile_name` 参数来使用 AWS 配置文件，如果未指定，则将使用默认凭据配置文件。\n", "- 将凭据作为参数传递（`aws_access_key_id`、`aws_secret_access_key`、`aws_session_token`、`region_name`）。\n", "\n", "有关更多详细信息，请查看[此链接](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**AWS配置文件名称**\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.llms.sagemaker_endpoint import SageMakerLLM\n", "\n", "AWS_ACCESS_KEY_ID = \"<-YOUR-AWS-ACCESS-KEY-ID->\"\n", "AWS_SECRET_ACCESS_KEY = \"<-YOUR-AWS-SECRET-ACCESS-KEY->\"\n", "AWS_SESSION_TOKEN = \"<-YOUR-AWS-SESSION-TOKEN->\"\n", "REGION_NAME = \"<-YOUR-ENDPOINT-REGION-NAME->\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["llm = SageMakerLLM(\n", "    endpoint_name=ENDPOINT_NAME,\n", "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n", "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n", "    aws_session_token=AWS_SESSION_TOKEN,\n", "    aws_region_name=REGION_NAME,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**使用凭据：**\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["来自llama_index.llms.sagemaker_endpoint的SageMakerLLM", "", "ENDPOINT_NAME = \"<-YOUR-ENDPOINT-NAME->\"", "PROFILE_NAME = \"<-YOUR-PROFILE-NAME->\"", "llm = SageMakerLLM(", "    endpoint_name=ENDPOINT_NAME, profile_name=PROFILE_NAME", ")  # 省略配置文件名称以使用默认配置文件"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 基本用法\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 使用提示调用`complete`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["66 years old (birthdate: September 4, 1951). He is a British-American computer scientist, programmer, and entrepreneur who is known for his work in the fields of artificial intelligence, machine learning, and computer vision. He is a professor emeritus at Stanford University and a researcher at the Stanford Artificial Intelligence Lab (SAIL).\n", "\n", "Graham has made significant contributions to the field of computer science, including the development of the concept of \"n-grams,\" which are sequences of n items that occur together in a dataset. He has also worked on the development of machine learning algorithms and has written extensively on the topic of machine learning.\n", "\n", "Graham has received numerous awards for his work, including the Association for Computing Machinery (ACM) A.M. Turing Award, the IEEE Neural Networks Pioneer Award, and the IJCAI Award\n"]}], "source": ["", "# 翻译结果", "", "resp = llm.complete(", "    \"Paul Graham is \", formatted=True", ")  # 设置formatted=True以避免添加系统提示", "print(resp)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 使用消息列表调用 `chat`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage\n", "\n", "messages = [\n", "    ChatMessage(\n", "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n", "    ),\n", "    ChatMessage(role=\"user\", content=\"What is your name\"),\n", "]\n", "resp = llm.chat(messages)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["assistant:   Arrrr, shiver me timbers! *adjusts eye patch* Me name be Cap'n Blackbeak, the most feared and infamous pirate on the seven seas! *winks*\n", "\n", "*ahem* But enough about me, matey. What be bringin' ye to these fair waters? Are ye here to plunder some booty, or just to share a pint o' grog with a salty old sea dog like meself? *chuckles*\n"]}], "source": ["print(resp)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 流式处理\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 使用 `stream_complete` 终端点\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["resp = llm.stream_complete(\"Paul Graham is \", formatted=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["64 today. He’s a computer sci\n", "ist, entrepreneur, and writer, best known for his work in the fields of artificial intelligence, machine learning, and computer graphics.\n", "Graham was born in 1956 in Boston, Massachusetts. He earned his Bachelor’s degree in Computer Science from Harvard University in 1978 and his PhD in Computer Science from the University of California, Berkeley in 1982.\n", "Graham’s early work focused on the development of the first computer graphics systems that could generate photorealistic images. In the 1980s, he became interested in the field of artificial intelligence and machine learning, and he co-founded a number of companies to explore these areas, including Viaweb, which was one of the first commercial web hosting services.\n", "Graham is also a prolific writer and has published a number of influential essays on topics such as the nature\n"]}], "source": ["for r in resp:\n", "    print(r.delta)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 使用 `stream_chat` 端点\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage\n", "\n", "messages = [\n", "    ChatMessage(\n", "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n", "    ),\n", "    ChatMessage(role=\"user\", content=\"What is your name\"),\n", "]\n", "resp = llm.stream_chat(messages)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["  ARRGH! *adjusts eye patch* Me hearty? *winks* Me name be Captain Blackbeak, the most feared and infamous pirate to ever sail the seven seas! *chuckles* Or, at least, that's what me matey mates tell me. *winks*\n", "\n", "So, what be bringin' ye to these waters, matey? Are ye here to plunder some booty or just to hear me tales of the high seas? *grins* Either way, I be ready to share me treasure with ye! *winks* Just don't be tellin' any landlubbers about me hidden caches o' gold, or ye might be walkin' the plank, savvy? *winks*"]}], "source": ["for r in resp:\n", "    print(r.delta, end=\"\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 配置模型\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["`SageMakerLLM`是与部署在Amazon SageMaker中的不同语言模型（LLM）进行交互的抽象。所有默认参数都与Llama 2模型兼容。因此，如果您使用不同的模型，您可能需要设置以下参数：\n", "\n", "- `messages_to_prompt`：接受一个`ChatMessage`对象列表和（如果消息中未指定）系统提示的可调用对象。它应返回一个包含端点LLM兼容格式中的消息的字符串。\n", "\n", "- `completion_to_prompt`：接受一个带有系统提示的完成字符串，并返回一个端点LLM兼容格式的字符串。\n", "\n", "- `content_handler`：一个从`llama_index.llms.sagemaker_llm_endpoint_utils.BaseIOHandler`继承并实现以下方法的类：`serialize_input`、`deserialize_output`、`deserialize_streaming_output`和`remove_prefix`。\n"]}], "metadata": {"kernelspec": {"display_name": "media_sensor", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}