{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# ç»Ÿä¸€\n", "\n", "[ç»Ÿä¸€](https://unify.ai/hub)åŠ¨æ€åœ°å°†æ¯ä¸ªæŸ¥è¯¢è·¯ç”±åˆ°æœ€ä½³çš„LLMï¼Œæ”¯æŒOpenAIã€MistralAIã€Perplexity AIå’ŒTogether AIç­‰æä¾›å•†ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨å•ä¸ªAPIå¯†é’¥è®¿é—®æ‰€æœ‰æä¾›å•†ã€‚\n", "\n", "æ‚¨å¯ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„[å®æ—¶åŸºå‡†æµ‹è¯•](https://unify.ai/hub/mixtral-8x7b-instruct-v0.1)ä»¥äº†è§£æ•°æ®çš„æ¥æºï¼\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## å®‰è£…è¯´æ˜\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®‰è£…LlamaIndex ğŸ¦™å’ŒUnifyé›†æˆã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-unify llama-index"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ç¯å¢ƒè®¾ç½®\n", "\n", "ç¡®ä¿è®¾ç½®`UNIFY_API_KEY`ç¯å¢ƒå˜é‡ã€‚æ‚¨å¯ä»¥åœ¨[Unifyæ§åˆ¶å°](https://console.unify.ai/login)ä¸­è·å–å¯†é’¥ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"UNIFY_API_KEY\"] = \"<YOUR API KEY>\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ä½¿ç”¨LlamaIndexä¸Unify\n", "\n", "LlamaIndexæ˜¯ä¸€ä¸ªç”¨äºç´¢å¼•å’Œæœç´¢æ–‡æœ¬æ•°æ®çš„å·¥å…·ï¼Œè€ŒUnifyæ˜¯ä¸€ä¸ªç”¨äºæ•°æ®æ•´åˆå’Œæ¸…æ´—çš„å·¥å…·ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨LlamaIndexå’ŒUnifyæ¥å¤„ç†æ–‡æœ¬æ•°æ®ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### è·¯ç”±è¯·æ±‚\n", "\n", "æˆ‘ä»¬å¯ä»¥åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯åˆå§‹åŒ–å¹¶æŸ¥è¯¢ä¸€ä¸ªèŠå¤©æ¨¡å‹ã€‚è¦é…ç½®Unifyçš„è·¯ç”±å™¨ï¼Œå¯ä»¥å°†ä¸€ä¸ªç«¯ç‚¹å­—ç¬¦ä¸²ä¼ é€’ç»™ `Unify`ã€‚æ‚¨å¯ä»¥åœ¨[Unifyæ–‡æ¡£](https://unify.ai/docs/hub/concepts/runtime_routing.html)ä¸­äº†è§£æ›´å¤šä¿¡æ¯ã€‚\n", "\n", "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åœ¨è¾“å…¥æˆæœ¬æ–¹é¢æœ€ä¾¿å®œçš„ `llama2-70b` ç«¯ç‚¹ï¼Œç„¶åä½¿ç”¨ `complete`ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["CompletionResponse(text=\"  I'm doing well, thanks for asking! It's always a pleasure to chat with you. I hope you're having a great day too! Is there anything specific you'd like to talk about or ask me? I'm here to help with any questions you might have.\", additional_kwargs={}, raw={'id': 'meta-llama/Llama-2-70b-chat-hf-b90de288-1927-4f32-9ecb-368983c45321', 'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"  I'm doing well, thanks for asking! It's always a pleasure to chat with you. I hope you're having a great day too! Is there anything specific you'd like to talk about or ask me? I'm here to help with any questions you might have.\", role='assistant', function_call=None, tool_calls=None, tool_call_id=None))], 'created': 1711047739, 'model': 'llama-2-70b-chat@anyscale', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=62, prompt_tokens=16, total_tokens=78, cost=7.8e-05)}, logprobs=None, delta=None)"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["from llama_index.llms.unify import Unify\n", "\n", "llm = Unify(model=\"llama-2-70b-chat@dinput-cost\")\n", "llm.complete(\"How are you today, llama?\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### å•ç‚¹ç™»å½•\n", "\n", "å¦‚æœæ‚¨ä¸å¸Œæœ›è·¯ç”±å™¨é€‰æ‹©æä¾›è€…ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„SSOåœ¨ä¸ä¸æ‰€æœ‰æä¾›è€…å»ºç«‹å¸æˆ·çš„æƒ…å†µä¸‹æŸ¥è¯¢ä¸åŒæä¾›è€…çš„ç«¯ç‚¹ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹æ‰€æœ‰ç«¯ç‚¹éƒ½æ˜¯æœ‰æ•ˆçš„ï¼š\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["llm = Unify(model=\"llama-2-70b-chat@together-ai\")\n", "llm = Unify(model=\"gpt-3.5-turbo@openai\")\n", "llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@mistral-ai\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["è¿™æ ·å¯ä»¥è®©æ‚¨å¿«é€Ÿåˆ‡æ¢å’Œæµ‹è¯•ä¸åŒçš„æ¨¡å‹å’Œæä¾›å•†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æ­£åœ¨å¼€å‘ä¸€ä¸ªä½¿ç”¨gpt-4ä½œä¸ºæ ¸å¿ƒçš„åº”ç”¨ç¨‹åºï¼Œæ‚¨å¯ä»¥åœ¨å¼€å‘å’Œ/æˆ–æµ‹è¯•è¿‡ç¨‹ä¸­ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½æ¥æŸ¥è¯¢æˆæœ¬æ›´ä½çš„LLMï¼Œä»¥é™ä½æˆæœ¬ã€‚\n", "\n", "åœ¨[è¿™é‡Œ](https://unify.ai/hub)æŸ¥çœ‹å¯ç”¨çš„å†…å®¹ï¼\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### æµå¼ä¼ è¾“å’Œä¼˜åŒ–å»¶è¿Ÿ\n", "\n", "å¦‚æœæ‚¨æ­£åœ¨æ„å»ºä¸€ä¸ªå¯¹å“åº”é€Ÿåº¦è¦æ±‚å¾ˆé«˜çš„åº”ç”¨ç¨‹åºï¼Œæ‚¨å¾ˆå¯èƒ½å¸Œæœ›è·å¾—ä¸€ä¸ªæµå¼å“åº”ã€‚æ­¤å¤–ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨å…·æœ‰æœ€ä½â€œé¦–ä¸ªä»¤ç‰Œæ—¶é—´â€çš„æä¾›è€…ï¼Œä»¥å‡å°‘ç”¨æˆ·ç­‰å¾…å“åº”çš„æ—¶é—´ã€‚åœ¨Unifyä¸­ï¼Œè¿™çœ‹èµ·æ¥åº”è¯¥æ˜¯è¿™æ ·çš„ï¼š\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@ttft\")\n", "\n", "response = llm.stream_complete(\n", "    \"Translate the following to German: \"\n", "    \"Hey, there's an emergency in translation street, \"\n", "    \"please send help asap!\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model and provider are : mixtral-8x7b-instruct-v0.1@mistral-ai\n", "\n", "Hallo, es gibt einen Notfall in der ÃœbersetzungsstraÃŸe, bitte senden Sie Hilfe so schnell wie mÃ¶glich!\n", "\n", "(Note: This is a literal translation and the term \"ÃœbersetzungsstraÃŸe\" is not a standard or commonly used term in German. A more natural way to express the idea of a \"emergency in translation\" could be \"Notfall bei Ãœbersetzungen\" or \"akute Ãœbersetzungsnotwendigkeit\".)"]}], "source": ["show_provider = True\n", "for r in response:\n", "    if show_provider:\n", "        print(f\"Model and provider are : {r.raw['model']}\\n\")\n", "        show_provider = False\n", "    print(r.delta, end=\"\", flush=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### å¼‚æ­¥è°ƒç”¨å’Œæœ€ä½è¾“å…¥æˆæœ¬\n", "\n", "æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„æ˜¯ï¼Œæ‚¨è¿˜å¯ä»¥å¼‚æ­¥è¿è¡Œè¯·æ±‚ã€‚å¯¹äºé•¿æ–‡æ¡£æ‘˜è¦ç­‰ä»»åŠ¡ï¼Œä¼˜åŒ–è¾“å…¥æˆæœ¬è‡³å…³é‡è¦ã€‚Unifyçš„åŠ¨æ€è·¯ç”±å™¨ä¹Ÿå¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model and provider are : mixtral-8x7b-instruct-v0.1@deepinfra\n", "\n", " OpenAI: Pioneering 'safe' artificial general intelligence.\n"]}], "source": ["llm = Unify(model=\"mixtral-8x7b-instruct-v0.1@input-cost\")\n", "\n", "response = await llm.acomplete(\n", "    \"Summarize this in 10 words or less. OpenAI is a U.S. based artificial intelligence \"\n", "    \"(AI) research organization founded in December 2015, researching artificial intelligence \"\n", "    \"with the goal of developing 'safe and beneficial' artificial general intelligence, \"\n", "    \"which it defines as 'highly autonomous systems that outperform humans at most economically \"\n", "    \"valuable work'. As one of the leading organizations of the AI spring, it has developed \"\n", "    \"several large language models, advanced image generation models, and previously, released \"\n", "    \"open-source models. Its release of ChatGPT has been credited with starting the AI spring\"\n", ")\n", "\n", "print(f\"Model and provider are : {response.raw['model']}\\n\")\n", "print(response)"]}], "metadata": {"kernelspec": {"display_name": "base", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}