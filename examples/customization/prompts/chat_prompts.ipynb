{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/customization/prompts/chat_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨Colabä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# è‡ªå®šä¹‰èŠå¤©æç¤º\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨Colabä¸Šæ‰“å¼€æ­¤ç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## è®¾ç½®æç¤º\n", "\n", "ä¸‹é¢ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é»˜è®¤æç¤ºå¹¶å¯¹å…¶è¿›è¡Œè‡ªå®šä¹‰ï¼Œä»¥ä¾¿å³ä½¿ä¸Šä¸‹æ–‡ä¸å¤Ÿæœ‰ç”¨ä¹Ÿèƒ½å§‹ç»ˆç»™å‡ºç­”æ¡ˆã€‚\n", "\n", "æˆ‘ä»¬å±•ç¤ºäº†ä¸¤ç§è®¾ç½®æç¤ºçš„æ–¹æ³•ï¼š\n", "1. æ˜ç¡®å®šä¹‰ChatMessageå’ŒMessageRoleå¯¹è±¡ã€‚\n", "2. è°ƒç”¨ChatPromptTemplate.from_messages\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["qa_prompt_str = (\n", "    \"Context information is below.\\n\"\n", "    \"---------------------\\n\"\n", "    \"{context_str}\\n\"\n", "    \"---------------------\\n\"\n", "    \"Given the context information and not prior knowledge, \"\n", "    \"answer the question: {query_str}\\n\"\n", ")\n", "\n", "refine_prompt_str = (\n", "    \"We have the opportunity to refine the original answer \"\n", "    \"(only if needed) with some more context below.\\n\"\n", "    \"------------\\n\"\n", "    \"{context_msg}\\n\"\n", "    \"------------\\n\"\n", "    \"Given the new context, refine the original answer to better \"\n", "    \"answer the question: {query_str}. \"\n", "    \"If the context isn't useful, output the original answer again.\\n\"\n", "    \"Original Answer: {existing_answer}\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1. æ˜ç¡®å®šä¹‰`ChatMessage`å’Œ`MessageRole`å¯¹è±¡\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage, MessageRole\n", "from llama_index.core import ChatPromptTemplate\n", "\n", "# æ–‡æœ¬é—®ç­”æç¤º\n", "chat_text_qa_msgs = [\n", "    ChatMessage(\n", "        role=MessageRole.SYSTEM,\n", "        content=(\n", "            \"å§‹ç»ˆå›ç­”é—®é¢˜ï¼Œå³ä½¿ä¸Šä¸‹æ–‡ä¸æ˜¯å¾ˆæœ‰å¸®åŠ©ã€‚\"\n", "        ),\n", "    ),\n", "    ChatMessage(role=MessageRole.USER, content=qa_prompt_str),\n", "]\n", "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n", "\n", "# ä¼˜åŒ–æç¤º\n", "chat_refine_msgs = [\n", "    ChatMessage(\n", "        role=MessageRole.SYSTEM,\n", "        content=(\n", "            \"å§‹ç»ˆå›ç­”é—®é¢˜ï¼Œå³ä½¿ä¸Šä¸‹æ–‡ä¸æ˜¯å¾ˆæœ‰å¸®åŠ©ã€‚\"\n", "        ),\n", "    ),\n", "    ChatMessage(role=MessageRole.USER, content=refine_prompt_str),\n", "]\n", "refine_template = ChatPromptTemplate(chat_refine_msgs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2. è°ƒç”¨`ChatPromptTemplate.from_messages`\n", "\n", "`from_messages`æ˜¯ä¸€ç§è¯­æ³•ç³–ï¼Œå…è®¸æ‚¨å°†èŠå¤©æç¤ºæ¨¡æ¿å®šä¹‰ä¸ºä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç»„å¯¹åº”ä¸€ä¸ªèŠå¤©æ¶ˆæ¯ï¼ˆ\"role\"ï¼Œ\"message\"ï¼‰ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import ChatPromptTemplate\n", "\n", "# æ–‡æœ¬é—®ç­”æç¤º\n", "chat_text_qa_msgs = [\n", "    (\n", "        \"system\",\n", "        \"å§‹ç»ˆå›ç­”é—®é¢˜ï¼Œå³ä½¿ä¸Šä¸‹æ–‡ä¸æ˜¯å¾ˆæœ‰å¸®åŠ©ã€‚\",\n", "    ),\n", "    (\"user\", qa_prompt_str),\n", "]\n", "text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n", "\n", "# ä¼˜åŒ–æç¤º\n", "chat_refine_msgs = [\n", "    (\n", "        \"system\",\n", "        \"å§‹ç»ˆå›ç­”é—®é¢˜ï¼Œå³ä½¿ä¸Šä¸‹æ–‡ä¸æ˜¯å¾ˆæœ‰å¸®åŠ©ã€‚\",\n", "    ),\n", "    (\"user\", refine_prompt_str),\n", "]\n", "refine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ä½¿ç”¨æç¤º\n", "\n", "ç°åœ¨ï¼Œæˆ‘ä»¬å°†åœ¨ç´¢å¼•æŸ¥è¯¢ä¸­ä½¿ç”¨è¿™äº›æç¤ºï¼\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import openai\n", "import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### ä¸‹è½½æ•°æ®\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndexå’ŒSimpleDirectoryReader\n", "from llama_index.llms.openai import OpenAI\n", "\n", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n", "\n", "# ä½¿ç”¨èŠå¤©æ¨¡å‹åˆ›å»ºç´¢å¼•ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨èŠå¤©æç¤ºï¼\n", "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n", "\n", "index = VectorStoreIndex.from_documents(documents)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["åœ¨æ·»åŠ æ¨¡æ¿ä¹‹å‰\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["I'm unable to provide an answer to that question based on the context information provided.\n"]}], "source": ["print(index.as_query_engine(llm=llm).query(\"Who is Joe Biden?\"))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### æ·»åŠ æ¨¡æ¿å\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Joe Biden is the current President of the United States, having taken office in January 2021. He previously served as Vice President under President Barack Obama from 2009 to 2017.\n"]}], "source": ["print(\n", "    index.as_query_engine(\n", "        text_qa_template=text_qa_template,\n", "        refine_template=refine_template,\n", "        llm=llm,\n", "    ).query(\"Who is Joe Biden?\")\n", ")"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v3", "language": "python", "name": "llama_index_v3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}