{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/customization/prompts/chat_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 自定义聊天提示\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["如果您在Colab上打开此笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 设置提示\n", "\n", "下面，我们将使用默认提示并对其进行自定义，以便即使上下文不够有用也能始终给出答案。\n", "\n", "我们展示了两种设置提示的方法：\n", "1. 明确定义ChatMessage和MessageRole对象。\n", "2. 调用ChatPromptTemplate.from_messages\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["qa_prompt_str = (\n", "    \"Context information is below.\\n\"\n", "    \"---------------------\\n\"\n", "    \"{context_str}\\n\"\n", "    \"---------------------\\n\"\n", "    \"Given the context information and not prior knowledge, \"\n", "    \"answer the question: {query_str}\\n\"\n", ")\n", "\n", "refine_prompt_str = (\n", "    \"We have the opportunity to refine the original answer \"\n", "    \"(only if needed) with some more context below.\\n\"\n", "    \"------------\\n\"\n", "    \"{context_msg}\\n\"\n", "    \"------------\\n\"\n", "    \"Given the new context, refine the original answer to better \"\n", "    \"answer the question: {query_str}. \"\n", "    \"If the context isn't useful, output the original answer again.\\n\"\n", "    \"Original Answer: {existing_answer}\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 1. 明确定义`ChatMessage`和`MessageRole`对象\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.llms import ChatMessage, MessageRole\n", "from llama_index.core import ChatPromptTemplate\n", "\n", "# 文本问答提示\n", "chat_text_qa_msgs = [\n", "    ChatMessage(\n", "        role=MessageRole.SYSTEM,\n", "        content=(\n", "            \"始终回答问题，即使上下文不是很有帮助。\"\n", "        ),\n", "    ),\n", "    ChatMessage(role=MessageRole.USER, content=qa_prompt_str),\n", "]\n", "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n", "\n", "# 优化提示\n", "chat_refine_msgs = [\n", "    ChatMessage(\n", "        role=MessageRole.SYSTEM,\n", "        content=(\n", "            \"始终回答问题，即使上下文不是很有帮助。\"\n", "        ),\n", "    ),\n", "    ChatMessage(role=MessageRole.USER, content=refine_prompt_str),\n", "]\n", "refine_template = ChatPromptTemplate(chat_refine_msgs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2. 调用`ChatPromptTemplate.from_messages`\n", "\n", "`from_messages`是一种语法糖，允许您将聊天提示模板定义为一个元组列表，其中每个元组对应一个聊天消息（\"role\"，\"message\"）。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import ChatPromptTemplate\n", "\n", "# 文本问答提示\n", "chat_text_qa_msgs = [\n", "    (\n", "        \"system\",\n", "        \"始终回答问题，即使上下文不是很有帮助。\",\n", "    ),\n", "    (\"user\", qa_prompt_str),\n", "]\n", "text_qa_template = ChatPromptTemplate.from_messages(chat_text_qa_msgs)\n", "\n", "# 优化提示\n", "chat_refine_msgs = [\n", "    (\n", "        \"system\",\n", "        \"始终回答问题，即使上下文不是很有帮助。\",\n", "    ),\n", "    (\"user\", refine_prompt_str),\n", "]\n", "refine_template = ChatPromptTemplate.from_messages(chat_refine_msgs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 使用提示\n", "\n", "现在，我们将在索引查询中使用这些提示！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import openai\n", "import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex和SimpleDirectoryReader\n", "from llama_index.llms.openai import OpenAI\n", "\n", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n", "\n", "# 使用聊天模型创建索引，这样我们就可以使用聊天提示！\n", "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n", "\n", "index = VectorStoreIndex.from_documents(documents)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["在添加模板之前\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["I'm unable to provide an answer to that question based on the context information provided.\n"]}], "source": ["print(index.as_query_engine(llm=llm).query(\"Who is Joe Biden?\"))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 添加模板后\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Joe Biden is the current President of the United States, having taken office in January 2021. He previously served as Vice President under President Barack Obama from 2009 to 2017.\n"]}], "source": ["print(\n", "    index.as_query_engine(\n", "        text_qa_template=text_qa_template,\n", "        refine_template=refine_template,\n", "        llm=llm,\n", "    ).query(\"Who is Joe Biden?\")\n", ")"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v3", "language": "python", "name": "llama_index_v3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}