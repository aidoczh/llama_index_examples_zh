{"cells": [{"cell_type": "markdown", "id": "6b39b02d", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/customization/llms/SimpleIndexDemo-Huggingface_stablelm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05", "metadata": {}, "source": ["# HuggingFace LLM - StableLM\n"]}, {"cell_type": "markdown", "id": "08efa764", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "id": "105ae323", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-huggingface"]}, {"cell_type": "code", "execution_count": null, "id": "5275e801", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n", "NumExpr defaulting to 8 threads.\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/gpt_index/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n"]}], "source": ["import logging\n", "import sys\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "from llama_index.llms.huggingface import HuggingFaceLLM\n", "from llama_index.core import Settings"]}, {"cell_type": "markdown", "id": "17480fd9", "metadata": {}, "source": ["#### ä¸‹è½½æ•°æ®\n"]}, {"cell_type": "code", "execution_count": null, "id": "ace579c0", "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119", "metadata": {}, "source": ["#### åŠ è½½æ–‡æ¡£ï¼Œæ„å»ºVectorStoreIndex\n"]}, {"cell_type": "code", "execution_count": null, "id": "90a895ac-2e8b-4d55-97bd-ad614dceda40", "metadata": {}, "outputs": [], "source": ["# åŠ è½½æ–‡æ¡£\n", "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"]}, {"cell_type": "code", "execution_count": null, "id": "b1726fff", "metadata": {}, "outputs": [], "source": ["# è®¾ç½®æç¤º - StableLMç‰¹æœ‰\n", "from llama_index.core import PromptTemplate\n", "\n", "system_prompt = \"\"\"<|SYSTEM|># StableLMè°ƒæ•´ç‰ˆï¼ˆAlphaç‰ˆæœ¬ï¼‰\n", "- StableLMæ˜¯ç”±StabilityAIå¼€å‘çš„æœ‰ç›Šä¸”æ— å®³çš„å¼€æºAIè¯­è¨€æ¨¡å‹ã€‚\n", "- StableLMå¾ˆé«˜å…´èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·ï¼Œä½†ä¼šæ‹’ç»æ‰§è¡Œä»»ä½•å¯èƒ½å¯¹ç”¨æˆ·æœ‰å®³çš„æ“ä½œã€‚\n", "- StableLMä¸ä»…ä»…æ˜¯ä¸€ä¸ªä¿¡æ¯æ¥æºï¼Œè¿˜èƒ½å†™è¯—ã€çŸ­ç¯‡æ•…äº‹å’Œå¼€ç©ç¬‘ã€‚\n", "- StableLMå°†æ‹’ç»å‚ä¸ä»»ä½•å¯èƒ½ä¼¤å®³äººç±»çš„äº‹æƒ…ã€‚\n", "\"\"\"\n", "\n", "# è¿™å°†åŒ…è£…llama-indexå†…éƒ¨çš„é»˜è®¤æç¤º\n", "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"]}, {"cell_type": "code", "execution_count": null, "id": "6a40a357-8a8f-405d-b355-e0caf23bee3c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:24<00:00, 12.21s/it]\n"]}], "source": ["import torch\n", "\n", "llm = HuggingFaceLLM(\n", "    context_window=4096,\n", "    max_new_tokens=256,\n", "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n", "    system_prompt=system_prompt,\n", "    query_wrapper_prompt=query_wrapper_prompt,\n", "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n", "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n", "    device_map=\"auto\",\n", "    stopping_ids=[50278, 50279, 50277, 1, 0],\n", "    tokenizer_kwargs={\"max_length\": 4096},\n", "    # å¦‚æœä½¿ç”¨CUDAæ¥å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œè¯·å–æ¶ˆæ³¨é‡Šä»¥ä¸‹å†…å®¹\n", "    # model_kwargs={\"torch_dtype\": torch.float16}\n", ")\n", "\n", "Settings.llm = llm\n", "Settings.chunk_size = 1024"]}, {"cell_type": "code", "execution_count": null, "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n", "> [build_index_from_nodes] Total embedding token usage: 20729 tokens\n"]}], "source": ["index = VectorStoreIndex.from_documents(documents)"]}, {"attachments": {}, "cell_type": "markdown", "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4", "metadata": {}, "source": ["#### æŸ¥è¯¢ç´¢å¼•\n"]}, {"cell_type": "code", "execution_count": null, "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n", "> [retrieve] Total embedding token usage: 8 tokens\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2126 tokens\n", "> [get_response] Total LLM token usage: 2126 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["# å°†æ—¥å¿—çº§åˆ«è®¾ç½®ä¸ºDEBUGï¼Œä»¥è·å¾—æ›´è¯¦ç»†çš„è¾“å‡º\n", "query_engine = index.as_query_engine()\n", "response = query_engine.query(\"ä½œè€…åœ¨æˆé•¿è¿‡ç¨‹ä¸­åšäº†ä»€ä¹ˆï¼Ÿ\")"]}, {"cell_type": "code", "execution_count": null, "id": "c42733dd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author is a computer scientist who has written several books on programming languages and software development. He worked on the IBM 1401 and wrote a program to calculate pi. He also wrote a program to predict how high a rocket ship would fly. The program was written in Fortran and used a TRS-80 microcomputer. The author is a PhD student and has been working on multiple projects, including a novel and a PBS documentary. He is envious of the author's work and feels that he has made significant contributions to the field of computer science. He is working on multiple projects and is envious of the author's work. He is also interested in learning Italian and is considering taking the entrance exam in Florence. The author is not aware of how he managed to pass the written exam and is not sure how he will manage to do so.\n"]}], "source": ["print(response)"]}, {"attachments": {}, "cell_type": "markdown", "id": "6b282580", "metadata": {}, "source": ["#### æŸ¥è¯¢ç´¢å¼• - æµå¼å¤„ç†\n"]}, {"cell_type": "code", "execution_count": null, "id": "313544bc", "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(streaming=True)"]}, {"cell_type": "code", "execution_count": null, "id": "c1c5ab85-25e4-4460-8b6a-3c119d92ba48", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n", "> [retrieve] Total embedding token usage: 8 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["> [get_response] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["# å°†æ—¥å¿—çº§åˆ«è®¾ç½®ä¸ºDEBUGï¼Œä»¥è·å¾—æ›´è¯¦ç»†çš„è¾“å‡º\n", "response_stream = query_engine.query(\"ä½œè€…åœ¨æˆé•¿è¿‡ç¨‹ä¸­åšäº†ä»€ä¹ˆï¼Ÿ\")"]}, {"cell_type": "code", "execution_count": null, "id": "f8d94603-8e91-451f-8f60-feb0ea0c8c06", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author is a computer scientist who has written several books on programming languages and software development. He worked on the IBM 1401 and wrote a program to calculate pi. He also wrote a program to predict how high a rocket ship would fly. The program was written in Fortran and used a TRS-80 microcomputer. The author is a PhD student and has been working on multiple projects, including a novel and a PBS documentary. He is envious of the author's work and feels that he has made significant contributions to the field of computer science. He is working on multiple projects and is envious of the author's work. He is also interested in learning Italian and is considering taking the entrance exam in Florence. The author is not aware of how he managed to pass the written exam and is not sure how he will manage to do so.<|USER|>"]}], "source": ["# å¼€å§‹æµå¼ä¼ è¾“å¯èƒ½è¾ƒæ…¢ï¼Œå› ä¸ºç¾Šé©¼ç´¢å¼•é€šå¸¸æ¶‰åŠè®¸å¤šç¾Šé©¼è°ƒç”¨\n", "response_stream.print_response_stream()"]}, {"cell_type": "code", "execution_count": null, "id": "b29749ee", "metadata": {}, "outputs": [], "source": ["# å¯ä»¥è·å¾—ä¸€ä¸ªæ™®é€šçš„å“åº”å¯¹è±¡\n", "response = response_stream.get_response()\n", "print(response)"]}, {"cell_type": "code", "execution_count": null, "id": "d14c15b3", "metadata": {}, "outputs": [], "source": ["# ä¹Ÿå¯ä»¥è‡ªå·±éå†ç”Ÿæˆå™¨\n", "generated_text = \"\"\n", "for text in response.response_gen:\n", "    generated_text += text"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}