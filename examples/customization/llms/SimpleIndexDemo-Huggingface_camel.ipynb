{"cells": [{"cell_type": "markdown", "id": "5f753ded", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/customization/llms/SimpleIndexDemo-Huggingface_camel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05", "metadata": {}, "source": ["# HuggingFace LLM - Camel-5b\n", "\n", "这是一个基于Camel-5b模型的HuggingFace LLM（语言模型）示例。\n"]}, {"cell_type": "markdown", "id": "0bc195b8", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "03c49377", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-huggingface"]}, {"cell_type": "code", "execution_count": null, "id": "5296c135", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:numexpr.utils:Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n", "NumExpr defaulting to 8 threads.\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/gpt_index/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n"]}], "source": ["import logging\n", "import sys\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "from llama_index.llms.huggingface import HuggingFaceLLM\n", "from llama_index.core import Settings"]}, {"cell_type": "markdown", "id": "36ca8fc2", "metadata": {}, "source": ["#### 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "edaab3b0", "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119", "metadata": {}, "source": ["#### 加载文档，构建VectorStoreIndex\n"]}, {"cell_type": "code", "execution_count": null, "id": "90a895ac-2e8b-4d55-97bd-ad614dceda40", "metadata": {}, "outputs": [], "source": ["# 加载文档\n", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"]}, {"cell_type": "code", "execution_count": null, "id": "b1726fff", "metadata": {}, "outputs": [], "source": ["# 设置提示 - 特定于 StableLM\n", "from llama_index.core import PromptTemplate\n", "\n", "# 这将包装llama-index内部的默认提示\n", "# 取自https://huggingface.co/Writer/camel-5b-hf\n", "query_wrapper_prompt = PromptTemplate(\n", "    \"以下是描述任务的指示。\"\n", "    \"编写一个适当完成请求的响应。\\n\\n\"\n", "    \"### 指示:\\n{query_str}\\n\\n### 响应:\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "6a40a357-8a8f-405d-b355-e0caf23bee3c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:43<00:00, 14.34s/it]\n"]}], "source": ["import torch\n", "\n", "llm = HuggingFaceLLM(\n", "    context_window=2048,\n", "    max_new_tokens=256,\n", "    generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\n", "    query_wrapper_prompt=query_wrapper_prompt,\n", "    tokenizer_name=\"Writer/camel-5b-hf\",\n", "    model_name=\"Writer/camel-5b-hf\",\n", "    device_map=\"auto\",\n", "    tokenizer_kwargs={\"max_length\": 2048},\n", "    # 如果使用CUDA来减少内存使用量，请取消注释下面的内容\n", "    # model_kwargs={\"torch_dtype\": torch.float16}\n", ")\n", "\n", "Settings.chunk_size = 512\n", "Settings.llm = llm"]}, {"cell_type": "code", "execution_count": null, "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 27212 tokens\n", "> [build_index_from_nodes] Total embedding token usage: 27212 tokens\n"]}], "source": ["index = VectorStoreIndex.from_documents(documents)"]}, {"attachments": {}, "cell_type": "markdown", "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4", "metadata": {}, "source": ["```python\n", "# Query Index\n", "\n", "This notebook demonstrates how to query the index of a DataFrame in pandas.\n", "\n", "## Querying the Index\n", "\n", "To query the index of a DataFrame, you can use the `index` attribute of the DataFrame.\n", "\n", "```python\n", "import pandas as pd\n", "\n", "# Create a sample DataFrame\n", "data = {'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n", "        'Age': [25, 30, 35, 40, 45]}\n", "df = pd.DataFrame(data)\n", "\n", "# Query the index\n", "index = df.index\n", "print(index)\n", "```\n", "\n", "The output will be the index of the DataFrame:\n", "\n", "```\n", "RangeIndex(start=0, stop=5, step=1)\n", "```\n", "\n", "In this example, the index is a `RangeIndex` starting from 0 and ending at 5 (exclusive) with a step of 1.\n", "\n", "You can also use the `iloc` attribute to query the index by position:\n", "\n", "```python\n", "# Query the index using iloc\n", "index_value = df.iloc[2].name\n", "print(index_value)\n", "```\n", "\n", "The output will be the index value at the specified position:\n", "\n", "```\n", "2\n", "```\n", "\n", "This demonstrates how to query the index of a DataFrame in pandas.\n", "```\n"]}, {"cell_type": "code", "execution_count": null, "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n", "> [retrieve] Total embedding token usage: 8 tokens\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n", "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1026 tokens\n", "> [get_response] Total LLM token usage: 1026 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["# 将日志级别设置为DEBUG，以获得更详细的输出\n", "query_engine = index.as_query_engine()\n", "response = query_engine.query(\"作者在成长过程中做了什么？\")"]}, {"cell_type": "code", "execution_count": null, "id": "c42733dd", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author grew up in a small town in England, attended a prestigious private school, and then went to Cambridge University, where he studied computer science. Afterward, he worked on web infrastructure, wrote essays, and then realized he could write about startups. He then started giving talks, wrote a book, and started interviewing founders for a book on startups.\n"]}], "source": ["print(response)"]}, {"attachments": {}, "cell_type": "markdown", "id": "6b282580", "metadata": {}, "source": ["#### 查询索引 - 流式处理\n"]}, {"cell_type": "code", "execution_count": null, "id": "313544bc", "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(streaming=True)"]}, {"cell_type": "code", "execution_count": null, "id": "c1c5ab85-25e4-4460-8b6a-3c119d92ba48", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n", "> [retrieve] Total embedding token usage: 8 tokens\n"]}, {"name": "stderr", "output_type": "stream", "text": ["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}, {"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n", "> [get_response] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["# 将日志级别设置为DEBUG，以获得更详细的输出\n", "response_stream = query_engine.query(\"作者在成长过程中做了什么？\")"]}, {"cell_type": "code", "execution_count": null, "id": "f8d94603-8e91-451f-8f60-feb0ea0c8c06", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author grew up in a small town in England, attended a prestigious private school, and then went to Cambridge University, where he studied computer science. Afterward, he worked on web infrastructure, wrote essays, and then realized he could write about startups. He then started giving talks, wrote a book, and started interviewing founders for a book on startups.<|endoftext|>"]}], "source": ["# 可能在开始流式传输时速度较慢，因为羊驼索引通常涉及许多 LLM 调用\n", "response_stream.print_response_stream()"]}, {"cell_type": "code", "execution_count": null, "id": "b29749ee", "metadata": {}, "outputs": [], "source": ["# 也可以获得一个普通的响应对象\n", "response = response_stream.get_response()\n", "print(response)"]}, {"cell_type": "code", "execution_count": null, "id": "d14c15b3", "metadata": {}, "outputs": [], "source": ["# 也可以自己迭代生成器\n", "generated_text = \"\"\n", "for text in response.response_gen:\n", "    generated_text += text"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}