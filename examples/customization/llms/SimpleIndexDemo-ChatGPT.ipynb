{"cells": [{"cell_type": "markdown", "id": "7ae43f8b", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/customization/llms/SimpleIndexDemo-ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "9c48213d-6e6a-4c10-838a-2a7c710c3a05", "metadata": {}, "source": ["# ChatGPT\n"]}, {"cell_type": "markdown", "id": "cf6108b0", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "590874bc", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "14790912", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "690a6918-7c75-4f95-9ccc-d2c4a1fe00d7", "metadata": {}, "outputs": [], "source": ["import logging\n", "import sys\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "from llama_index.core import Settings\n", "from llama_index.llms.openai import OpenAI\n", "from IPython.display import Markdown, display"]}, {"cell_type": "markdown", "id": "9bc1c58d", "metadata": {}, "source": ["#### 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "18c8d851", "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "id": "50d3b817-b70e-4667-be4f-d3a0fe4bd119", "metadata": {}, "source": ["#### 加载文档，构建VectorStoreIndex\n"]}, {"cell_type": "code", "execution_count": null, "id": "03d1691e-544b-454f-825b-5ee12f7faa8a", "metadata": {}, "outputs": [], "source": ["# 加载文档\n", "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"]}, {"cell_type": "code", "execution_count": null, "id": "6cc980e8-f4e1-4fad-93f8-ab1bbaa874f3", "metadata": {}, "outputs": [], "source": ["# 设置全局设置配置\n", "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n", "Settings.llm = llm\n", "Settings.chunk_size = 512"]}, {"cell_type": "code", "execution_count": null, "id": "ad144ee7-96da-4dd6-be00-fd6cf0c78e58", "metadata": {}, "outputs": [], "source": ["index = VectorStoreIndex.from_documents(documents)"]}, {"attachments": {}, "cell_type": "markdown", "id": "b6caf93b-6345-4c65-a346-a95b0f1746c4", "metadata": {}, "source": ["## 查询索引\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "83e2905e-3789-4793-82b9-0ac488246824", "metadata": {}, "source": ["默认情况下，借助langchain的PromptSelector抽象，如果使用ChatGPT模型，则会使用经过修改的细化提示，以适应ChatGPT的使用。\n"]}, {"cell_type": "code", "execution_count": null, "id": "85466fdf-93f3-4cb1-a5f9-0056a8245a6f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 8 tokens\n", "> [retrieve] Total embedding token usage: 8 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n", "> [get_response] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["query_engine = index.as_query_engine(\n", "    similarity_top_k=3,\n", "    streaming=True,\n", ")\n", "response = query_engine.query(\n", "    \"What did the author do growing up?\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "b0262da8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Before college, the author worked on writing short stories and programming on an IBM 1401 using an early version of Fortran. They also worked on programming with microcomputers and eventually created a new dialect of Lisp called Arc. They later realized the potential of publishing essays on the web and began writing and publishing them. The author also worked on spam filters, painting, and cooking for groups."]}], "source": ["response.print_response_stream()"]}, {"cell_type": "code", "execution_count": null, "id": "ec88df57", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 12 tokens\n", "> [retrieve] Total embedding token usage: 12 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 0 tokens\n", "> [get_response] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["query_engine = index.as_query_engine(\n", "    similarity_top_k=5,\n", "    streaming=True,\n", ")\n", "response = query_engine.query(\n", "    \"What did the author do during his time at RISD?\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "67e8e675-1b03-423a-b53e-23ab278ba03b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author attended RISD and took classes in fundamental subjects like drawing, color, and design. They also learned a lot in the color class they took, but otherwise, they were basically teaching themselves to paint. The author dropped out of RISD in 1993."]}], "source": ["response.print_response_stream()"]}, {"attachments": {}, "cell_type": "markdown", "id": "88ca1808-d112-4c28-b110-b65dcc9b7207", "metadata": {}, "source": ["**细化提示**：这是聊天细化提示\n"]}, {"cell_type": "code", "execution_count": null, "id": "2f0c270d-9de5-40bf-88fc-83a360523db0", "metadata": {}, "outputs": [], "source": ["from llama_index.core.prompts.chat_prompts import CHAT_REFINE_PROMPT"]}, {"cell_type": "code", "execution_count": null, "id": "4db38651-9790-4a61-ac3d-689ce6dfa369", "metadata": {}, "outputs": [], "source": ["dict(CHAT_REFINE_PROMPT.prompt)"]}, {"attachments": {}, "cell_type": "markdown", "id": "6cb664e8-f53f-4d6c-a086-1f2784cc1dc8", "metadata": {}, "source": ["#### 查询索引（使用标准的改进提示）\n", "\n", "如果我们使用“标准”的改进提示（其中提示是一个文本模板而不是多个消息），我们发现在ChatGPT上的结果更糟糕。\n"]}, {"cell_type": "code", "execution_count": null, "id": "29c416f8-d5ab-47d6-8b16-f615bfa58219", "metadata": {}, "outputs": [], "source": ["from llama_index.core.prompts.default_prompts import DEFAULT_REFINE_PROMPT"]}, {"cell_type": "code", "execution_count": null, "id": "3df1acc4-735a-48ac-9fb4-73d9d7eabc02", "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(\n", "    refine_template=DEFAULT_REFINE_PROMPT,\n", "    similarity_top_k=5,\n", "    streaming=True,\n", ")\n", "response = query_engine.query(\n", "    \"What did the author do during his time at RISD?\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "b8938077-6527-4008-8d0c-af7a8178ff10", "metadata": {}, "outputs": [], "source": ["response.print_response_stream()"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}