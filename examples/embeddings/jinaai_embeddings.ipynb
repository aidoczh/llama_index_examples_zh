{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/jinaai_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Jina Embeddings\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-jinaai\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "markdown", "metadata": {}, "source": ["您可能还需要其他包，这些包不是直接与llama-index一起提供的。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install Pillow"]}, {"cell_type": "markdown", "metadata": {}, "source": ["对于这个示例，你需要一个API密钥，你可以从https://jina.ai/embeddings/ 获取。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 使用您的API密钥进行初始化\n", "import os\n", "\n", "jinaai_api_key = \"YOUR_JINAAI_API_KEY\"\n", "os.environ[\"JINAAI_API_KEY\"] = jinaai_api_key"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 通过JinaAI API使用Jina嵌入模型嵌入文本和查询\n", "\n", "Jina是一个用于构建搜索解决方案的开源框架，它提供了一种简单而强大的方式来嵌入文本和查询。通过JinaAI API，我们可以轻松地使用Jina的嵌入模型来将文本和查询转换为向量表示形式，从而支持各种搜索和相似度匹配任务。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["您可以使用JinaEmbedding类对文本和查询进行编码。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.jinaai import JinaEmbedding\n", "\n", "embed_model = JinaEmbedding(\n", "    api_key=jinaai_api_key,\n", "    model=\"jina-embeddings-v2-base-en\",\n", ")\n", "\n", "embeddings = embed_model.get_text_embedding(\"This is the text to embed\")\n", "\n", "print(len(embeddings))\n", "print(embeddings[:5])\n", "\n", "embeddings = embed_model.get_query_embedding(\"This is the query to embed\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 分批嵌入\n", "\n", "在处理大型数据集时，有时候我们需要将数据分批处理以避免内存溢出或提高计算效率。这种情况下，我们可以使用分批嵌入的方法，将数据分成小批量进行嵌入处理。接下来我们将介绍如何在PyTorch中实现分批嵌入。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["您也可以批量嵌入文本，批量大小可以通过设置`embed_batch_size`参数来控制（如果未传递，默认值为10，且不应大于2048）。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embed_model = JinaEmbedding(\n", "    api_key=jinaai_api_key,\n", "    model=\"jina-embeddings-v2-base-en\",\n", "    embed_batch_size=16,\n", ")\n", "\n", "embeddings = embed_model.get_text_embedding_batch(\n", "    [\"This is the text to embed\", \"More text can be provided in a batch\"]\n", ")\n", "\n", "print(len(embeddings))\n", "print(embeddings[0][:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 让我们使用Jina AI Embeddings构建一个RAG管道\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["#### 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["```python\n", "# 导入所需的库\n", "import numpy as np\n", "import pandas as pd\n", "```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging\n", "import sys\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core.response.notebook_utils import display_source_node\n", "\n", "from IPython.display import Markdown, display"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["```python\n", "# 加载数据\n", "```\n", "\n", "将数据加载到notebook中。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["```python\n", "# 构建索引\n", "```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["your_openai_key = \"YOUR_OPENAI_KEY\"\n", "llm = OpenAI(api_key=your_openai_key)\n", "embed_model = JinaEmbedding(\n", "    api_key=jinaai_api_key,\n", "    model=\"jina-embeddings-v2-base-en\",\n", "    embed_batch_size=16,\n", ")\n", "\n", "index = VectorStoreIndex.from_documents(\n", "    documents=documents, embed_model=embed_model\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["```python\n", "class Retriever:\n", "    def __init__(self, url):\n", "        self.url = url\n", "\n", "    def fetch(self):\n", "        # Fetch the data from the specified URL\n", "        pass\n", "\n", "    def extract(self):\n", "        # Extract relevant information from the fetched data\n", "        pass\n", "```\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["search_query_retriever = index.as_retriever()\n", "\n", "search_query_retrieved_nodes = search_query_retriever.retrieve(\n", "    \"What happened after the thesis?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Node ID:** d8db9dfe-ab7a-4709-9863-877b68d2210d<br>**Similarity:** 0.7698250992788241<br>**Text:** There were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.\n", "\n", "I wanted not just to build things, but to build things that would last.\n", "\n", "In this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.\n", "\n", "And moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.\n", "\n", "I had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art — that it didn't just appear spontaneously — but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.\n", "\n", "That fall I started taking art classes at Harvard. Grad students could take classes in any department, and my advisor, Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything.\n", "\n", "So now I was in a PhD program in computer science, yet planning to be an...<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/markdown": ["**Node ID:** 848bb0f8-629c-4491-b539-85f3ff5b77a2<br>**Similarity:** 0.7679106002573213<br>**Text:** Our teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\n", "\n", "I wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\n", "\n", "Interleaf had done something pretty bold. Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction. Toward the end of the year I spent much of my time surreptitiously working on On Lisp, which I had by this time gotten a contract to publish.\n", "\n", "The good part was that I got paid huge amounts of money, especially by art student standards. In Florence, after paying my part of the rent, my budget for everything else had been $7 a day. Now I was getting paid more than 4 times that every hour, even when I was just sitting in a meeting. By living cheaply I not only managed to save enough to go back to RISD, but a...<br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["for n in search_query_retrieved_nodes:\n", "    display_source_node(n, source_length=2000)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}, "vscode": {"interpreter": {"hash": "64bcadabe4cd61f3d117ba0da9d14bf2f8e35582ff79e821f2e71056f2723d1e"}}}, "nbformat": 4, "nbformat_minor": 4}