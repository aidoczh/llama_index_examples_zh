{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/drive/176IfpC2akqWOhDpVSnAA_eLEbzt4a5Fw?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# PremAI嵌入\n", "\n", "[PremAI](https://premai.io/)是一个一体化平台，简化了由生成式人工智能驱动的健壮、生产就绪应用程序的创建过程。通过简化开发流程，PremAI使您能够集中精力提升用户体验，并推动应用程序的整体增长。您可以在[这里](https://docs.premai.io/quick-start)快速开始使用我们的平台。\n", "\n", "在本节中，我们将讨论如何使用`PremEmbeddings`和llama-index来访问不同的嵌入模型。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 安装和设置\n", "\n", "我们首先安装 `llama-index` 和 `premai-sdk`。您可以输入以下命令进行安装：\n", "\n", "```bash\n", "pip install premai llama-index\n", "```\n", "\n", "在继续之前，请确保您已在 PremAI 上创建了一个账户并已经创建了一个项目。如果没有，请参考 [快速入门](https://docs.premai.io/introduction) 指南开始使用 PremAI 平台。创建您的第一个项目并获取您的 API 密钥。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-premai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.premai import PremAIEmbeddings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 在LlamaIndex中设置PremAIEmbeddings实例\n", "\n", "一旦我们导入了所需的模块，让我们设置我们的客户端。现在假设我们的`project_id`是`8`。但请确保您使用您自己的项目ID，否则会抛出错误。\n", "\n", "为了在PremAI中使用llama-index，您不需要传递任何模型名称或设置任何参数给我们的chat-client。默认情况下，它将使用在[LaunchPad](https://docs.premai.io/get-started/launchpad)中使用的模型名称和参数。\n", "\n", "我们支持许多最先进的嵌入模型。您可以在[这里](https://docs.premai.io/get-started/supported-models)查看我们支持的LLM和嵌入模型的列表。现在让我们为这个示例选择`text-embedding-3-large`模型。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import getpass\n", "\n", "if os.environ.get(\"PREMAI_API_KEY\") is None:\n", "    os.environ[\"PREMAI_API_KEY\"] = getpass.getpass(\"PremAI API Key:\")\n", "\n", "prem_embedding = PremAIEmbeddings(\n", "    project_id=8, model_name=\"text-embedding-3-large\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 调用嵌入模型\n", "\n", "现在你已经准备就绪了。现在让我们开始使用我们的嵌入模型，首先是单个查询，然后是多个查询（也称为文档）。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query = \"Hello, this is a test query\"\n", "query_result = prem_embedding.get_text_embedding(query)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Dimension of embeddings: 3072\n"]}], "source": ["print(f\"Dimension of embeddings: {len(query_result)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["[-0.02129288576543331,\n", " 0.0008162345038726926,\n", " -0.004556538071483374,\n", " 0.02918623760342598,\n", " -0.02547479420900345]"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["query_result[:5]"]}], "metadata": {"kernelspec": {"display_name": "venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}