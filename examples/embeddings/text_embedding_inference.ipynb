{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/text_embedding_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 文本嵌入推断\n", "\n", "本笔记本演示了如何配置`TextEmbeddingInference`嵌入。\n", "\n", "第一步是部署嵌入服务器。有关详细说明，请参阅[Text Embeddings Inference官方存储库](https://github.com/huggingface/text-embeddings-inference)。\n", "\n", "部署完成后，下面的代码将连接并提交嵌入以进行推断。\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-text-embeddings-inference"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["", "# from llama_index.embeddings.text_embeddings_inference import TextEmbeddingsInference", "# 创建TextEmbeddingsInference对象", "embed_model = TextEmbeddingsInference(", "    model_name=\"BAAI/bge-large-en-v1.5\",  # 用于格式化推断文本的必需参数", "    timeout=60,  # 超时时间（秒）", "    embed_batch_size=10,  # 嵌入的批处理大小", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1024\n", "[0.010597229, 0.05895996, 0.022445679, -0.012046814, -0.03164673]\n"]}], "source": ["embeddings = embed_model.get_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1024\n", "[0.010597229, 0.05895996, 0.022445679, -0.012046814, -0.03164673]\n"]}], "source": ["embeddings = await embed_model.aget_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}], "metadata": {"kernelspec": {"display_name": "llama-index-4a-wkI5X-py3.11", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}