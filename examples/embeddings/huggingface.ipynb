{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ä½¿ç”¨HuggingFaceè¿›è¡Œæœ¬åœ°åµŒå…¥\n", "\n", "LlamaIndexæ”¯æŒHuggingFaceåµŒå…¥æ¨¡å‹ï¼ŒåŒ…æ‹¬BGEã€Instructorç­‰ç­‰ã€‚\n", "\n", "æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›äº†å·¥å…·æ¥ä½¿ç”¨HuggingFaceçš„[Optimumåº“](https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-optimumonnxruntime)æ¥åˆ›å»ºå’Œä½¿ç”¨ONNXæ¨¡å‹ã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## HuggingFaceEmbedding\n", "\n", "åŸºæœ¬çš„ `HuggingFaceEmbedding` ç±»æ˜¯å¯¹ä»»ä½• HuggingFace æ¨¡å‹è¿›è¡ŒåµŒå…¥çš„é€šç”¨åŒ…è£…å™¨ã€‚Hugging Face ä¸Šçš„æ‰€æœ‰[åµŒå…¥æ¨¡å‹](https://huggingface.co/models?library=sentence-transformers)éƒ½åº”è¯¥å¯ä»¥ä½¿ç”¨ã€‚æ‚¨å¯ä»¥å‚è€ƒ[åµŒå…¥æ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)è·å–æ›´å¤šæ¨èä¿¡æ¯ã€‚\n", "\n", "è¿™ä¸ªç±»ä¾èµ–äº sentence-transformers åŒ…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `pip install sentence-transformers` è¿›è¡Œå®‰è£…ã€‚\n", "\n", "æ³¨æ„ï¼šå¦‚æœæ‚¨ä¹‹å‰ä½¿ç”¨çš„æ˜¯ LangChain çš„ `HuggingFaceEmbeddings`ï¼Œé‚£ä¹ˆè¿™åº”è¯¥ä¼šç»™å‡ºç›¸åŒçš„ç»“æœã€‚\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-huggingface\n", "%pip install llama-index-embeddings-instructor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n", "  warnings.warn(\"Can't initialize NVML\")\n"]}], "source": ["", "æ¥è‡ªllama_index.embeddings.huggingfaceçš„HuggingFaceEmbedding", "", "# åŠ è½½BAAI/bge-small-en", "# embed_model = HuggingFaceEmbedding()", "", "# åŠ è½½BAAI/bge-small-en-v1.5", "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Hello World!\n", "384\n", "[-0.030880315229296684, -0.11021008342504501, 0.3917851448059082, -0.35962796211242676, 0.22797748446464539]\n"]}], "source": ["embeddings = embed_model.get_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## InstructorEmbedding\n", "\n", "æ•™å¸ˆåµŒå…¥æ˜¯ä¸€ç±»ç‰¹åˆ«è®­ç»ƒçš„åµŒå…¥ï¼Œæ ¹æ®æŒ‡ç¤ºæ¥å¢å¼ºå®ƒä»¬çš„åµŒå…¥ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒæŸ¥è¯¢ä½¿ç”¨ `query_instruction=\"è¡¨ç¤ºç”¨äºæ£€ç´¢æ”¯æŒæ–‡æ¡£çš„é—®é¢˜ï¼š\"`ï¼Œæ–‡æœ¬ä½¿ç”¨ `text_instruction=\"è¡¨ç¤ºç”¨äºæ£€ç´¢çš„æ–‡æ¡£ï¼š\"`ã€‚\n", "\n", "å®ƒä»¬ä¾èµ–äº `Instructor` å’Œ `SentenceTransformers`ï¼ˆç‰ˆæœ¬2.2.2ï¼‰pipåŒ…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ `pip install InstructorEmbedding` å’Œ `pip install -U sentence-transformers==2.2.2` è¿›è¡Œå®‰è£…ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n", "  from tqdm.autonotebook import trange\n"]}, {"name": "stdout", "output_type": "stream", "text": ["load INSTRUCTOR_Transformer\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n", "  warnings.warn(\"Can't initialize NVML\")\n"]}, {"name": "stdout", "output_type": "stream", "text": ["max_seq_length  512\n"]}], "source": ["from llama_index.embeddings.instructor import InstructorEmbedding\n", "\n", "embed_model = InstructorEmbedding(model_name=\"hkunlp/instructor-base\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["768\n", "[ 0.02155361 -0.06098218  0.01796207  0.05490903  0.01526906]\n"]}], "source": ["embeddings = embed_model.get_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## OptimumEmbedding\n", "\n", "Optimumæ˜¯HuggingFaceåº“ä¸­ç”¨äºå°†HuggingFaceæ¨¡å‹å¯¼å‡ºå¹¶åœ¨ONNXæ ¼å¼ä¸­è¿è¡Œçš„å·¥å…·ã€‚\n", "\n", "æ‚¨å¯ä»¥ä½¿ç”¨`pip install transformers optimum[exporters]`å®‰è£…ä¾èµ–é¡¹ã€‚\n", "\n", "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºONNXæ¨¡å‹ã€‚ONNXæ¨¡å‹æä¾›äº†æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶ä¸”å¯ä»¥è·¨å¹³å°ä½¿ç”¨ï¼ˆä¾‹å¦‚åœ¨TransformersJSä¸­ï¼‰ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n", "  warnings.warn(\"Can't initialize NVML\")\n", "Framework not specified. Using pt to export to ONNX.\n", "Using the export variant default. Available variants are:\n", "\t- default: The default ONNX variant.\n", "Using framework PyTorch: 2.0.1+cu117\n", "Overriding 1 configuration item(s)\n", "\t- use_cache -> False\n"]}, {"name": "stdout", "output_type": "stream", "text": ["============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n", "verbose: False, log level: Level.ERROR\n", "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n", "\n", "Saved optimum model to ./bge_onnx. Use it with `embed_model = OptimumEmbedding(folder_name='./bge_onnx')`.\n"]}], "source": ["from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n", "\n", "OptimumEmbedding.create_and_save_optimum_model(\n", "    \"BAAI/bge-small-en-v1.5\", \"./bge_onnx\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["384\n", "[-0.10364960134029388, -0.20998482406139374, -0.01883639395236969, -0.5241696834564209, 0.0335749015212059]\n"]}], "source": ["embeddings = embed_model.get_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## åŸºå‡†æµ‹è¯•\n", "\n", "è®©æˆ‘ä»¬å°è¯•æ¯”è¾ƒä½¿ç”¨ç»å…¸çš„å¤§å‹æ–‡æ¡£â€”â€”ã€ŠIPCCæ°”å€™æŠ¥å‘Šã€‹ç¬¬3ç« ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n", "To disable this warning, you can either:\n", "\t- Avoid using `tokenizers` before the fork if possible\n", "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n", "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n", "                                 Dload  Upload   Total   Spent    Left  Speed\n", "100 20.7M  100 20.7M    0     0  16.5M      0  0:00:01  0:00:01 --:--:-- 16.5M\n"]}], "source": ["!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "from llama_index.core import Settings\n", "\n", "documents = SimpleDirectoryReader(\n", "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n", ").load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨HuggingFaceåº“ä¸­çš„é¢„è®­ç»ƒæ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚è¿™äº›åµŒå…¥å¯ä»¥ç”¨äºæ–‡æœ¬åˆ†ç±»ã€ç›¸ä¼¼åº¦åŒ¹é…ç­‰ä»»åŠ¡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`transformers`åº“æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç”Ÿæˆæ–‡æœ¬åµŒå…¥ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os", "import openai", "", "# ä¹‹åéœ€è¦åˆæˆå›å¤", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.huggingface import HuggingFaceEmbedding", "", "# åŠ è½½BAAI/bge-small-en-v1.5", "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")", "test_emeds = embed_model.get_text_embedding(\"Hello World!\")", "", "Settings.embed_model = embed_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3bfb6c2115c447e5af19bb18b5a07ebe", "version_major": 2, "version_minor": 0}, "text/plain": ["Parsing documents into nodes:   0%|          | 0/172 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "55c188c3d18049df933cd87bd5a49ed1", "version_major": 2, "version_minor": 0}, "text/plain": ["Generating embeddings:   0%|          | 0/428 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["1min 27s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each)\n"]}], "source": ["%%timeit -r 1 -n 1\n", "index = VectorStoreIndex.from_documents(documents, show_progress=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### æœ€ä½³åµŒå…¥\n", "\n", "æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„onnxåµŒå…¥ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n", "\n", "embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")\n", "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n", "\n", "Settings.embed_model = embed_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "cf49fd773c6f486f9fee323db590bcd5", "version_major": 2, "version_minor": 0}, "text/plain": ["Parsing documents into nodes:   0%|          | 0/172 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "7f1f3f8a5d3140bd9982a158a6ef9b9f", "version_major": 2, "version_minor": 0}, "text/plain": ["Generating embeddings:   0%|          | 0/428 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["1min 9s Â± 0 ns per loop (mean Â± std. dev. of 1 run, 1 loop each)\n"]}], "source": ["%%timeit -r 1 -n 1\n", "index = VectorStoreIndex.from_documents(documents, show_progress=True)"]}], "metadata": {"kernelspec": {"display_name": "llama-index", "language": "python", "name": "llama-index"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}