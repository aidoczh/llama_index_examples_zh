{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/embeddings/huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 使用HuggingFace进行本地嵌入\n", "\n", "LlamaIndex支持HuggingFace嵌入模型，包括BGE、Instructor等等。\n", "\n", "此外，我们提供了工具来使用HuggingFace的[Optimum库](https://huggingface.co/docs/transformers/serialization#exporting-a-transformers-model-to-onnx-with-optimumonnxruntime)来创建和使用ONNX模型。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## HuggingFaceEmbedding\n", "\n", "基本的 `HuggingFaceEmbedding` 类是对任何 HuggingFace 模型进行嵌入的通用包装器。Hugging Face 上的所有[嵌入模型](https://huggingface.co/models?library=sentence-transformers)都应该可以使用。您可以参考[嵌入排行榜](https://huggingface.co/spaces/mteb/leaderboard)获取更多推荐信息。\n", "\n", "这个类依赖于 sentence-transformers 包，您可以使用 `pip install sentence-transformers` 进行安装。\n", "\n", "注意：如果您之前使用的是 LangChain 的 `HuggingFaceEmbeddings`，那么这应该会给出相同的结果。\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-huggingface\n", "%pip install llama-index-embeddings-instructor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n", "  warnings.warn(\"Can't initialize NVML\")\n"]}], "source": ["", "来自llama_index.embeddings.huggingface的HuggingFaceEmbedding", "", "# 加载BAAI/bge-small-en", "# embed_model = HuggingFaceEmbedding()", "", "# 加载BAAI/bge-small-en-v1.5", "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Hello World!\n", "384\n", "[-0.030880315229296684, -0.11021008342504501, 0.3917851448059082, -0.35962796211242676, 0.22797748446464539]\n"]}], "source": ["embeddings = embed_model.get_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## InstructorEmbedding\n", "\n", "教师嵌入是一类特别训练的嵌入，根据指示来增强它们的嵌入。默认情况下，查询使用 `query_instruction=\"表示用于检索支持文档的问题：\"`，文本使用 `text_instruction=\"表示用于检索的文档：\"`。\n", "\n", "它们依赖于 `Instructor` 和 `SentenceTransformers`（版本2.2.2）pip包，您可以使用 `pip install InstructorEmbedding` 和 `pip install -U sentence-transformers==2.2.2` 进行安装。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/InstructorEmbedding/instructor.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n", "  from tqdm.autonotebook import trange\n"]}, {"name": "stdout", "output_type": "stream", "text": ["load INSTRUCTOR_Transformer\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n", "  warnings.warn(\"Can't initialize NVML\")\n"]}, {"name": "stdout", "output_type": "stream", "text": ["max_seq_length  512\n"]}], "source": ["from llama_index.embeddings.instructor import InstructorEmbedding\n", "\n", "embed_model = InstructorEmbedding(model_name=\"hkunlp/instructor-base\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["768\n", "[ 0.02155361 -0.06098218  0.01796207  0.05490903  0.01526906]\n"]}], "source": ["embeddings = embed_model.get_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## OptimumEmbedding\n", "\n", "Optimum是HuggingFace库中用于将HuggingFace模型导出并在ONNX格式中运行的工具。\n", "\n", "您可以使用`pip install transformers optimum[exporters]`安装依赖项。\n", "\n", "首先，我们需要创建ONNX模型。ONNX模型提供了更快的推理速度，并且可以跨平台使用（例如在TransformersJS中）。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/home/loganm/miniconda3/envs/llama-index/lib/python3.11/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n", "  warnings.warn(\"Can't initialize NVML\")\n", "Framework not specified. Using pt to export to ONNX.\n", "Using the export variant default. Available variants are:\n", "\t- default: The default ONNX variant.\n", "Using framework PyTorch: 2.0.1+cu117\n", "Overriding 1 configuration item(s)\n", "\t- use_cache -> False\n"]}, {"name": "stdout", "output_type": "stream", "text": ["============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n", "verbose: False, log level: Level.ERROR\n", "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n", "\n", "Saved optimum model to ./bge_onnx. Use it with `embed_model = OptimumEmbedding(folder_name='./bge_onnx')`.\n"]}], "source": ["from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n", "\n", "OptimumEmbedding.create_and_save_optimum_model(\n", "    \"BAAI/bge-small-en-v1.5\", \"./bge_onnx\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["384\n", "[-0.10364960134029388, -0.20998482406139374, -0.01883639395236969, -0.5241696834564209, 0.0335749015212059]\n"]}], "source": ["embeddings = embed_model.get_text_embedding(\"Hello World!\")\n", "print(len(embeddings))\n", "print(embeddings[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 基准测试\n", "\n", "让我们尝试比较使用经典的大型文档——《IPCC气候报告》第3章。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n", "To disable this warning, you can either:\n", "\t- Avoid using `tokenizers` before the fork if possible\n", "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n", "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n", "                                 Dload  Upload   Total   Spent    Left  Speed\n", "100 20.7M  100 20.7M    0     0  16.5M      0  0:00:01  0:00:01 --:--:-- 16.5M\n"]}], "source": ["!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "from llama_index.core import Settings\n", "\n", "documents = SimpleDirectoryReader(\n", "    input_files=[\"IPCC_AR6_WGII_Chapter03.pdf\"]\n", ").load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["在这个示例中，我们将使用HuggingFace库中的预训练模型来生成文本嵌入。这些嵌入可以用于文本分类、相似度匹配等任务。我们将使用`transformers`库来加载预训练模型，并使用它来生成文本嵌入。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os", "import openai", "", "# 之后需要合成回复", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.huggingface import HuggingFaceEmbedding", "", "# 加载BAAI/bge-small-en-v1.5", "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")", "test_emeds = embed_model.get_text_embedding(\"Hello World!\")", "", "Settings.embed_model = embed_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3bfb6c2115c447e5af19bb18b5a07ebe", "version_major": 2, "version_minor": 0}, "text/plain": ["Parsing documents into nodes:   0%|          | 0/172 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "55c188c3d18049df933cd87bd5a49ed1", "version_major": 2, "version_minor": 0}, "text/plain": ["Generating embeddings:   0%|          | 0/428 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["1min 27s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}], "source": ["%%timeit -r 1 -n 1\n", "index = VectorStoreIndex.from_documents(documents, show_progress=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 最佳嵌入\n", "\n", "我们可以使用之前创建的onnx嵌入。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.huggingface_optimum import OptimumEmbedding\n", "\n", "embed_model = OptimumEmbedding(folder_name=\"./bge_onnx\")\n", "test_emeds = embed_model.get_text_embedding(\"Hello World!\")\n", "\n", "Settings.embed_model = embed_model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "cf49fd773c6f486f9fee323db590bcd5", "version_major": 2, "version_minor": 0}, "text/plain": ["Parsing documents into nodes:   0%|          | 0/172 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "7f1f3f8a5d3140bd9982a158a6ef9b9f", "version_major": 2, "version_minor": 0}, "text/plain": ["Generating embeddings:   0%|          | 0/428 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["1min 9s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"]}], "source": ["%%timeit -r 1 -n 1\n", "index = VectorStoreIndex.from_documents(documents, show_progress=True)"]}], "metadata": {"kernelspec": {"display_name": "llama-index", "language": "python", "name": "llama-index"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}