{"cells": [{"cell_type": "markdown", "id": "7d05ee2e5015619a", "metadata": {}, "source": ["# Llamafile嵌入\n"]}, {"cell_type": "markdown", "id": "7ec795e92b745944", "metadata": {}, "source": ["在本地运行LLM的最简单方法之一是使用[llamafile](https://github.com/Mozilla-Ocho/llamafile)。 llamafiles将模型权重和[specially-compiled](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#technical-details)版本的[`llama.cpp`](https://github.com/ggerganov/llama.cpp)捆绑到一个单个文件中，可以在大多数计算机上运行，而无需任何额外的依赖。它们还配备了一个嵌入的推理服务器，提供一个[API](https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/README.md#api-endpoints)来与您的模型进行交互。\n", "\n", "## 设置\n", "\n", "1) 从[HuggingFace](https://huggingface.co/models?other=llamafile)下载一个llamafile\n", "2) 使文件可执行\n", "3) 运行文件\n", "\n", "下面是一个展示所有3个设置步骤的简单bash脚本：\n", "\n", "```bash\n", "# 从HuggingFace下载一个llamafile\n", "wget https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n", "\n", "# 使文件可执行。在Windows上，只需将文件重命名为以\".exe\"结尾。\n", "chmod +x TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile\n", "\n", "# 启动模型服务器。默认情况下在http://localhost:8080监听。\n", "./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile --server --nobrowser --embedding\n", "```\n", "\n", "默认情况下，您的模型推理服务器在localhost:8080上监听。\n"]}, {"cell_type": "code", "execution_count": null, "id": "429b804c", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-llamafile"]}, {"cell_type": "code", "execution_count": null, "id": "bd65f26028357e05", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "a45593c62b5a6518", "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.llamafile import LlamafileEmbedding\n", "\n", "embedding = LlamafileEmbedding(\n", "    base_url=\"http://localhost:8080\",\n", ")\n", "\n", "pass_embedding = embedding.get_text_embedding_batch(\n", "    [\"This is a passage!\", \"This is another passage\"], show_progress=True\n", ")\n", "print(len(pass_embedding), len(pass_embedding[0]))\n", "\n", "query_embedding = embedding.get_query_embedding(\"Where is blue?\")\n", "print(len(query_embedding))\n", "print(query_embedding[:10])"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}