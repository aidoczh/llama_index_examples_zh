{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 在Intel CPU上使用IPEX-LLM进行本地嵌入\n", "\n", "> [IPEX-LLM](https://github.com/intel-analytics/ipex-llm/) 是一个用于在Intel CPU和GPU上运行LLM的PyTorch库（例如，带有iGPU的本地PC，离散GPU，如Arc、Flex和Max），具有非常低的延迟。\n", "\n", "本示例介绍了如何使用LlamaIndex在Intel CPU上进行嵌入任务，同时利用`ipex-llm`进行优化。这对于RAG、文档QA等应用非常有帮助。\n", "\n", "> **注意**\n", ">\n", "> 您可以参考[这里](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/embeddings/llama-index-embeddings-ipex-llm/examples)获取`IpexLLMEmbedding`的完整示例。请注意，在Intel CPU上运行示例时，请在命令参数中指定`-d 'cpu'`。\n", "\n", "## 安装`llama-index-embeddings-ipex-llm`\n", "\n", "这也将安装`ipex-llm`及其依赖项。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-ipex-llm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## `IpexLLMEmbedding`\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.ipex_llm import IpexLLMEmbedding\n", "\n", "embedding_model = IpexLLMEmbedding(model_name=\"BAAI/bge-large-en-v1.5\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["请注意，`IpexLLMEmbedding` 目前仅为 Hugging Face Bge 模型提供优化。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6810cbccd883423b9b7ce2ad464d1be8", "version_major": 2, "version_minor": 0}, "text/plain": ["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["embedding[:10]: [0.03578318655490875, 0.032746609300374985, -0.016696255654096603, 0.0074520050548017025, 0.016294749453663826, -0.001968140248209238, -0.002897330094128847, -0.041390497237443924, 0.030955366790294647, 0.05438097193837166]\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "702a88de60564d3dbdbdbfbdee243ced", "version_major": 2, "version_minor": 0}, "text/plain": ["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["text_embeddings[0][:10]: [0.03578318655490875, 0.032746609300374985, -0.016696255654096603, 0.0074520050548017025, 0.016294749453663826, -0.001968140248209238, -0.002897330094128847, -0.041390497237443924, 0.030955366790294647, 0.05438097193837166]\n", "text_embeddings[1][:10]: [0.03155018016695976, 0.03177601844072342, -0.00304483063519001, 0.004364349413663149, 0.005002604331821203, -0.02680951915681362, -0.005840071476995945, -0.022466979920864105, 0.05162270367145538, 0.05928812175989151]\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9c752ee866f642b08d60080bfc2ce566", "version_major": 2, "version_minor": 0}, "text/plain": ["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["query_embedding[:10]: [0.053250256925821304, 0.0036771567538380623, 0.003390512429177761, 0.014903719536960125, -0.00263631297275424, -0.022365037351846695, -0.004524332471191883, -0.018143195658922195, 0.03799865022301674, 0.07393667846918106]\n"]}], "source": ["sentence = \"IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\"\n", "query = \"What is IPEX-LLM?\"\n", "\n", "text_embedding = embedding_model.get_text_embedding(sentence)\n", "print(f\"embedding[:10]: {text_embedding[:10]}\")\n", "\n", "text_embeddings = embedding_model.get_text_embedding_batch([sentence, query])\n", "print(f\"text_embeddings[0][:10]: {text_embeddings[0][:10]}\")\n", "print(f\"text_embeddings[1][:10]: {text_embeddings[1][:10]}\")\n", "\n", "query_embedding = embedding_model.get_query_embedding(query)\n", "print(f\"query_embedding[:10]: {query_embedding[:10]}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}