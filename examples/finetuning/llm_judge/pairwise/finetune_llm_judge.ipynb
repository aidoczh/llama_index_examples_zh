{"cells": [{"cell_type": "markdown", "id": "722c13cc-a78a-4037-859e-48c538d00d9b", "metadata": {}, "source": ["# 使用知识蒸馏对GPT-3.5法官（成对）进行微调\n", "\n", "最近的研究表明，GPT-4在评估LLM生成的文本时能够与人类评审员紧密对齐（例如，参见[[1]](https://arxiv.org/abs/2306.05685)，[[2]](https://arxiv.org/abs/2303.16634)）。在本笔记本中，我们将演示如何使用`llama_index`库从GPT-4中蒸馏知识到GPT-3.5，使得较小的GPT-3.5能够接近GPT-4的性能；从而间接地接近人类评审员。\n", "\n", "为此，我们将执行以下高级步骤：\n", "\n", "1. 生成数据集：`train_dataset` 和 `test_dataset`\n", "2. 进行知识蒸馏（使用`train_dataset`）\n", "3. 在`test_dataset`上评估蒸馏模型\n"]}, {"cell_type": "code", "execution_count": null, "id": "ac1863cf", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-readers-wikipedia\n", "%pip install llama-index-finetuning\n", "%pip install llama-index-llms-openai\n", "%pip install llama-index-finetuning-callbacks\n", "%pip install llama-index-llms-huggingface"]}, {"cell_type": "code", "execution_count": null, "id": "4e24ebb2", "metadata": {}, "outputs": [], "source": ["# 注意：此笔记本进行了多次API调用，以使用OpenAI GPT模型生成文本，以及托管在HuggingFace上的模型。如果您不想等待这些生成过程，则可以使用下面提供的`wget`命令获取此笔记本的数据。", "", "# !wget \"https://www.dropbox.com/scl/fo/m7skpjdbpb0g3p76y6epe/h?rlkey=omh2ysgh9qqqztf81qvjlivu2&dl=1\" -O pairwise.zip"]}, {"cell_type": "code", "execution_count": null, "id": "206d01ad-d2c3-46a5-876b-7461dd593147", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "id": "065f7df9-a048-43aa-b862-290e832ea631", "metadata": {}, "outputs": [], "source": ["", "import os", "", "# 我们将使用HuggingFace上的模型作为我们的LLM答案生成器", "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")", "", "# 我们将使用GPT-4和GPT-3.5 + OpenAI Fine-Tuning", "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"]}, {"cell_type": "code", "execution_count": null, "id": "0ec7031c-9da8-4116-969a-b1180a0fc118", "metadata": {}, "outputs": [], "source": ["import pandas as pd", "", "", "# 定义jupyter显示函数", "def display_eval_df(question, source, answer_a, answer_b, result) -> None:", "    \"\"\"漂亮打印问题/答案 + gpt-4判断数据集。\"\"\"", "    eval_df = pd.DataFrame(", "        {", "            \"Question\": question,", "            \"Source\": source,", "            \"Model A\": answer_a[\"model\"],", "            \"Answer A\": answer_a[\"text\"],", "            \"Model B\": answer_b[\"model\"],", "            \"Answer B\": answer_b[\"text\"],", "            \"Score\": result.score,", "            \"Judgement\": result.feedback,", "        },", "        index=[0],", "    )", "    eval_df = eval_df.style.set_properties(", "        **{", "            \"inline-size\": \"300px\",", "            \"overflow-wrap\": \"break-word\",", "        },", "        subset=[\"Answer A\", \"Answer B\"]", "    )", "    display(eval_df)"]}, {"cell_type": "markdown", "id": "2c89edf1-b359-4370-8b1e-fad279508c68", "metadata": {}, "source": ["## 步骤1 生成数据集：`train_dataset` 和 `test_dataset`\n", "\n", "对于我们将生成问题并提示各种LLM回答的数据集，我们将使用`WikipediaReader`来读取几个城市的“<city>的历史”。我们将把我们的城市分成两个列表：一个用于`train_dataset`，另一个用于`test_dataset`。\n"]}, {"cell_type": "code", "execution_count": null, "id": "88777f6b-a746-4df9-80c4-e6296d4e7a3b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}], "source": ["!pip install wikipedia -q"]}, {"cell_type": "code", "execution_count": null, "id": "bff11314-315d-465b-983a-f17d657436ec", "metadata": {}, "outputs": [], "source": ["# 维基百科页面", "from llama_index.readers.wikipedia import WikipediaReader", "", "train_cities = [", "    \"旧金山\",", "    \"多伦多\",", "    \"纽约\",", "    \"温哥华\",", "    \"蒙特利尔\",", "    \"波士顿\",", "]", "", "test_cities = [", "    \"东京\",", "    \"新加坡\",", "    \"巴黎\",", "]", "", "train_documents = WikipediaReader().load_data(", "    pages=[f\"{x}的历史\" for x in train_cities]", ")", "test_documents = WikipediaReader().load_data(", "    pages=[f\"{x}的历史\" for x in test_cities]", ")"]}, {"cell_type": "markdown", "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590", "metadata": {}, "source": ["### 使用`DatasetGenerator`构建`train_dataset`和`test_dataset`\n", "\n", "现在我们已经有了`Document`的训练集和测试集，下一步是生成问题。为此，我们将使用`DatasetGenerator`，它使用LLM从给定的文档集生成问题。\n"]}, {"cell_type": "markdown", "id": "cef531ed-8e97-4d8f-8cc3-6f7e0c6ca141", "metadata": {}, "source": ["在这个部分，我们将生成一些问题，以便进行问答练习。\n"]}, {"cell_type": "code", "execution_count": null, "id": "99afd212-38b0-492c-91ea-a810e126ad2d", "metadata": {}, "outputs": [], "source": ["QUESTION_GEN_PROMPT = (\n", "    \"You are a Teacher/ Professor. Your task is to setup \"\n", "    \"a quiz/examination. Using the provided context, formulate \"\n", "    \"a single question that captures an important fact from the \"\n", "    \"context. Restrict the question to the context information provided.\"\n", ")"]}, {"cell_type": "markdown", "id": "f2dc60af-2ef8-43b6-8b24-f46adc223d03", "metadata": {}, "source": ["搞定这一切之后，让我们开始行动吧。首先，我们将下载参考的pdf文档，并根据它创建一组问题。\n"]}, {"cell_type": "code", "execution_count": null, "id": "b0e10603-a0e0-4c87-a4d5-fdf8f1ca0303", "metadata": {}, "outputs": [], "source": ["# 根据块生成问题", "from llama_index.core.evaluation import DatasetGenerator", "from llama_index.llms.openai import OpenAI", "", "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)", "", "", "# 实例化用于训练和测试的DatasetGenerator", "train_dataset_generator = DatasetGenerator.from_documents(", "    train_documents,", "    question_gen_query=QUESTION_GEN_PROMPT,", "    llm=llm,", "    show_progress=True,", "    num_questions_per_chunk=25,", ")", "", "test_dataset_generator = DatasetGenerator.from_documents(", "    test_documents,", "    question_gen_query=QUESTION_GEN_PROMPT,", "    llm=llm,", "    show_progress=True,", "    num_questions_per_chunk=25,", ")"]}, {"cell_type": "code", "execution_count": null, "id": "f03e2276-92dc-48c3-8cd0-583655ab7ee1", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:02<00:00, 36.34it/s]\n"]}], "source": ["# 使用DatasetGenerator从节点创建问题", "train_questions = train_dataset_generator.generate_questions_from_nodes(", "    num=200", ")"]}, {"cell_type": "code", "execution_count": null, "id": "5df0a873-e22d-4eb7-97f9-336ca33346fe", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:02<00:00, 29.98it/s]\n"]}], "source": ["test_questions = test_dataset_generator.generate_questions_from_nodes(num=150)"]}, {"cell_type": "code", "execution_count": null, "id": "c84e1a9a-3d3f-48c9-a8e2-c5ce124c0250", "metadata": {}, "outputs": [{"data": {"text/plain": ["(75, 64)"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["len(train_questions), len(test_questions)"]}, {"cell_type": "code", "execution_count": null, "id": "845f3dd3-62ab-4aac-a995-cf16d3306f80", "metadata": {}, "outputs": [{"data": {"text/plain": ["['What event in 1906 caused significant damage to San Francisco but was followed by a quick rebuild?',\n", " 'What was the name of the first significant homestead established outside the immediate vicinity of Mission Dolores in San Francisco?',\n", " \"What event in 1855 led to the establishment of San Francisco's first county hospital and the development of California's system of county hospitals for the poor?\"]"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["# 让我们来看看其中的一些", "train_questions[:3]"]}, {"cell_type": "code", "execution_count": null, "id": "52556d80-0aef-4e2e-a4c8-2ba3a48ffe2d", "metadata": {}, "outputs": [{"data": {"text/plain": ["['Question: What was the name of the oldest Buddhist temple in Tokyo, founded in 628?',\n", " 'What event marked the end of the samurai system and feudal class divisions in Tokyo?',\n", " 'Question: What role did the Tokyo Imperial University play in the Meiji Era?']"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["test_questions[:3]"]}, {"cell_type": "markdown", "id": "b201d9cb-4746-4c71-8728-55e56cb8b76f", "metadata": {}, "source": ["#### 生成问题的答案\n", "\n", "接下来的步骤是使用LLM生成答案。请记住，重点是评判这些生成的答案。因此，我们稍后将使用GPT模型来评判这些答案。\n", "\n", "但是为了生成这些问题的答案，我们将使用另外两个LLM，即：Llama-2和Mistral。为了做到这一点，我们首先要为我们的文档创建一个向量存储和一个相关的检索器，这两个LLM答案生成器都将使用。\n"]}, {"cell_type": "code", "execution_count": null, "id": "e973d108-a350-4afc-8add-69969c59c710", "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex", "from llama_index.core.retrievers import VectorIndexRetriever", "", "# 创建向量索引", "train_index = VectorStoreIndex.from_documents(documents=train_documents)", "", "# 在该索引上创建检索器", "train_retriever = VectorIndexRetriever(", "    index=train_index,", "    similarity_top_k=2,", ")", "", "# 为后续使用创建测试的向量索引", "test_index = VectorStoreIndex.from_documents(documents=test_documents)", "", "# 为后续使用创建测试的检索器", "test_retriever = VectorIndexRetriever(", "    index=test_index,", "    similarity_top_k=2,", ")"]}, {"cell_type": "markdown", "id": "65125845-b96c-4f84-b45c-7ddaf6910d92", "metadata": {}, "source": ["从这里开始，我们将构建`RetrieverQueryEngine`，它将接收我们的查询（即问题）进行处理。请注意，我们使用`HuggingFaceInferenceAPI`来进行LLM答案生成，而Llama-2需要权限。如果您还没有获得对这些模型的访问权限，那么可以随意将Llama-2替换为您选择的其他模型。\n"]}, {"cell_type": "code", "execution_count": null, "id": "4b687ddf-c8fa-4073-9aa9-0b9cfec23f55", "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_engine import RetrieverQueryEngine", "from llama_index.llms.huggingface import HuggingFaceInferenceAPI", "", "", "def create_query_engine(", "    hf_name: str, retriever: VectorIndexRetriever, hf_llm_generators: dict", ") -> RetrieverQueryEngine:", "    \"\"\"使用HuggingFaceInferenceAPI LLM创建一个RetrieverQueryEngine\"\"\"", "    if hf_name not in hf_llm_generators:", "        raise KeyError(\"模型未在hf_llm_generators中列出\")", "    llm = HuggingFaceInferenceAPI(", "        model_name=hf_llm_generators[hf_name],", "        context_window=2048,  # 用于使用refine", "        token=HUGGING_FACE_TOKEN,", "    )", "    return RetrieverQueryEngine.from_args(retriever=retriever, llm=llm)"]}, {"cell_type": "code", "execution_count": null, "id": "84331853-ac29-49ca-85c1-e874d26e5f83", "metadata": {}, "outputs": [], "source": ["# 定义我们的llm生成器（查询引擎）", "hf_llm_generators = {", "    \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.1\",", "    \"llama2-7b-chat\": \"meta-llama/Llama-2-7b-chat-hf\",", "}", "", "train_query_engines = {", "    mdl: create_query_engine(mdl, train_retriever, hf_llm_generators)", "    for mdl in hf_llm_generators.keys()", "}", "", "test_query_engines = {", "    mdl: create_query_engine(mdl, test_retriever, hf_llm_generators)", "    for mdl in hf_llm_generators.keys()", "}"]}, {"cell_type": "markdown", "id": "1760a173-675a-4db0-8f66-0b396c2a34d7", "metadata": {}, "source": ["我们现在准备从各种LLM中生成答案。我们现在将为`train_dataset`执行此操作，并暂时不为`test_dataset`执行此操作，直到我们使用它的时候。\n", "\n", "注意：生成答案可能需要一些时间。如果您不想等待，您可以选择加载包含每个问题的Llama-2和Mistral答案的`train_qa.jsonl`文件。\n"]}, {"cell_type": "code", "execution_count": null, "id": "7d1d126f-8fcc-42ea-b143-4533638763a4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [07:40<00:00,  6.14s/it]\n"]}], "source": ["import tqdm", "import random", "", "train_dataset = []", "for q in tqdm.tqdm(train_questions):", "    # 随机选择两个LLM来生成对这个问题的答案", "    model_versus = random.sample(list(train_query_engines.items()), 2)", "", "    # 为这个问题准备数据", "    data_entry = {\"question\": q}", "    responses = []", "    source = None", "", "    # 生成答案", "    for name, engine in model_versus:", "        response = engine.query(q)", "        response_struct = {}", "        response_struct[\"model\"] = name", "        response_struct[\"text\"] = str(response)", "        if source is not None:", "            assert source == response.source_nodes[0].node.text[:1000] + \"...\"", "        else:", "            source = response.source_nodes[0].node.text[:1000] + \"...\"", "        responses.append(response_struct)", "", "    data_entry[\"answers\"] = responses", "    data_entry[\"source\"] = source", "    train_dataset.append(data_entry)"]}, {"cell_type": "markdown", "id": "6e653515-75c4-4987-87e4-c0a0b17a0bdf", "metadata": {}, "source": ["### 获取Mistral和LLama-2答案的GPT-4评估\n", "\n", "正如之前多次提到的，本指南的目的是从GPT-4法官中微调LLM法官。因此，为了完成我们的`train_dataset`，我们现在需要实例化我们的GPT-4法官，并让它评估其他LLM（Llama-2和Mistral）提供的答案。为此，我们将使用`PairwiseComparisonEvaluator`类。然后，这个法官将比较这两个答案，并给出一个裁决，判断Llama-2的答案更好，Mistral的答案更好，还是平局。\n", "\n", "这里有一点额外的细微之处，因为在两两评估中，我们必须注意“位置偏见”的潜在问题。这是指法官偏向于首先呈现给它的第一个答案（在提示/上下文中）。为了考虑这种位置偏见，我们要求GPT-4法官对每个样本进行两次评估，在第二次评估中，我们交换两个答案的呈现顺序（即第一次评估：Llama-2然后Mistral，第二次评估：Mistral然后Llama-2）。\n", "\n", "最后，我们还使用`OpenAIFineTuningHandler`，它将收集我们最终需要微调GPT-3.5的所有聊天记录。\n", "\n", "注意：生成裁决需要一些时间。同样，您可以选择将`train_qa.jsonl`加载为`train_dataset`。此外，我们还存储了传递给OpenAI用于微调GPT-3.5的JSONL文件。\n"]}, {"cell_type": "code", "execution_count": null, "id": "afe8958f-62fd-41a8-8d21-3055cd80de9c", "metadata": {}, "outputs": [], "source": ["# 实例化gpt-4评判器", "from llama_index.llms.openai import OpenAI", "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler", "from llama_index.core.callbacks import CallbackManager", "from llama_index.core.evaluation import PairwiseComparisonEvaluator", "from llama_index.core import Settings", "", "# 注意：这个finetuning_handler将为每个查询收集2倍的聊天记录：", "# 一个是原始的，另一个是翻转后的", "main_finetuning_handler = OpenAIFineTuningHandler()", "callback_manager = CallbackManager([main_finetuning_handler])", "Settings.callback_manager = callback_manager", "", "llm_4 = OpenAI(temperature=0, model=\"gpt-4\", callback_manager=callback_manager)", "", "gpt4_judge = PairwiseComparisonEvaluator(llm=llm)", ""]}, {"cell_type": "code", "execution_count": null, "id": "27c0a4d4-e55f-4189-8e64-132c128bd5cd", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [48:04<00:00, 38.46s/it]\n"]}], "source": ["for data_entry in tqdm.tqdm(train_dataset):", "    final_eval_result = await gpt4_judge.aevaluate(", "        query=data_entry[\"question\"],", "        response=data_entry[\"answers\"][0][\"text\"],", "        second_response=data_entry[\"answers\"][1][\"text\"],", "        reference=data_entry[\"source\"],", "    )", "", "    # 保存最终结果", "    judgement = {}", "    judgement[\"llm\"] = \"gpt_4\"", "    judgement[\"score\"] = final_eval_result.score", "    judgement[\"text\"] = final_eval_result.response", "    judgement[\"source\"] = final_eval_result.pairwise_source", "    data_entry[\"evaluations\"] = [judgement]"]}, {"cell_type": "markdown", "id": "7faae6f0-bb84-4762-8716-5594bf8f25a7", "metadata": {}, "source": ["让我们看看GPT-4评估中的一个示例。\n"]}, {"cell_type": "code", "execution_count": null, "id": "acfd4601-289e-4fba-bc74-b9a0cfcce09b", "metadata": {}, "outputs": [{"data": {"text/html": ["<style type=\"text/css\">\n", "#T_47a77_row0_col3, #T_47a77_row0_col5 {\n", "  inline-size: 300px;\n", "  overflow-wrap: break-word;\n", "}\n", "</style>\n", "<table id=\"T_47a77\">\n", "  <thead>\n", "    <tr>\n", "      <th class=\"blank level0\" >&nbsp;</th>\n", "      <th id=\"T_47a77_level0_col0\" class=\"col_heading level0 col0\" >Question</th>\n", "      <th id=\"T_47a77_level0_col1\" class=\"col_heading level0 col1\" >Source</th>\n", "      <th id=\"T_47a77_level0_col2\" class=\"col_heading level0 col2\" >Model A</th>\n", "      <th id=\"T_47a77_level0_col3\" class=\"col_heading level0 col3\" >Answer A</th>\n", "      <th id=\"T_47a77_level0_col4\" class=\"col_heading level0 col4\" >Model B</th>\n", "      <th id=\"T_47a77_level0_col5\" class=\"col_heading level0 col5\" >Answer B</th>\n", "      <th id=\"T_47a77_level0_col6\" class=\"col_heading level0 col6\" >Score</th>\n", "      <th id=\"T_47a77_level0_col7\" class=\"col_heading level0 col7\" >Judgement</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th id=\"T_47a77_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n", "      <td id=\"T_47a77_row0_col0\" class=\"data row0 col0\" >What resources are available in Boston for studying and preserving historical artifacts and remains?</td>\n", "      <td id=\"T_47a77_row0_col1\" class=\"data row0 col1\" >In 1963, Boston Mayor John F. Collins and Boston Redevelopment Authority (BRA) executive Edward J. Logue organized a consortium of savings banks, cooperatives, and federal and state savings and loan associations in the city called the Boston Banks Urban Renewal Group (B-BURG) that would reverse redline parts of Dorchester, Roxbury, and Mattapan along Blue Hill Avenue. Despite the passage of legislation by the 156th Massachusetts General Court banning racial discrimination or segregation in housing in 1950, as well as the issuance of Executive Order 11063 by President John F. Kennedy in 1962 requiring all federal agencies to prevent racial discrimination in all federally-funded subsidized housing in the United States, the Boston Housing Authority (BHA) Board actively segregated the public housing developments in the city during the Collins administration as well, with BHA departments engaging in bureaucratic resistance against integration through at least 1966 and the Board retaining co...</td>\n", "      <td id=\"T_47a77_row0_col2\" class=\"data row0 col2\" >llama2-7b-chat</td>\n", "      <td id=\"T_47a77_row0_col3\" class=\"data row0 col3\" >\n", "There are several resources available in Boston for studying and preserving historical artifacts and remains. Some of these include:\n", "\n", "1. The Boston Historical Society: This organization provides access to historical artifacts, photographs, and documents related to Boston's history.\n", "2. Boston Mapjunction: This website provides over 200 historical maps of Boston dating back to 1630, as well as aerial photos compared with maps of today.\n", "3. City of Boston Archaeology Program and Lab: The City of Boston has a City Archaeologist on staff to oversee any lots of land to be developed for historical artifacts and significance, and to manage the archaeological remains located on public land in Boston. The program also has an Archaeology Laboratory and Education and Curation Center.\n", "4. The Freedom House Photographs Collection: This collection contains over 2,000 images of Roxbury people, places, and events from 1950 to 1975.\n", "5. Vital Records of Boston: This resource provides access to historical records related to births, marriages, and deaths in Boston.\n", "6. Reading and Everyday Life in</td>\n", "      <td id=\"T_47a77_row0_col4\" class=\"data row0 col4\" >mistral-7b-instruct</td>\n", "      <td id=\"T_47a77_row0_col5\" class=\"data row0 col5\" >\n", "The City of Boston has a City Archaeologist on staff to oversee any lots of land to be developed for historical artifacts and significance, and to manage the archaeological remains located on public land in Boston, and also has a City Archaeology Program and an Archaeology Laboratory, Education and Curation Center. The Freedom House Photographs Collection contains over 2,000 images of Roxbury people, places and events, 1950–1975 (Archives and Special Collections of the Northeastern University Libraries in Boston, MA).</td>\n", "      <td id=\"T_47a77_row0_col6\" class=\"data row0 col6\" >1.000000</td>\n", "      <td id=\"T_47a77_row0_col7\" class=\"data row0 col7\" >Assistant A provides a more comprehensive answer, listing several resources available in Boston for studying and preserving historical artifacts and remains. These include the Boston Historical Society, Boston Mapjunction, the City of Boston Archaeology Program and Lab, the Freedom House Photographs Collection, and Vital Records of Boston. This answer is more detailed and provides a wider range of resources for the user to explore.\n", "\n", "Assistant B, on the other hand, only mentions the City of Boston Archaeology Program and Lab and the Freedom House Photographs Collection. While these are relevant resources, the answer lacks the depth and variety of Assistant A's response.\n", "\n", "Therefore, based on the depth, variety, and level of detail in the responses, Assistant A's answer is superior.\n", "\n", "Final Verdict: [[A]]</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n"], "text/plain": ["<pandas.io.formats.style.Styler at 0x2c6687610>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# 让我们看看最后一个", "display_eval_df(", "    question=data_entry[\"question\"],", "    source=data_entry[\"source\"],", "    answer_a=data_entry[\"answers\"][0],", "    answer_b=data_entry[\"answers\"][1],", "    result=final_eval_result,", ")", ""]}, {"cell_type": "markdown", "id": "35956034-42aa-4bb7-8107-f979cd82c7e3", "metadata": {}, "source": ["#### 对JSONL进行特别注意\n", "\n", "由于存在两种评估（一种是LLM答案原始呈现顺序的评估，另一种是翻转顺序的评估），我们需要小心选择正确的评估结果，以保留在我们的微调数据集中。这意味着我们需要挑选出由我们的 `OpenAIFineTuningHandler` 收集的正确事件，然后只使用这些事件来准备我们将传递给OpenAI微调API的JSONL。\n"]}, {"cell_type": "code", "execution_count": null, "id": "302d6e45-b6c6-4694-8aee-fbba853d57a9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Wrote 150 examples to pairwise_finetuning_events.jsonl\n"]}], "source": ["main_finetuning_handler.save_finetuning_events(\n", "    \"pairwise_finetuning_events.jsonl\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "9e05e278-0f94-4aa1-a745-6ad02f7ee99d", "metadata": {}, "outputs": [], "source": ["import json", "", "# 获取fine_tuning_examples主数据集", "with open(\"pairwise_finetuning_events.jsonl\") as f:", "    combined_finetuning_events = [json.loads(line) for line in f]"]}, {"cell_type": "code", "execution_count": null, "id": "209391b1-a858-47e3-b2a6-7513c147c78d", "metadata": {}, "outputs": [], "source": ["finetuning_events = (", "    []", ")  # 用于存储使用原始呈现顺序的事件", "flipped_finetuning_events = (", "    []", ")  # 用于存储使用翻转顺序的事件", "", "for ix, event in enumerate(combined_finetuning_events):", "    if ix % 2 == 0:  # 我们总是先进行原始排序", "        finetuning_events += [event]", "    else:  # 然后我们翻转顺序，让GPT-4做出另一个判断", "        flipped_finetuning_events += [event]"]}, {"cell_type": "code", "execution_count": null, "id": "dcd58a37-ca96-41f5-9c07-9db11e791f80", "metadata": {}, "outputs": [], "source": ["assert len(finetuning_events) == len(flipped_finetuning_events)"]}, {"cell_type": "code", "execution_count": null, "id": "cec6e0a5-a8a3-4270-a514-a80f1e2aaa5d", "metadata": {}, "outputs": [], "source": ["# 我们需要选择要保留的聊天记录", "resolved_finetuning_events = []", "for ix, data_entry in enumerate(train_dataset):", "    if data_entry[\"evaluations\"][0][\"source\"] == \"original\":", "        resolved_finetuning_events += [finetuning_events[ix]]", "    elif data_entry[\"evaluations\"][0][\"source\"] == \"flipped\":", "        resolved_finetuning_events += [flipped_finetuning_events[ix]]", "    else:", "        continue"]}, {"cell_type": "code", "execution_count": null, "id": "62e56e30-9676-418d-bb57-a0ed90909b44", "metadata": {}, "outputs": [], "source": ["with open(\"resolved_pairwise_finetuning_events.jsonl\", \"w\") as outfile:\n", "    for entry in resolved_finetuning_events:\n", "        print(json.dumps(entry), file=outfile)"]}, {"cell_type": "markdown", "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626", "metadata": {}, "source": ["## 第2步 执行知识蒸馏\n", "\n", "好的，现在是时候从GPT-4中提炼一些知识到GPT-3.5了。为了做到这一点，我们将使用`OpenAIFinetuneEngine`类以及刚刚创建的`resolved_pairwise_finetuning_events.jsonl`文件。\n"]}, {"cell_type": "code", "execution_count": null, "id": "cab328da-c235-4ae6-a7ff-4315fa0b07eb", "metadata": {}, "outputs": [], "source": ["from llama_index.finetuning import OpenAIFinetuneEngine\n", "\n", "finetune_engine = OpenAIFinetuneEngine(\n", "    \"gpt-3.5-turbo\",\n", "    \"resolved_pairwise_finetuning_events.jsonl\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "6fd14ca3-97c5-4a1c-a257-6ed183b09968", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Num examples: 72\n", "First example:\n", "{'role': 'system', 'content': \"Please act as an impartial judge and evaluate the quality of the responses provided by two AI question-answering assistants to the user question perhaps with added reference which are displayed below. You should choose the assistant that follows the user’s instructions and answers the user’s question better using the provided context. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: '[[A]]' if assistant A is better, '[[B]]' if assistant B is better, and '[[C]]' for a tie.\\n\"}\n", "{'role': 'user', 'content': \"[User Question]\\nWhat event in 1906 caused significant damage to San Francisco but was followed by a quick rebuild?\\n\\n[The Start of Reference]\\n=== Reconstruction ===\\nAlmost immediately after the quake re-planning and reconstruction plans were hatched to quickly rebuild the city. One of the more famous and ambitious plans, proposed before the fire, came from famed urban planner, Daniel Burnham. His bold plan called for Haussmann style avenues, boulevards, and arterial thoroughfares that radiated across the city, a massive civic center complex with classical structures, what would have been the largest urban park in the world, stretching from Twin Peaks to Lake Merced with a large athenaeum at its peak, and various other proposals. This plan was dismissed by critics (both at the time and now), as impractical and unrealistic to municipal supply and demand. Property owners and the Real Estate industry were against the idea as well due to the amounts of their land the city would have to purchase to realize such proposals. While the original street grid was restored, many of Burnham's proposals eventually saw the light of day such ...\\n[The End of Reference]\\n\\n[The Start of Assistant A’s Answer]\\n\\n\\nThe 1906 earthquake and fire caused significant damage to San Francisco, with over 80% of the city destroyed, including almost all of the downtown core. The earthquake, which had a magnitude of 7.8 on the Richter scale, resulted from the rupture of over 270 miles of the San Andreas Fault, centered immediately offshore of San Francisco. Water mains ruptured throughout the city, and the fires that followed burned out of control for days, trapping many residents and causing widespread destruction. Despite the devastation, the city quickly rebuilt, with many of the buildings constructed after the disaster being more earthquake-resistant than those that had come before. The official death toll from this event was initially reported as 478, but was later revised to over 3,000.\\n[The End of Assistant A’s Answer]\\n\\n[The Start of Assistant B’s Answer]\\n1906 earthquake and fire.\\n[The End of Assistant B’s Answer]\"}\n", "{'role': 'assistant', 'content': \"Assistant A provides a detailed and comprehensive response to the user's question, explaining the event that caused significant damage to San Francisco in 1906, which was the earthquake and fire. It also provides additional information about the earthquake's magnitude, the extent of the damage, and the city's quick rebuilding efforts. On the other hand, Assistant B's response is very brief and only mentions the earthquake and fire, without providing any additional information or context. Therefore, Assistant A's response is more helpful, relevant, accurate, and detailed. \\n\\nFinal Verdict: [[A]]\"}\n", "No errors found\n", "Num examples missing system message: 0\n", "Num examples missing user message: 0\n", "\n", "#### Distribution of num_messages_per_example:\n", "min / max: 3, 3\n", "mean / median: 3.0, 3.0\n", "p5 / p95: 3.0, 3.0\n", "\n", "#### Distribution of num_total_tokens_per_example:\n", "min / max: 579, 1198\n", "mean / median: 818.9305555555555, 772.0\n", "p5 / p95: 625.9, 1076.0\n", "\n", "#### Distribution of num_assistant_tokens_per_example:\n", "min / max: 66, 248\n", "mean / median: 129.26388888888889, 117.5\n", "p5 / p95: 81.0, 193.9\n", "\n", "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n", "Dataset has ~58963 tokens that will be charged for during training\n", "By default, you'll train for 3 epochs on this dataset\n", "By default, you'll be charged for ~176889 tokens\n", "As of August 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n", "This means your total cost for training will be $0.471704 per epoch.\n"]}], "source": ["finetune_engine.finetune()"]}, {"cell_type": "code", "execution_count": null, "id": "d882690c-b072-406c-a3d2-91ffca762a7d", "metadata": {}, "outputs": [{"data": {"text/plain": ["<FineTuningJob fine_tuning.job id=ftjob-jLxZggQbHz2F98IlhQEI9KIw at 0x2e6b91170> JSON: {\n", "  \"object\": \"fine_tuning.job\",\n", "  \"id\": \"ftjob-jLxZggQbHz2F98IlhQEI9KIw\",\n", "  \"model\": \"gpt-3.5-turbo-0613\",\n", "  \"created_at\": 1698817329,\n", "  \"finished_at\": 1698817949,\n", "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:llamaindex::8FyRSSOl\",\n", "  \"organization_id\": \"org-1ZDAvajC6v2ZtAP9hLEIsXRz\",\n", "  \"result_files\": [\n", "    \"file-qLTnxGSZX2rHP0Q7wJIDDNWX\"\n", "  ],\n", "  \"status\": \"succeeded\",\n", "  \"validation_file\": null,\n", "  \"training_file\": \"file-xsAaOBjQ949ti0qk1xHHLOiF\",\n", "  \"hyperparameters\": {\n", "    \"n_epochs\": 3\n", "  },\n", "  \"trained_tokens\": 176457,\n", "  \"error\": null\n", "}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["# 我们可以按如下方式检查当前作业的状态", "# 这可能需要一些时间...", "finetune_engine.get_current_job()"]}, {"cell_type": "markdown", "id": "3e631ccf-b4f5-478d-b112-52dc88ffed1e", "metadata": {}, "source": ["## 3 评估在测试数据集上对GPT-3.5进行微调\n", "\n", "现在我们已经有了我们的微调过的GPT-3.5，让我们看看它在测试集上的表现如何。但首先，请记住我们说过要推迟创建`test_dataset`直到我们需要它的时候？现在是时候了。因此，我们将在这里重复创建`train_dataset`的过程，但这次是为了`test_dataset`。\n", "\n", "注意：生成这些答案和评估需要一些时间。您可以选择加载`test_qa_complete.jsonl`，其中包含了三个考虑的LLM评估的所有内容。您可以将其加载为`test_dataset`，然后运行下面“指标”小节中的代码。\n"]}, {"cell_type": "code", "execution_count": null, "id": "9155fba3-a4bd-4822-b2ba-93c8eec428b5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [28:23<00:00, 26.62s/it]\n"]}], "source": ["import random", "", "# 使用Llama-2和Mistral LLMs生成测试查询的答案", "test_dataset = []", "for q in tqdm.tqdm(test_questions):", "    # 随机选择两个LLMs来生成这个问题的答案", "    model_versus = random.sample(list(test_query_engines.items()), 2)", "", "    # 为这个问题准备数据", "    data_entry = {\"question\": q}", "    responses = []", "    source = None", "", "    # 生成答案", "    for name, engine in model_versus:", "        response = engine.query(q)", "        response_struct = {}", "        response_struct[\"model\"] = name", "        response_struct[\"text\"] = str(response)", "        if source is not None:", "            assert source == response.source_nodes[0].node.text[:1000] + \"...\"", "        else:", "            source = response.source_nodes[0].node.text[:1000] + \"...\"", "        responses.append(response_struct)", "", "    data_entry[\"answers\"] = responses", "    data_entry[\"source\"] = source", "    test_dataset.append(data_entry)"]}, {"cell_type": "code", "execution_count": null, "id": "ccb666db-5f57-49b4-9e96-18c0de17beac", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [43:21<00:00, 40.66s/it]\n"]}], "source": ["# 获取Mistal和Llama-2答案的GPT-4判断", "for data_entry in tqdm.tqdm(test_dataset):", "    final_eval_result = await gpt4_judge.aevaluate(", "        query=data_entry[\"question\"],", "        response=data_entry[\"answers\"][0][\"text\"],", "        second_response=data_entry[\"answers\"][1][\"text\"],", "        reference=data_entry[\"source\"],", "    )", "", "    # 保存最终结果", "    judgement = {}", "    judgement[\"llm\"] = \"gpt_4\"", "    judgement[\"score\"] = final_eval_result.score", "    judgement[\"text\"] = final_eval_result.response", "    judgement[\"source\"] = final_eval_result.pairwise_source", "    data_entry[\"evaluations\"] = [judgement]"]}, {"cell_type": "code", "execution_count": null, "id": "991a888e-865c-48de-a5c0-b98daa992904", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [04:08<00:00,  3.88s/it]\n"]}], "source": ["from llama_index.core.evaluation import EvaluationResult", "", "# 使用我们经过精细调整的 GPT-3.5 来评估答案", "ft_llm = finetune_engine.get_finetuned_model()", "", "", "ft_gpt_3p5_judge = PairwiseComparisonEvaluator(llm=ft_llm)", "", "for data_entry in tqdm.tqdm(test_dataset):", "    try:", "        final_eval_result = await ft_gpt_3p5_judge.aevaluate(", "            query=data_entry[\"question\"],", "            response=data_entry[\"answers\"][0][\"text\"],", "            second_response=data_entry[\"answers\"][1][\"text\"],", "            reference=data_entry[\"source\"],", "        )", "    except:", "        final_eval_result = EvaluationResult(", "            query=data_entry[\"question\"],", "            response=\"\",", "            passing=None,", "            score=0.5,", "            feedback=\"\",", "            pairwise_source=\"output-cannot-be-parsed\",", "        )", "", "    # 保存最终结果", "    judgement = {}", "    judgement[\"llm\"] = \"ft_gpt_3p5\"", "    judgement[\"score\"] = final_eval_result.score", "    judgement[\"text\"] = final_eval_result.response", "    judgement[\"source\"] = final_eval_result.pairwise_source", "    data_entry[\"evaluations\"] += [judgement]"]}, {"cell_type": "code", "execution_count": null, "id": "8e2caba7-6e8e-4daa-9145-807f9670a6ec", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [09:32<00:00,  8.95s/it]\n"]}], "source": ["# 同样地，使用一个未经微调的评判器来评估答案", "gpt_3p5_llm = OpenAI(model=\"gpt-3.5-turbo\")", "", "gpt_3p5_judge = PairwiseComparisonEvaluator(llm=gpt_3p5_llm)", "", "for data_entry in tqdm.tqdm(test_dataset):", "    try:", "        final_eval_result = await gpt_3p5_judge.evaluate(", "            query=data_entry[\"question\"],", "            response=data_entry[\"answers\"][0][\"text\"],", "            second_response=data_entry[\"answers\"][1][\"text\"],", "            reference=data_entry[\"source\"],", "        )", "    except:", "        final_eval_result = EvaluationResult(", "            query=data_entry[\"question\"],", "            response=\"\",", "            passing=None,", "            score=0.5,", "            feedback=\"\",", "            pairwise_source=\"output-cannot-be-parsed\",", "        )", "", "    # 保存最终结果", "    judgement = {}", "    judgement[\"llm\"] = \"gpt_3p5\"", "    judgement[\"score\"] = final_eval_result.score", "    judgement[\"text\"] = final_eval_result.response", "    judgement[\"source\"] = final_eval_result.pairwise_source", "    data_entry[\"evaluations\"] += [judgement]"]}, {"cell_type": "markdown", "id": "35120c06-3218-4317-ad40-b94df1d6ca14", "metadata": {}, "source": ["### 指标\n", "\n", "哇！现在我们已经生成了LLM法官对测试查询中Llama-2/Mistral答案的所有评估。现在让我们从定量的角度来看一下fine-tuned GPT-3.5与GPT-4有多接近。\n", "\n", "为此，我们报告几个指标，即：\n", "- 与GPT-4评估的一致率\n", "- 与GPT-4评估的相关性\n", "- 与GPT-4评估的Jaccard相似度\n", "\n", "我们还报告了“不确定”的次数，这是指当LLM法官在呈现了Llama-2和Mistral答案的翻转顺序后改变了决定的情况。更高的不确定次数表明LLM法官容易受到位置偏见的影响，这是不好的！\n"]}, {"cell_type": "code", "execution_count": null, "id": "99bd3d10-087b-4055-9386-43dfbb3f968e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}], "source": ["!pip install scikit-learn -q"]}, {"cell_type": "code", "execution_count": null, "id": "63636af6-326c-41de-abdb-d54e7b59fb5a", "metadata": {}, "outputs": [], "source": ["import numpy as np", "", "# 存储每个LLM评委对每个样本的分数和不确定性布尔值", "scores = {\"gpt_4\": [], \"gpt_3p5\": [], \"ft_gpt_3p5\": []}", "inconclusives = {\"gpt_4\": [], \"gpt_3p5\": [], \"ft_gpt_3p5\": []}", "", "for ix, d in enumerate(test_dataset):", "    for e in d[\"evaluations\"]:", "        scores[e[\"llm\"]].append(e[\"score\"])", "        inconclusives[e[\"llm\"]].append(", "            e[\"source\"] not in [\"original\", \"flipped\"]", "        )"]}, {"cell_type": "code", "execution_count": null, "id": "91b9267a-d427-4959-9f99-0c7cdf31e6b3", "metadata": {}, "outputs": [], "source": ["REPORT_FMT_STR = (\n", "    \"{model}\\n\"\n", "    \"-----------------\\n\"\n", "    \"Number of inconclusives: {inconclusive}\\n\"\n", "    \"Number of agreements with GPT-4: {agreement} out of {total}\\n\"\n", "    \"Agreement rate: {agreement_rate}\\n\"\n", "    \"Correlation: {corr}\\n\"\n", "    \"Jaccard: {jacc}\\n\\n\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "b7e99836-adc6-46c5-a69d-2fef4bfde4f3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["GPT-3.5 w/ fine-tuning\n", "-----------------\n", "Number of inconclusives: 15\n", "Number of agreements with GPT-4: 41 out of 47\n", "Agreement rate: 0.8723404255319149\n", "Correlation: 0.765365523658036\n", "Jaccard: 0.773126734505088\n", "\n", "\n", "GPT-3.5 w/out fine-tuning\n", "-----------------\n", "Number of inconclusives: 24\n", "Number of agreements with GPT-4: 32 out of 38\n", "Agreement rate: 0.8421052631578947\n", "Correlation: 0.671929323262293\n", "Jaccard: 0.7308712958867757\n", "\n", "\n", "GPT-4\n", "-----------------\n", "Inconclusive Count: 4\n"]}], "source": ["from sklearn.metrics import jaccard_score", "", "# numpy转换", "np_scores_gpt_4 = np.array(scores[\"gpt_4\"])", "np_scores_gpt_3p5 = np.array(scores[\"gpt_3p5\"])", "np_scores_ft_gpt_3p5 = np.array(scores[\"ft_gpt_3p5\"])", "", "# 只有当两个评分者都有非不确定结果时才能进行比较", "ft_mask = ~np.array(inconclusives[\"gpt_4\"]) * ~np.array(", "    inconclusives[\"ft_gpt_3p5\"]", ")", "no_ft_mask = ~np.array(inconclusives[\"gpt_4\"]) * ~np.array(", "    inconclusives[\"gpt_3p5\"]", ")", "", "# 一致率", "agreement_ft = sum(np_scores_gpt_4[ft_mask] == np_scores_ft_gpt_3p5[ft_mask])", "agreement_rate_ft = agreement_ft / sum(ft_mask)", "agreement_no_ft = sum(", "    np_scores_gpt_4[no_ft_mask] == np_scores_gpt_3p5[no_ft_mask]", ")", "agreement_rate_no_ft = agreement_no_ft / sum(no_ft_mask)", "", "# 相关性", "corr_ft = np.corrcoef(np_scores_gpt_4[ft_mask], np_scores_ft_gpt_3p5[ft_mask])[", "    0, 1", "]", "corr_no_ft = np.corrcoef(", "    np_scores_gpt_4[no_ft_mask], np_scores_gpt_3p5[no_ft_mask]", ")[0, 1]", "", "# jaccard", "jaccard_ft = jaccard_score(", "    np_scores_gpt_4[ft_mask].astype(str),", "    np_scores_ft_gpt_3p5[ft_mask].astype(str),", "    average=\"weighted\",", ")", "jaccard_no_ft = jaccard_score(", "    np_scores_gpt_4[no_ft_mask].astype(str),", "    np_scores_gpt_3p5[no_ft_mask].astype(str),", "    average=\"weighted\",", ")", "", "print(", "    REPORT_FMT_STR.format(", "        model=\"GPT-3.5 w/ fine-tuning\",", "        inconclusive=sum(inconclusives[\"ft_gpt_3p5\"]),", "        agreement=agreement_ft,", "        total=sum(ft_mask),", "        agreement_rate=agreement_rate_ft,", "        corr=corr_ft,", "        jacc=jaccard_ft,", "    )", ")", "print(", "    REPORT_FMT_STR.format(", "        model=\"GPT-3.5 w/out fine-tuning\",", "        inconclusive=sum(inconclusives[\"gpt_3p5\"]),", "        agreement=agreement_no_ft,", "        total=sum(no_ft_mask),", "        agreement_rate=agreement_rate_no_ft,", "        corr=corr_no_ft,", "        jacc=jaccard_no_ft,", "    )", ")", "print(", "    f\"GPT-4\\n-----------------\\nInconclusive Count: {sum(inconclusives['gpt_4'])}\"", ")"]}, {"cell_type": "markdown", "id": "32dece60-e813-4002-986f-ad3f95f7aa7e", "metadata": {}, "source": ["## 结论\n", "\n", "从上面的数据中，我们可以看到，对GPT-3.5评判进行微调会产生比未经微调的GPT-3.5评判更高的一致性得分、相关性和Jaccard相似度。更重要的是，我们还看到经过微调后，无法得出结论的数量也有所减少。总的来说，我们看到在这里进行微调帮助我们获得了一个更接近GPT-4评判（因此间接地更接近人类判断）的GPT-3.5评判，同时也帮助纠正了未经微调的GPT-3.5可能存在的位置偏见。\n"]}], "metadata": {"kernelspec": {"display_name": "llama_index_3.10", "language": "python", "name": "llama_index_3.10"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}