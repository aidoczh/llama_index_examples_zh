{"cells": [{"cell_type": "markdown", "id": "917b5345-2958-444f-a81f-6c3ae328912e", "metadata": {}, "source": ["# 使用知识蒸馏对GPT-3.5评判器进行微调（正确性）\n", "\n", "这个笔记本涉及对LLM评判器进行微调，该评判器评估另一个LLM对用户查询的响应。更具体地，我们演示如何使用`llama_index`库从GPT-4评判器中提炼知识到GPT-3.5评判器。为此，我们将执行以下步骤：\n", "\n", "1. 生成数据集：`train`和`test`\n", "2. 执行知识蒸馏（使用`train`）\n", "3. 在`test`上评估提炼的模型\n", "\n", "更具体地，我们将使用`CorrectnessEvaluator`作为我们的LLM评判器。\n"]}, {"cell_type": "code", "execution_count": null, "id": "2acfbbe5", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-readers-wikipedia\n", "%pip install llama-index-finetuning\n", "%pip install llama-index-llms-openai\n", "%pip install llama-index-finetuning-callbacks\n", "%pip install llama-index-llms-huggingface"]}, {"cell_type": "code", "execution_count": null, "id": "8ecb4e18", "metadata": {}, "outputs": [], "source": ["# 注意：此笔记本进行了多次API调用，以使用OpenAI GPT模型和HuggingFace托管的模型生成文本。如果您不想等待这些生成过程，那么可以使用下面提供的`wget`命令获取本笔记本的数据。\n", "\n", "# !wget \"https://www.dropbox.com/scl/fo/3kkm8v6qvhxnu449xwp3d/h?rlkey=fxom1yixru1nags9mmao1hkg2&dl=1\" -O correctness.zip"]}, {"cell_type": "code", "execution_count": null, "id": "5b2f8b72-3337-4a2a-929c-87083bbf7fc2", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "id": "065f7df9-a048-43aa-b862-290e832ea631", "metadata": {}, "outputs": [], "source": ["\n", "# 我们将使用HuggingFace上的模型作为我们的LLM答案生成器\n", "HUGGING_FACE_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")\n", "\n", "# 我们将使用GPT-4和GPT-3.5 + OpenAI Fine-Tuning\n", "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"]}, {"cell_type": "markdown", "id": "2c89edf1-b359-4370-8b1e-fad279508c68", "metadata": {}, "source": ["## 步骤1 生成数据集：`train_dataset` 和 `test_dataset`\n", "\n", "对于我们将生成问题并提示各种LLM回答的数据集，我们将使用`WikipediaReader`来读取几个城市的“<city>的历史”。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6ebbecc4-543d-41d8-ae96-5f729e7e8a8f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n", "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"]}], "source": ["!pip install wikipedia -q"]}, {"cell_type": "code", "execution_count": null, "id": "ff858baa-5ae1-4dda-8407-6d23237b6b9c", "metadata": {}, "outputs": [], "source": ["# 维基百科页面\n", "from llama_index.readers.wikipedia import WikipediaReader\n", "\n", "城市 = [\n", "    \"旧金山\",\n", "    \"多伦多\",\n", "    \"纽约\",\n", "    \"温哥华\",\n", "    \"蒙特利尔\",\n", "    \"东京\",\n", "    \"新加坡\",\n", "    \"巴黎\",\n", "]\n", "\n", "文档 = WikipediaReader().load_data(\n", "    pages=[f\"{x}的历史\" for x in cities]\n", ")"]}, {"cell_type": "markdown", "id": "c66486ab-38cf-4ed6-bef4-6fe9deee0590", "metadata": {}, "source": ["### 使用`DatasetGenerator`构建`train_dataset`和`test_dataset`\n", "\n", "现在我们已经有了`Document`的训练集和测试集，下一步是生成问题。为此，我们将使用`DatasetGenerator`，它使用LLM从给定的文档集生成问题。\n"]}, {"cell_type": "markdown", "id": "cef531ed-8e97-4d8f-8cc3-6f7e0c6ca141", "metadata": {}, "source": ["在这个部分，我们将生成一些问题，以便进行后续的讨论和分析。\n"]}, {"cell_type": "code", "execution_count": null, "id": "99afd212-38b0-492c-91ea-a810e126ad2d", "metadata": {}, "outputs": [], "source": ["QUESTION_GEN_PROMPT = (\n", "    \"You are a Teacher/ Professor. Your task is to setup \"\n", "    \"a quiz/examination. Using the provided context, formulate \"\n", "    \"a single question that captures an important fact from the \"\n", "    \"context. Restrict the question to the context information provided.\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "e667f6ed-145b-41f3-92d2-be36aa1e47e6", "metadata": {}, "outputs": [], "source": ["# 根据块生成问题\n", "from llama_index.core.evaluation import DatasetGenerator\n", "from llama_index.llms.openai import OpenAI\n", "\n", "# 为llm提供上下文\n", "gpt_35_llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n", "\n", "# 实例化一个DatasetGenerator\n", "dataset_generator = DatasetGenerator.from_documents(\n", "    documents,\n", "    question_gen_query=QUESTION_GEN_PROMPT,\n", "    llm=gpt_35_llm,\n", "    num_questions_per_chunk=25,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "05ce06f6-1e85-4694-a4ac-9b4d465ed9e7", "metadata": {}, "outputs": [], "source": ["qrd = dataset_generator.generate_dataset_from_nodes(num=350)"]}, {"cell_type": "code", "execution_count": null, "id": "512e2f82-29b7-41cc-8f23-ae254f873031", "metadata": {}, "outputs": [], "source": ["# 如果您想将其保存以备将来使用\n", "# qrd.save_json(\"qrd.json\")"]}, {"cell_type": "markdown", "id": "b201d9cb-4746-4c71-8728-55e56cb8b76f", "metadata": {}, "source": ["#### 生成问题的答案\n", "\n", "接下来的步骤是使用LLM生成答案。请记住，重点是评估这些生成的答案。因此，我们稍后将使用GPT模型来评判这些答案。\n", "\n", "为了生成问题的答案，我们将使用另一个LLM，即Llama-2。为了做到这一点，我们首先需要为我们的文档创建一个向量存储和一个相关的检索器，这个LLM答案生成器将使用这些。\n"]}, {"cell_type": "code", "execution_count": null, "id": "60f08da0-27c0-4805-8c99-fad7041b3b16", "metadata": {}, "outputs": [], "source": ["# from llama_index.core import VectorStoreIndex\n", "from llama_index.core import VectorStoreIndex\n", "from llama_index.core.retrievers import VectorIndexRetriever\n", "\n", "# 创建向量索引\n", "the_index = VectorStoreIndex.from_documents(documents=documents)\n", "\n", "# 在此索引上创建检索器\n", "the_retriever = VectorIndexRetriever(\n", "    index=the_index,\n", "    similarity_top_k=2,\n", ")"]}, {"cell_type": "markdown", "id": "65125845-b96c-4f84-b45c-7ddaf6910d92", "metadata": {}, "source": ["从这里开始，我们将构建`RetrieverQueryEngine`，用于接收我们的查询（即问题）进行处理。请注意，我们使用`HuggingFaceInferenceAPI`来进行LLM答案生成器，而Llama-2需要权限。如果您还没有获得这些模型的访问权限，可以随意将Llama-2替换为您选择的其他模型。\n", "\n", "在这一点上，我们将生成的问题分成两组：一组用于构建`train_dataset`，另一组用于构建我们将在下一节中构建的`test_dataset`。\n"]}, {"cell_type": "code", "execution_count": null, "id": "eb1d6f7f-7067-48a8-ae8f-5619dc18d5d6", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/nerdai/Library/Caches/pypoetry/virtualenvs/llama-index-e6cjsBOJ-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n"]}], "source": ["from llama_index.core.query_engine import RetrieverQueryEngine\n", "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n", "\n", "llm = HuggingFaceInferenceAPI(\n", "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n", "    context_window=2048,  # 用于使用细化\n", "    token=HUGGING_FACE_TOKEN,\n", ")\n", "\n", "query_engine = RetrieverQueryEngine.from_args(retriever=the_retriever, llm=llm)"]}, {"cell_type": "code", "execution_count": null, "id": "cb345deb-d34e-4b48-9f72-ef108ad3afbc", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [08:30<00:00,  6.46s/it]\n"]}], "source": ["import tqdm\n", "\n", "# 我们将使用生成的问题的65%用于训练\n", "train_dataset = []\n", "num_train_questions = int(0.65 * len(qrd.qr_pairs))\n", "\n", "for q, a in tqdm.tqdm(qrd.qr_pairs[:num_train_questions]):\n", "    # 这个问题的数据\n", "    data_entry = {\"question\": q, \"reference\": a}\n", "    response = query_engine.query(q)\n", "    response_struct = {}\n", "    response_struct[\"model\"] = \"llama-2\"\n", "    response_struct[\"text\"] = str(response)\n", "    response_struct[\"context\"] = (\n", "        response.source_nodes[0].node.text[:1000] + \"...\"\n", "    )\n", "\n", "    data_entry[\"response_data\"] = response_struct\n", "    train_dataset.append(data_entry)"]}, {"cell_type": "markdown", "id": "6e653515-75c4-4987-87e4-c0a0b17a0bdf", "metadata": {}, "source": ["### 获取Mistral和LLama-2答案的GPT-4评估\n", "\n", "正如之前多次提到的，本指南的目的是从GPT-4法官中微调LLM法官。因此，为了完成我们的`train_dataset`，我们现在需要实例化我们的GPT-4法官，并让它评估LLama-2提供的答案。为此，我们将使用`CorrectnessEvaluator`类。然后，这个法官将比较答案和参考答案，并根据提供的答案与参考答案的接近程度在1到5之间（分数越高越好）提供评分。\n", "\n", "还要注意，我们使用`OpenAIFineTuningHandler`，它将收集我们最终需要微调GPT-3.5的所有聊天历史记录。\n"]}, {"cell_type": "code", "execution_count": null, "id": "17f8bbcf-6bc6-4a6c-a778-d43c6896fe89", "metadata": {}, "outputs": [], "source": ["# 实例化gpt-4评估器\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.finetuning.callbacks import OpenAIFineTuningHandler\n", "from llama_index.core.callbacks import CallbackManager\n", "from llama_index.core.evaluation import CorrectnessEvaluator\n", "\n", "finetuning_handler = OpenAIFineTuningHandler()\n", "callback_manager = CallbackManager([finetuning_handler])\n", "gpt_4_llm = OpenAI(\n", "    temperature=0, model=\"gpt-4\", callback_manager=callback_manager\n", ")\n", "\n", "gpt4_judge = CorrectnessEvaluator(llm=gpt_4_llm)"]}, {"cell_type": "code", "execution_count": null, "id": "9225358e-e898-4f32-a99b-47a67941f715", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [12:31<00:00,  9.51s/it]\n"]}], "source": ["import tqdm\n", "\n", "# 对于“训练”\n", "for data_entry in tqdm.tqdm(train_dataset):\n", "    eval_result = await gpt4_judge.aevaluate(\n", "        query=data_entry[\"question\"],\n", "        response=data_entry[\"response_data\"][\"text\"],\n", "        context=data_entry[\"response_data\"][\"context\"],\n", "        reference=data_entry[\"reference\"],\n", "    )\n", "\n", "    # 保存最终结果\n", "    judgement = {}\n", "    judgement[\"llm\"] = \"gpt_4\"\n", "    judgement[\"score\"] = eval_result.score\n", "    judgement[\"text\"] = eval_result.response\n", "    data_entry[\"evaluations\"] = [judgement]"]}, {"cell_type": "code", "execution_count": null, "id": "f56ed14f-2209-4fbb-86cf-2fa69aa1fdc1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Wrote 79 examples to correction_finetuning_events.jsonl\n"]}], "source": ["finetuning_handler.save_finetuning_events(\"correction_finetuning_events.jsonl\")"]}, {"cell_type": "markdown", "id": "11eb16d0-15b1-45f3-96b0-3b51574d1626", "metadata": {}, "source": ["## 第2步 执行知识蒸馏\n", "\n", "好的，现在是时候从GPT-4中提炼一些知识到GPT-3.5了。为了做到这一点，我们将利用`OpenAIFinetuneEngine`类以及我们刚刚创建的`correction_finetuning_events.jsonl`文件。\n"]}, {"cell_type": "code", "execution_count": null, "id": "bb90eb9e-aad3-4bce-a9b5-c2343c4e3d66", "metadata": {}, "outputs": [], "source": ["from llama_index.finetuning import OpenAIFinetuneEngine\n", "\n", "finetune_engine = OpenAIFinetuneEngine(\n", "    \"gpt-3.5-turbo\",\n", "    \"correction_finetuning_events.jsonl\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "32b6b392-8932-4eda-93eb-911513be3cfb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Num examples: 79\n", "First example:\n", "{'role': 'system', 'content': '\\nYou are an expert evaluation system for a question answering chatbot.\\n\\nYou are given the following information:\\n- a user query,\\n- a reference answer, and\\n- a generated answer.\\n\\nYour job is to judge the relevance and correctness of the generated answer.\\nOutput a single score that represents a holistic evaluation.\\nYou must return your response in a line with only the score.\\nDo not return answers in any other format.\\nOn a separate line provide your reasoning for the score as well.\\n\\nFollow these guidelines for scoring:\\n- Your score has to be between 1 and 5, where 1 is the worst and 5 is the best.\\n- If the generated answer is not relevant to the user query, you should give a score of 1.\\n- If the generated answer is relevant but contains mistakes, you should give a score between 2 and 3.\\n- If the generated answer is relevant and fully correct, you should give a score between 4 and 5.\\n\\nExample Response:\\n4.0\\nThe generated answer has the exact same metrics as the reference answer,     but it is not as concise.\\n\\n'}\n", "{'role': 'user', 'content': '\\n## User Query\\nWhat event in 1906 caused significant damage to San Francisco but was followed by a quick rebuild?\\n\\n## Reference Answer\\nThe great earthquake and fire in 1906 caused significant damage to San Francisco but was followed by a quick rebuild.\\n\\n## Generated Answer\\n1906 earthquake and fire.\\n'}\n", "{'role': 'assistant', 'content': '4.0\\nThe generated answer is relevant and correct, but it lacks the detail and context provided in the reference answer.'}\n", "No errors found\n", "Num examples missing system message: 0\n", "Num examples missing user message: 0\n", "\n", "#### Distribution of num_messages_per_example:\n", "min / max: 3, 3\n", "mean / median: 3.0, 3.0\n", "p5 / p95: 3.0, 3.0\n", "\n", "#### Distribution of num_total_tokens_per_example:\n", "min / max: 315, 782\n", "mean / median: 479.49367088607596, 465.0\n", "p5 / p95: 355.6, 634.6\n", "\n", "#### Distribution of num_assistant_tokens_per_example:\n", "min / max: 19, 110\n", "mean / median: 57.63291139240506, 56.0\n", "p5 / p95: 29.6, 83.2\n", "\n", "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n", "Dataset has ~37880 tokens that will be charged for during training\n", "By default, you'll train for 3 epochs on this dataset\n", "By default, you'll be charged for ~113640 tokens\n", "As of August 22, 2023, fine-tuning gpt-3.5-turbo is $0.008 / 1K Tokens.\n", "This means your total cost for training will be $0.30304000000000003 per epoch.\n"]}], "source": ["# 我们可以通过以下方式检查当前作业的状态\n", "# 这可能需要一些时间...\n", "finetune_engine.finetune()"]}, {"cell_type": "code", "execution_count": null, "id": "32d0234b-040d-44f6-901b-38e6eb241644", "metadata": {}, "outputs": [{"data": {"text/plain": ["<FineTuningJob fine_tuning.job id=ftjob-9y8G7rzbCkzPjsKtPMsfwRSu at 0x1778d6a70> JSON: {\n", "  \"object\": \"fine_tuning.job\",\n", "  \"id\": \"ftjob-9y8G7rzbCkzPjsKtPMsfwRSu\",\n", "  \"model\": \"gpt-3.5-turbo-0613\",\n", "  \"created_at\": 1698851177,\n", "  \"finished_at\": 1698851823,\n", "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:llamaindex::8G7FovVj\",\n", "  \"organization_id\": \"org-1ZDAvajC6v2ZtAP9hLEIsXRz\",\n", "  \"result_files\": [\n", "    \"file-bx2ObrpVPq7Q2pmv743W1eFQ\"\n", "  ],\n", "  \"status\": \"succeeded\",\n", "  \"validation_file\": null,\n", "  \"training_file\": \"file-xAwZ2NSzbck3p8u24kznzySX\",\n", "  \"hyperparameters\": {\n", "    \"n_epochs\": 3\n", "  },\n", "  \"trained_tokens\": 113166,\n", "  \"error\": null\n", "}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["finetune_engine.get_current_job()"]}, {"cell_type": "markdown", "id": "3e631ccf-b4f5-478d-b112-52dc88ffed1e", "metadata": {}, "source": ["## 3 在测试数据集上评估经过微调的GPT-3.5评判器\n", "\n", "现在我们已经有了经过微调的GPT-3.5，让我们看看它在测试集上的表现如何。但首先，请记住我们说过要推迟创建`test_dataset`直到我们需要它的时候？现在就是需要的时候了。因此，我们将重复在这里创建`train_dataset`的过程，但现在是为了`test_dataset`。\n", "\n", "注意：生成这些答案和评估需要一些时间。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3e6db08a-da63-4f0f-b148-16ed089ca585", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [05:07<00:00,  6.99s/it]\n"]}], "source": ["# 使用Llama-2生成测试问题的答案\n", "test_dataset = []\n", "for q, a in tqdm.tqdm(qrd.qr_pairs[num_train_questions:]):\n", "    # 问题的数据\n", "    data_entry = {\"question\": q, \"reference\": a}\n", "    response = query_engine.query(q)\n", "    response_struct = {}\n", "    response_struct[\"model\"] = \"llama-2\"\n", "    response_struct[\"text\"] = str(response)\n", "    response_struct[\"context\"] = (\n", "        response.source_nodes[0].node.text[:1000] + \"...\"\n", "    )\n", "\n", "    data_entry[\"response_data\"] = response_struct\n", "    test_dataset.append(data_entry)"]}, {"cell_type": "code", "execution_count": null, "id": "b70e535a-bd3a-4664-943a-1bf3ff02a8da", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [06:52<00:00,  9.37s/it]\n"]}], "source": ["# 获取Llama-2答案的GPT-4评估\n", "for data_entry in tqdm.tqdm(test_dataset):\n", "    eval_result = await gpt4_judge.aevaluate(\n", "        query=data_entry[\"question\"],\n", "        response=data_entry[\"response_data\"][\"text\"],\n", "        context=data_entry[\"response_data\"][\"context\"],\n", "        reference=data_entry[\"reference\"],\n", "    )\n", "\n", "    # 保存最终结果\n", "    judgement = {}\n", "    judgement[\"llm\"] = \"gpt_4\"\n", "    judgement[\"score\"] = eval_result.score\n", "    judgement[\"text\"] = eval_result.response\n", "    data_entry[\"evaluations\"] = [judgement]"]}, {"cell_type": "code", "execution_count": null, "id": "532d2661-3d06-42c6-9a99-86616ca4a31e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:44<00:00,  1.02s/it]\n"]}], "source": ["from llama_index.core.evaluation import EvaluationResult\n", "\n", "# 使用我们经过精调的GPT-3.5来评估答案\n", "ft_llm = finetune_engine.get_finetuned_model()\n", "\n", "ft_gpt_3p5_judge = CorrectnessEvaluator(llm=ft_llm)\n", "\n", "for data_entry in tqdm.tqdm(test_dataset):\n", "    eval_result = await ft_gpt_3p5_judge.evaluate(\n", "        query=data_entry[\"question\"],\n", "        response=data_entry[\"response_data\"][\"text\"],\n", "        context=data_entry[\"response_data\"][\"context\"],\n", "        reference=data_entry[\"reference\"],\n", "    )\n", "\n", "    # 保存最终结果\n", "    judgement = {}\n", "    judgement[\"llm\"] = \"ft_gpt_3p5\"\n", "    judgement[\"score\"] = eval_result.score\n", "    judgement[\"text\"] = eval_result.response\n", "    data_entry[\"evaluations\"] += [judgement]"]}, {"cell_type": "code", "execution_count": null, "id": "1ee6616b-ea0a-44c4-8749-68e64e0ebbee", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [01:36<00:00,  2.19s/it]\n"]}], "source": ["# 同样地，使用一个未经微调的评估器来评估答案\n", "gpt_3p5_llm = OpenAI(model=\"gpt-3.5-turbo\")\n", "\n", "gpt_3p5_judge = CorrectnessEvaluator(llm=gpt_3p5_llm)\n", "\n", "for data_entry in tqdm.tqdm(test_dataset):\n", "    eval_result = await gpt_3p5_judge.evaluate(\n", "        query=data_entry[\"question\"],\n", "        response=data_entry[\"response_data\"][\"text\"],\n", "        context=data_entry[\"response_data\"][\"context\"],\n", "        reference=data_entry[\"reference\"],\n", "    )\n", "\n", "    # 保存最终结果\n", "    judgement = {}\n", "    judgement[\"llm\"] = \"gpt_3p5\"\n", "    judgement[\"score\"] = eval_result.score\n", "    judgement[\"text\"] = eval_result.response\n", "    data_entry[\"evaluations\"] += [judgement]"]}, {"cell_type": "markdown", "id": "35120c06-3218-4317-ad40-b94df1d6ca14", "metadata": {}, "source": ["### 评估指标\n", "\n", "哇！现在我们已经生成了LLM法官对测试查询中Llama-2/Mistral答案的所有评估。现在让我们 quantitatively 观察一下，fine-tuned GPT-3.5 和 GPT-4 有多接近。\n", "\n", "为此，我们报告了fine-tuned（和未fine-tuned）GPT-3.5的分数与GPT-4法官之间的相关性。\n"]}, {"cell_type": "code", "execution_count": null, "id": "ded5f971-488a-469b-b240-bf2b15d530f2", "metadata": {}, "outputs": [], "source": ["REPORT_FMT_STR = (\n", "    \"{model}\\n\"\n", "    \"-----------------\\n\"\n", "    \"Number of obs.: {total_obs}\\n\"\n", "    \"Correlation with GPT-4: {corr}\\n\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "d612ee7b-f065-49fe-80ad-344fda9ff6b1", "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "scores = {\"gpt_4\": [], \"gpt_3p5\": [], \"ft_gpt_3p5\": []}\n", "for ix, d in enumerate(test_dataset):\n", "    for e in d[\"evaluations\"]:\n", "        scores[e[\"llm\"]].append(e[\"score\"])"]}, {"cell_type": "code", "execution_count": null, "id": "61637486-4df2-419f-9973-d449e18efcea", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["GPT-3.5 w/ fine-tuning\n", "-----------------\n", "Number of obs.: 44\n", "Correlation with GPT-4: 0.9279850303778618\n", "\n", "\n", "\n", "GPT-3.5 w/out fine-tuning\n", "-----------------\n", "Number of obs.: 44\n", "Correlation with GPT-4: 0.8737418723878325\n", "\n"]}], "source": ["# numpy转换\n", "np_scores_gpt_4 = np.array(scores[\"gpt_4\"])  # 将\"gpt_4\"的分数转换为numpy数组\n", "np_scores_gpt_3p5 = np.array(scores[\"gpt_3p5\"])  # 将\"gpt_3p5\"的分数转换为numpy数组\n", "np_scores_ft_gpt_3p5 = np.array(scores[\"ft_gpt_3p5\"])  # 将\"ft_gpt_3p5\"的分数转换为numpy数组\n", "\n", "# 相关性\n", "corr_ft = np.corrcoef(np_scores_gpt_4, np_scores_ft_gpt_3p5)[0, 1]  # 计算\"np_scores_gpt_4\"和\"np_scores_ft_gpt_3p5\"的相关性\n", "corr_no_ft = np.corrcoef(np_scores_gpt_4, np_scores_gpt_3p5)[0, 1]  # 计算\"np_scores_gpt_4\"和\"np_scores_gpt_3p5\"的相关性\n", "\n", "print(\n", "    REPORT_FMT_STR.format(\n", "        model=\"GPT-3.5 w/ fine-tuning\",\n", "        total_obs=np_scores_gpt_4.shape[0],\n", "        corr=corr_ft,\n", "    )\n", ")\n", "print(\"\\n\")\n", "print(\n", "    REPORT_FMT_STR.format(\n", "        model=\"GPT-3.5 w/out fine-tuning\",\n", "        total_obs=np_scores_gpt_4.shape[0],\n", "        corr=corr_no_ft,\n", "    )\n", ")"]}, {"cell_type": "markdown", "id": "a0dde4f3-7802-49bc-95d8-b2683879a441", "metadata": {}, "source": ["## 结论\n", "\n", "从以上数字可以看出，对GPT-3.5评判器进行微调可以使其与GPT-4的相关性更高，而非经过微调的评判器则不然。因此，在这种情况下，我们可以看到微调帮助我们获得了一个更接近GPT-4评判器（因此间接地更接近人类判断）的GPT-3.5评判器。\n"]}], "metadata": {"kernelspec": {"display_name": "llama_index_3.10", "language": "python", "name": "llama_index_3.10"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}