{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/cross_encoder_finetuning/cross_encoder_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 如何使用LLamaIndex对交叉编码器进行微调\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["如果您在Colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-finetuning-cross-encoders\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n", "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h"]}], "source": ["# 下载依赖库\n", "!pip install datasets --quiet\n", "!pip install sentence-transformers --quiet\n", "!pip install openai --quiet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 过程\n", "\n", "- 使用Datasets库从HuggingFace Hub下载QASPER数据集（https://huggingface.co/datasets/allenai/qasper）\n", "\n", "- 从数据集的训练集和测试集中分别提取800个和80个样本\n", "\n", "- 使用从训练数据中收集的800个样本，这些样本在研究论文上有相应的问题，以生成CrossEncoder微调所需格式的数据集。目前我们使用的格式是，微调数据的单个样本包括两个句子（问题和上下文）和一个分数，分数为0或1，其中1表示问题和上下文相关，0表示它们不相关。\n", "\n", "- 使用测试集的100个样本提取两种类型的评估数据集\n", "  * Rag评估数据集：一个数据集包含样本，其中单个样本包括研究论文内容、研究论文上的问题列表、研究论文上问题列表的答案。在形成此数据集时，我们仅保留具有长答案/自由形式答案的问题，以便与RAG生成的答案进行更好的比较。\n", "\n", "  * 重新排序评估数据集：另一个数据集包含样本，其中单个样本包括研究论文内容、研究论文上的问题列表、与每个问题相关的研究论文内容列表\n", "\n", "- 我们使用在llamaindex中编写的辅助工具微调交叉编码器，并使用huggingface cli tokens login将其推送到HuggingFace Hub，可以在此处找到：- https://huggingface.co/settings/tokens\n", "\n", "- 我们使用两种指标和三种情况对两个数据集进行评估\n", "     1. 仅使用OpenAI嵌入，没有任何重新排序器\n", "     2. 将OpenAI嵌入与cross-encoder/ms-marco-MiniLM-L-12-v2作为重新排序器结合使用\n", "     3. 将OpenAI嵌入与我们微调的交叉编码器模型作为重新排序器结合使用\n", "\n", "* 每个评估数据集的评估标准\n", "  - 命中指标：用于评估重新排序评估数据集，我们简单地使用LLamaIndex的检索器+后处理功能，以查看在不同情况下相关上下文被检索的次数，并将其称为命中指标。\n", "\n", "  - 两两比较评估器：我们使用LLamaIndex提供的两两比较评估器（https://github.com/run-llama/llama_index/blob/main/llama_index/evaluation/pairwise.py）来比较每种情况下创建的查询引擎的响应与提供的参考自由形式答案。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 加载数据集\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "from datasets import load_dataset\n", "import random\n", "\n", "# 从HuggingFace下载QASPER数据集 https://huggingface.co/datasets/allenai/qasper\n", "dataset = load_dataset(\"allenai/qasper\")\n", "\n", "# 将数据集分割为训练、验证和测试集\n", "train_dataset = dataset[\"train\"]\n", "validation_dataset = dataset[\"validation\"]\n", "test_dataset = dataset[\"test\"]\n", "\n", "random.seed(42)  # 设置随机种子以便重现结果\n", "\n", "# 从训练集中随机抽取800行数据\n", "train_sampled_indices = random.sample(range(len(train_dataset)), 800)\n", "train_samples = [train_dataset[i] for i in train_sampled_indices]\n", "\n", "# 从测试集中随机抽取100行数据\n", "test_sampled_indices = random.sample(range(len(test_dataset)), 80)\n", "test_samples = [test_dataset[i] for i in test_sampled_indices]\n", "\n", "# 现在我们有800篇研究论文用于训练，以及80篇研究论文用于评估。"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## QASPER数据集\n", "* 每一行都有以下6列\n", "    - id：研究论文的唯一标识符\n", "\n", "    - title：研究论文的标题\n", "\n", "    - abstract：研究论文的摘要\n", "\n", "    - full_text：研究论文的全文\n", "\n", "    - qas：与每篇研究论文相关的问题和答案\n", "\n", "    - figures_and_tables：每篇研究论文的图表\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 从QASPER的训练样本中获取完整的论文数据和论文上的问题，以生成用于交叉编码器微调的训练数据集\n", "from typing import List\n", "\n", "\n", "# 从数据集中获取研究论文的全文的实用函数\n", "def get_full_text(sample: dict) -> str:\n", "    \"\"\"\n", "    :param dict sample: QASPER中的行样本\n", "    \"\"\"\n", "    title = sample[\"title\"]\n", "    abstract = sample[\"abstract\"]\n", "    sections_list = sample[\"full_text\"][\"section_name\"]\n", "    paragraph_list = sample[\"full_text\"][\"paragraphs\"]\n", "    combined_sections_with_paras = \"\"\n", "    if len(sections_list) == len(paragraph_list):\n", "        combined_sections_with_paras += title + \"\\t\"\n", "        combined_sections_with_paras += abstract + \"\\t\"\n", "        for index in range(0, len(sections_list)):\n", "            combined_sections_with_paras += str(sections_list[index]) + \"\\t\"\n", "            combined_sections_with_paras += \"\".join(paragraph_list[index])\n", "        return combined_sections_with_paras\n", "\n", "    else:\n", "        print(\"不同数量的章节和段落列表\")\n", "\n", "\n", "# 从数据集中提取问题列表的实用函数\n", "def get_questions(sample: dict) -> List[str]:\n", "    \"\"\"\n", "    :param dict sample: QASPER中的行样本\n", "    \"\"\"\n", "    questions_list = sample[\"qas\"][\"question\"]\n", "    return questions_list\n", "\n", "\n", "doc_qa_dict_list = []\n", "\n", "for train_sample in train_samples:\n", "    full_text = get_full_text(train_sample)\n", "    questions_list = get_questions(train_sample)\n", "    local_dict = {\"paper\": full_text, \"questions\": questions_list}\n", "    doc_qa_dict_list.append(local_dict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["800"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["len(doc_qa_dict_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 将训练数据保存为csv格式\n", "import pandas as pd\n", "\n", "df_train = pd.DataFrame(doc_qa_dict_list)\n", "df_train.to_csv(\"train.csv\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 生成RAG评估测试数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 获取评估数据的论文、问题和答案\n", "\"\"\"\n", "数据集中的答案字段遵循以下格式:-\n", "无法回答的答案将\"unanswerable\"设置为true。\n", "\n", "其余的答案中，只有以下字段中的一个是非空的。\n", "\n", "\"extractive_spans\"是论文中作为答案的片段。\n", "\"free_form_answer\"是书面答案。\n", "\"yes_no\"为true，如果答案是Yes，为false，如果答案是No。\n", "\n", "我们只接受自由形式的答案，对于其他类型的答案，我们将它们的值设置为'Unacceptable'，\n", "以更好地评估使用成对比较评估器的查询引擎的性能，因为它使用偏向于更喜欢长答案的GPT-4。\n", "https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1\n", "\n", "因此，在'yes_no'答案的情况下，它可能更偏向于查询引擎的答案而不是参考答案。\n", "同样，在提取的片段的情况下，它可能更偏向于参考答案而不是查询引擎生成的答案。\n", "\n", "\"\"\"\n", "\n", "\n", "eval_doc_qa_answer_list = []\n", "\n", "\n", "# 从数据集中提取答案的实用函数\n", "def get_answers(sample: dict) -> List[str]:\n", "    \"\"\"\n", "    :param dict sample: QASPER训练集中的行样本\n", "    \"\"\"\n", "    final_answers_list = []\n", "    answers = sample[\"qas\"][\"answers\"]\n", "    for answer in answers:\n", "        local_answer = \"\"\n", "        types_of_answers = answer[\"answer\"][0]\n", "        if types_of_answers[\"unanswerable\"] == False:\n", "            if types_of_answers[\"free_form_answer\"] != \"\":\n", "                local_answer = types_of_answers[\"free_form_answer\"]\n", "            else:\n", "                local_answer = \"Unacceptable\"\n", "        else:\n", "            local_answer = \"Unacceptable\"\n", "\n", "        final_answers_list.append(local_answer)\n", "\n", "    return final_answers_list\n", "\n", "\n", "for test_sample in test_samples:\n", "    full_text = get_full_text(test_sample)\n", "    questions_list = get_questions(test_sample)\n", "    answers_list = get_answers(test_sample)\n", "    local_dict = {\n", "        \"paper\": full_text,\n", "        \"questions\": questions_list,\n", "        \"answers\": answers_list,\n", "    }\n", "    eval_doc_qa_answer_list.append(local_dict)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["80\n"]}], "source": ["len(eval_doc_qa_answer_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 将评估数据保存为csv\n", "import pandas as pd\n", "\n", "df_test = pd.DataFrame(eval_doc_qa_answer_list)\n", "df_test.to_csv(\"test.csv\")\n", "\n", "# Rag评估测试数据可以在下面的dropbox链接中找到\n", "# https://www.dropbox.com/scl/fi/3lmzn6714oy358mq0vawm/test.csv?rlkey=yz16080te4van7fvnksi9kaed&dl=0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 生成微调数据集\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 下载最新版本的llama-index\n", "!pip install llama-index --quiet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 从QASPER收集的初始训练数据中生成所需格式的相应训练数据集\n", "import os\n", "from llama_index.core import SimpleDirectoryReader\n", "import openai\n", "from llama_index.finetuning.cross_encoders.dataset_gen import (\n", "    generate_ce_fine_tuning_dataset,\n", "    generate_synthetic_queries_over_documents,\n", ")\n", "\n", "from llama_index.finetuning.cross_encoders import CrossEncoderFinetuneEngine\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import Document\n", "\n", "final_finetuning_data_list = []\n", "for paper in doc_qa_dict_list:\n", "    questions_list = paper[\"questions\"]\n", "    documents = [Document(text=paper[\"paper\"])]\n", "    local_finetuning_dataset = generate_ce_fine_tuning_dataset(\n", "        documents=documents,\n", "        questions_list=questions_list,\n", "        max_chunk_length=256,\n", "        top_k=5,\n", "    )\n", "    final_finetuning_data_list.extend(local_finetuning_dataset)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["11674"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["# 最终微调数据集中的样本总数\n", "len(final_finetuning_data_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 保存最终的微调数据集\n", "import pandas as pd\n", "\n", "df_finetuning_dataset = pd.DataFrame(final_finetuning_data_list)\n", "df_finetuning_dataset.to_csv(\"fine_tuning.csv\")\n", "\n", "# 可以在下面的dropbox链接中找到微调数据集:-\n", "# https://www.dropbox.com/scl/fi/zu6vtisp1j3wg2hbje5xv/fine_tuning.csv?rlkey=0jr6fud8sqk342agfjbzvwr9x&dl=0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 加载微调数据集\n", "\n", "微调数据集 = 最终微调数据列表"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["CrossEncoderFinetuningDatasetSample(query='Do they repot results only on English data?', context='addition to precision, recall, and F1 scores for both tasks, we show the average of the F1 scores across both tasks. On the ADE dataset, we achieve SOTA results for both the NER and RE tasks. On the CoNLL04 dataset, we achieve SOTA results on the NER task, while our performance on the RE task is competitive with other recent models. On both datasets, we achieve SOTA results when considering the average F1 score across both tasks. The largest gain relative to the previous SOTA performance is on the RE task of the ADE dataset, where we see an absolute improvement of 4.5 on the macro-average F1 score.While the model of Eberts and Ulges eberts2019span outperforms our proposed architecture on the CoNLL04 RE task, their results come at the cost of greater model complexity. As mentioned above, Eberts and Ulges fine-tune the BERTBASE model, which has 110 million trainable parameters. In contrast, given the hyperparameters used for final training on the CoNLL04 dataset, our proposed architecture has approximately 6 million trainable parameters.The fact that the optimal number of task-specific layers differed between the two datasets demonstrates the', score=0)"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["finetuning_dataset[0]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 生成重新排名评估测试数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 下载 RAG 评估测试数据\n", "!wget -O test.csv https://www.dropbox.com/scl/fi/3lmzn6714oy358mq0vawm/test.csv?rlkey=yz16080te4van7fvnksi9kaed&dl=0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Number of papers in the test sample:- 80\n"]}], "source": ["# 从评估数据生成重新排序评估数据集\n", "import pandas as pd\n", "import ast  # 用于安全地将字符串评估为列表\n", "\n", "# 加载评估数据\n", "df_test = pd.read_csv(\"/content/test.csv\", index_col=0)\n", "\n", "df_test[\"questions\"] = df_test[\"questions\"].apply(ast.literal_eval)\n", "df_test[\"answers\"] = df_test[\"answers\"].apply(ast.literal_eval)\n", "print(f\"测试样本中的论文数量：- {len(df_test)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import Document\n", "\n", "final_eval_data_list = []\n", "for index, row in df_test.iterrows():\n", "    documents = [Document(text=row[\"paper\"])]\n", "    query_list = row[\"questions\"]\n", "    local_eval_dataset = generate_ce_fine_tuning_dataset(\n", "        documents=documents,\n", "        questions_list=query_list,\n", "        max_chunk_length=256,\n", "        top_k=5,\n", "    )\n", "    relevant_query_list = []\n", "    relevant_context_list = []\n", "\n", "    for item in local_eval_dataset:\n", "        if item.score == 1:\n", "            relevant_query_list.append(item.query)\n", "            relevant_context_list.append(item.context)\n", "\n", "    if len(relevant_query_list) > 0:\n", "        final_eval_data_list.append(\n", "            {\n", "                \"paper\": row[\"paper\"],\n", "                \"questions\": relevant_query_list,\n", "                \"context\": relevant_context_list,\n", "            }\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["38"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["# 重新排序评估数据集的长度\n", "len(final_eval_data_list)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 保存重新排序评估数据集\n", "import pandas as pd\n", "\n", "df_finetuning_dataset = pd.DataFrame(final_eval_data_list)\n", "df_finetuning_dataset.to_csv(\"reranking_test.csv\")\n", "\n", "# 可以在下面的dropbox链接中找到重新排序数据集\n", "# https://www.dropbox.com/scl/fi/mruo5rm46k1acm1xnecev/reranking_test.csv?rlkey=hkniwowq0xrc3m0ywjhb2gf26&dl=0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 微调交叉编码器\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install huggingface_hub --quiet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1ed078bb7d4e49678ecfa42dc06a2398", "version_major": 2, "version_minor": 0}, "text/plain": ["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from huggingface_hub import notebook_login\n", "\n", "notebook_login()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "91f37d51eceb442885a371db97cf3381", "version_major": 2, "version_minor": 0}, "text/plain": ["Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c331e837ed604a9ba04acdd723e8ea89", "version_major": 2, "version_minor": 0}, "text/plain": ["Iteration:   0%|          | 0/1460 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "7f6f2a6f61ad48c1a97bd2a5eb0bc26f", "version_major": 2, "version_minor": 0}, "text/plain": ["Iteration:   0%|          | 0/1460 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from sentence_transformers import SentenceTransformer\n", "\n", "# 初始化交叉编码器微调引擎\n", "finetuning_engine = CrossEncoderFinetuneEngine(\n", "    dataset=finetuning_dataset, epochs=2, batch_size=8\n", ")\n", "\n", "# 对交叉编码器模型进行微调\n", "finetuning_engine.finetune()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "4d836ead969d49d2b35a56483bf09889", "version_major": 2, "version_minor": 0}, "text/plain": ["pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# 将模型推送到HuggingFace Hub\n", "finetuning_engine.push_to_hub(\n", "    repo_id=\"bpHigh/Cross-Encoder-LLamaIndex-Demo-v2\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 重新排名评估\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install nest-asyncio --quiet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 将其附加到相同的事件循环\n", "import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2023-10-12 04:47:18--  https://www.dropbox.com/scl/fi/mruo5rm46k1acm1xnecev/reranking_test.csv?rlkey=hkniwowq0xrc3m0ywjhb2gf26\n", "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6035:18::a27d:5512\n", "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n", "HTTP request sent, awaiting response... 302 Found\n", "Location: https://uc414efe80c7598407c86166866d.dl.dropboxusercontent.com/cd/0/inline/CFcxAwrNZkpcZLmEipK-DxnJF6BKMu8rKmoRp-FUoqRF83K1t0kG0OzBliY-8E7EmbRqkkRZENO4ayEUPgul8lzY7iyARc7kauQ4iHdGps9_Y4jHyuLstzxbVT1TDQyhotVUYWZ9uHNmDHI9UFWAKBVm/file# [following]\n", "--2023-10-12 04:47:18--  https://uc414efe80c7598407c86166866d.dl.dropboxusercontent.com/cd/0/inline/CFcxAwrNZkpcZLmEipK-DxnJF6BKMu8rKmoRp-FUoqRF83K1t0kG0OzBliY-8E7EmbRqkkRZENO4ayEUPgul8lzY7iyARc7kauQ4iHdGps9_Y4jHyuLstzxbVT1TDQyhotVUYWZ9uHNmDHI9UFWAKBVm/file\n", "Resolving uc414efe80c7598407c86166866d.dl.dropboxusercontent.com (uc414efe80c7598407c86166866d.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6035:15::a27d:550f\n", "Connecting to uc414efe80c7598407c86166866d.dl.dropboxusercontent.com (uc414efe80c7598407c86166866d.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 967072 (944K) [text/plain]\n", "Saving to: ‘reranking_test.csv’\n", "\n", "reranking_test.csv  100%[===================>] 944.41K  3.55MB/s    in 0.3s    \n", "\n", "2023-10-12 04:47:19 (3.55 MB/s) - ‘reranking_test.csv’ saved [967072/967072]\n", "\n"]}], "source": ["# 下载重新排名测试数据\n", "!wget -O reranking_test.csv https://www.dropbox.com/scl/fi/mruo5rm46k1acm1xnecev/reranking_test.csv?rlkey=hkniwowq0xrc3m0ywjhb2gf26&dl=0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Number of papers in the reranking eval dataset:- 38\n"]}], "source": ["# 加载重新排序数据集\n", "import pandas as pd\n", "import ast\n", "\n", "df_reranking = pd.read_csv(\"/content/reranking_test.csv\", index_col=0)\n", "df_reranking[\"questions\"] = df_reranking[\"questions\"].apply(ast.literal_eval)\n", "df_reranking[\"context\"] = df_reranking[\"context\"].apply(ast.literal_eval)\n", "print(f\"重新排序评估数据集中的论文数量：- {len(df_reranking)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["\n", "  <div id=\"df-e1282d93-cd7a-4536-a8a7-4f4ac8db179b\" class=\"colab-df-container\">\n", "    <div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>paper</th>\n", "      <th>questions</th>\n", "      <th>context</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Identifying Condition-Action Statements in Med...</td>\n", "      <td>[What supervised machine learning models do th...</td>\n", "      <td>[Identifying Condition-Action Statements in Me...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>\n", "    <div class=\"colab-df-buttons\">\n", "\n", "  <div class=\"colab-df-container\">\n", "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1282d93-cd7a-4536-a8a7-4f4ac8db179b')\"\n", "            title=\"Convert this dataframe to an interactive table.\"\n", "            style=\"display:none;\">\n", "\n", "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n", "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n", "  </svg>\n", "    </button>\n", "\n", "  <style>\n", "    .colab-df-container {\n", "      display:flex;\n", "      gap: 12px;\n", "    }\n", "\n", "    .colab-df-convert {\n", "      background-color: #E8F0FE;\n", "      border: none;\n", "      border-radius: 50%;\n", "      cursor: pointer;\n", "      display: none;\n", "      fill: #1967D2;\n", "      height: 32px;\n", "      padding: 0 0 0 0;\n", "      width: 32px;\n", "    }\n", "\n", "    .colab-df-convert:hover {\n", "      background-color: #E2EBFA;\n", "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "      fill: #174EA6;\n", "    }\n", "\n", "    .colab-df-buttons div {\n", "      margin-bottom: 4px;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert {\n", "      background-color: #3B4455;\n", "      fill: #D2E3FC;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert:hover {\n", "      background-color: #434B5C;\n", "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n", "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n", "      fill: #FFFFFF;\n", "    }\n", "  </style>\n", "\n", "    <script>\n", "      const buttonEl =\n", "        document.querySelector('#df-e1282d93-cd7a-4536-a8a7-4f4ac8db179b button.colab-df-convert');\n", "      buttonEl.style.display =\n", "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "\n", "      async function convertToInteractive(key) {\n", "        const element = document.querySelector('#df-e1282d93-cd7a-4536-a8a7-4f4ac8db179b');\n", "        const dataTable =\n", "          await google.colab.kernel.invokeFunction('convertToInteractive',\n", "                                                    [key], {});\n", "        if (!dataTable) return;\n", "\n", "        const docLinkHtml = 'Like what you see? Visit the ' +\n", "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n", "          + ' to learn more about interactive tables.';\n", "        element.innerHTML = '';\n", "        dataTable['output_type'] = 'display_data';\n", "        await google.colab.output.renderOutput(dataTable, element);\n", "        const docLink = document.createElement('div');\n", "        docLink.innerHTML = docLinkHtml;\n", "        element.appendChild(docLink);\n", "      }\n", "    </script>\n", "  </div>\n", "\n", "    </div>\n", "  </div>\n"], "text/plain": ["                                               paper  \\\n", "0  Identifying Condition-Action Statements in Med...   \n", "\n", "                                           questions  \\\n", "0  [What supervised machine learning models do th...   \n", "\n", "                                             context  \n", "0  [Identifying Condition-Action Statements in Me...  "]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["df_reranking.head(1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "34cf70bc3dbb48e2b1f1cf74836ec442", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)lve/main/config.json:   0%|          | 0.00/854 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "31993884af454bfa835dbaec8d0a0be1", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "bf39b437040f44af8bf41bf3d4a38a26", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)okenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "42fbc3d09200448f81651b5ddcd5e773", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "184ef6ea433747e9b2db933613db71ad", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "447733e06f7347a181624c40e859e46d", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# 评估是通过计算每个（问题，上下文）对的命中来进行的，\n", "# 我们使用问题检索前k个文档，\n", "# 如果结果包含上下文，则算作命中\n", "from llama_index.core.postprocessor import SentenceTransformerRerank\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Response\n", "from llama_index.core.retrievers import VectorIndexRetriever\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Document\n", "from llama_index.core import Settings\n", "\n", "import os\n", "import openai\n", "import pandas as pd\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n", "\n", "Settings.chunk_size = 256\n", "\n", "rerank_base = SentenceTransformerRerank(\n", "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_n=3\n", ")\n", "\n", "rerank_finetuned = SentenceTransformerRerank(\n", "    model=\"bpHigh/Cross-Encoder-LLamaIndex-Demo-v2\", top_n=3\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["without_reranker_hits = 0\n", "base_reranker_hits = 0\n", "finetuned_reranker_hits = 0\n", "total_number_of_context = 0\n", "for index, row in df_reranking.iterrows():\n", "    documents = [Document(text=row[\"paper\"])]\n", "    query_list = row[\"questions\"]\n", "    context_list = row[\"context\"]\n", "\n", "    assert len(query_list) == len(context_list)\n", "    vector_index = VectorStoreIndex.from_documents(documents)\n", "\n", "    retriever_without_reranker = vector_index.as_query_engine(\n", "        similarity_top_k=3, response_mode=\"no_text\"\n", "    )\n", "    retriever_with_base_reranker = vector_index.as_query_engine(\n", "        similarity_top_k=8,\n", "        response_mode=\"no_text\",\n", "        node_postprocessors=[rerank_base],\n", "    )\n", "    retriever_with_finetuned_reranker = vector_index.as_query_engine(\n", "        similarity_top_k=8,\n", "        response_mode=\"no_text\",\n", "        node_postprocessors=[rerank_finetuned],\n", "    )\n", "\n", "    for index in range(0, len(query_list)):\n", "        query = query_list[index]\n", "        context = context_list[index]\n", "        total_number_of_context += 1\n", "\n", "        response_without_reranker = retriever_without_reranker.query(query)\n", "        without_reranker_nodes = response_without_reranker.source_nodes\n", "\n", "        for node in without_reranker_nodes:\n", "            if context in node.node.text or node.node.text in context:\n", "                without_reranker_hits += 1\n", "\n", "        response_with_base_reranker = retriever_with_base_reranker.query(query)\n", "        with_base_reranker_nodes = response_with_base_reranker.source_nodes\n", "\n", "        for node in with_base_reranker_nodes:\n", "            if context in node.node.text or node.node.text in context:\n", "                base_reranker_hits += 1\n", "\n", "        response_with_finetuned_reranker = (\n", "            retriever_with_finetuned_reranker.query(query)\n", "        )\n", "        with_finetuned_reranker_nodes = (\n", "            response_with_finetuned_reranker.source_nodes\n", "        )\n", "\n", "        for node in with_finetuned_reranker_nodes:\n", "            if context in node.node.text or node.node.text in context:\n", "                finetuned_reranker_hits += 1\n", "\n", "        assert (\n", "            len(with_finetuned_reranker_nodes)\n", "            == len(with_base_reranker_nodes)\n", "            == len(without_reranker_nodes)\n", "            == 3\n", "        )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 结果\n", "\n", "如下所示，与其他选项相比，我们使用finetuned_cross_encoder获得了更多的点击次数。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>Metric</th>\n", "      <th>OpenAI_Embeddings</th>\n", "      <th>Base_cross_encoder</th>\n", "      <th>Finetuned_cross_encoder</th>\n", "      <th>Total Relevant Context</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Hits</td>\n", "      <td>30</td>\n", "      <td>34</td>\n", "      <td>37</td>\n", "      <td>85</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["  Metric  OpenAI_Embeddings  Base_cross_encoder  Finetuned_cross_encoder  \\\n", "0   Hits                 30                  34                       37   \n", "\n", "   Total Relevant Context  \n", "0                      85  "]}, "metadata": {}, "output_type": "display_data"}], "source": ["without_reranker_scores = [without_reranker_hits]\n", "base_reranker_scores = [base_reranker_hits]\n", "finetuned_reranker_scores = [finetuned_reranker_hits]\n", "reranker_eval_dict = {\n", "    \"Metric\": \"Hits\",\n", "    \"OpenAI_Embeddings\": without_reranker_scores,\n", "    \"Base_cross_encoder\": base_reranker_scores,\n", "    \"Finetuned_cross_encoder\": finetuned_reranker_hits,\n", "    \"Total Relevant Context\": total_number_of_context,\n", "}\n", "df_reranker_eval_results = pd.DataFrame(reranker_eval_dict)\n", "display(df_reranker_eval_results)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## RAG 评估\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2023-10-12 04:47:36--  https://www.dropbox.com/scl/fi/3lmzn6714oy358mq0vawm/test.csv?rlkey=yz16080te4van7fvnksi9kaed\n", "Resolving www.dropbox.com (www.dropbox.com)... 162.125.85.18, 2620:100:6035:18::a27d:5512\n", "Connecting to www.dropbox.com (www.dropbox.com)|162.125.85.18|:443... connected.\n", "HTTP request sent, awaiting response... 302 Found\n", "Location: https://ucb6087b1b853dad24e8201987fc.dl.dropboxusercontent.com/cd/0/inline/CFfI9UezsVwFpN4CHgYrSFveuNE01DfczDaeFGZO-Ud5VdDRff1LNG7hEhkBZwVljuRde-EZU336ASpnZs32qVePvpQEFnKB2SeplFpMt50G0m5IZepyV6pYPbNAhm0muYE_rjhlolHxRUQP_iaJBX9z/file# [following]\n", "--2023-10-12 04:47:38--  https://ucb6087b1b853dad24e8201987fc.dl.dropboxusercontent.com/cd/0/inline/CFfI9UezsVwFpN4CHgYrSFveuNE01DfczDaeFGZO-Ud5VdDRff1LNG7hEhkBZwVljuRde-EZU336ASpnZs32qVePvpQEFnKB2SeplFpMt50G0m5IZepyV6pYPbNAhm0muYE_rjhlolHxRUQP_iaJBX9z/file\n", "Resolving ucb6087b1b853dad24e8201987fc.dl.dropboxusercontent.com (ucb6087b1b853dad24e8201987fc.dl.dropboxusercontent.com)... 162.125.80.15, 2620:100:6035:15::a27d:550f\n", "Connecting to ucb6087b1b853dad24e8201987fc.dl.dropboxusercontent.com (ucb6087b1b853dad24e8201987fc.dl.dropboxusercontent.com)|162.125.80.15|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 1821706 (1.7M) [text/plain]\n", "Saving to: ‘test.csv’\n", "\n", "test.csv            100%[===================>]   1.74M  6.37MB/s    in 0.3s    \n", "\n", "2023-10-12 04:47:38 (6.37 MB/s) - ‘test.csv’ saved [1821706/1821706]\n", "\n"]}], "source": ["# 下载 RAG 评估测试数据\n", "!wget -O test.csv https://www.dropbox.com/scl/fi/3lmzn6714oy358mq0vawm/test.csv?rlkey=yz16080te4van7fvnksi9kaed&dl=0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Number of papers in the test sample:- 80\n"]}], "source": ["import pandas as pd\n", "import ast  # 用于安全地将字符串作为列表进行评估\n", "\n", "# 加载评估数据\n", "df_test = pd.read_csv(\"/content/test.csv\", index_col=0)\n", "\n", "df_test[\"questions\"] = df_test[\"questions\"].apply(ast.literal_eval)\n", "df_test[\"answers\"] = df_test[\"answers\"].apply(ast.literal_eval)\n", "print(f\"测试样本中的论文数量：- {len(df_test)}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["\n", "  <div id=\"df-8dd2786b-981d-4642-b009-9531bd14adde\" class=\"colab-df-container\">\n", "    <div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>paper</th>\n", "      <th>questions</th>\n", "      <th>answers</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Identifying Condition-Action Statements in Med...</td>\n", "      <td>[What supervised machine learning models do th...</td>\n", "      <td>[Unacceptable, Unacceptable, 1470 sentences, U...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>\n", "    <div class=\"colab-df-buttons\">\n", "\n", "  <div class=\"colab-df-container\">\n", "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dd2786b-981d-4642-b009-9531bd14adde')\"\n", "            title=\"Convert this dataframe to an interactive table.\"\n", "            style=\"display:none;\">\n", "\n", "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n", "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n", "  </svg>\n", "    </button>\n", "\n", "  <style>\n", "    .colab-df-container {\n", "      display:flex;\n", "      gap: 12px;\n", "    }\n", "\n", "    .colab-df-convert {\n", "      background-color: #E8F0FE;\n", "      border: none;\n", "      border-radius: 50%;\n", "      cursor: pointer;\n", "      display: none;\n", "      fill: #1967D2;\n", "      height: 32px;\n", "      padding: 0 0 0 0;\n", "      width: 32px;\n", "    }\n", "\n", "    .colab-df-convert:hover {\n", "      background-color: #E2EBFA;\n", "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "      fill: #174EA6;\n", "    }\n", "\n", "    .colab-df-buttons div {\n", "      margin-bottom: 4px;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert {\n", "      background-color: #3B4455;\n", "      fill: #D2E3FC;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert:hover {\n", "      background-color: #434B5C;\n", "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n", "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n", "      fill: #FFFFFF;\n", "    }\n", "  </style>\n", "\n", "    <script>\n", "      const buttonEl =\n", "        document.querySelector('#df-8dd2786b-981d-4642-b009-9531bd14adde button.colab-df-convert');\n", "      buttonEl.style.display =\n", "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "\n", "      async function convertToInteractive(key) {\n", "        const element = document.querySelector('#df-8dd2786b-981d-4642-b009-9531bd14adde');\n", "        const dataTable =\n", "          await google.colab.kernel.invokeFunction('convertToInteractive',\n", "                                                    [key], {});\n", "        if (!dataTable) return;\n", "\n", "        const docLinkHtml = 'Like what you see? Visit the ' +\n", "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n", "          + ' to learn more about interactive tables.';\n", "        element.innerHTML = '';\n", "        dataTable['output_type'] = 'display_data';\n", "        await google.colab.output.renderOutput(dataTable, element);\n", "        const docLink = document.createElement('div');\n", "        docLink.innerHTML = docLinkHtml;\n", "        element.appendChild(docLink);\n", "      }\n", "    </script>\n", "  </div>\n", "\n", "    </div>\n", "  </div>\n"], "text/plain": ["                                               paper  \\\n", "0  Identifying Condition-Action Statements in Med...   \n", "\n", "                                           questions  \\\n", "0  [What supervised machine learning models do th...   \n", "\n", "                                             answers  \n", "0  [Unacceptable, Unacceptable, 1470 sentences, U...  "]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["# 查看一个样本的评估数据，其中包含一个研究论文问题和相应的参考答案\n", "df_test.head(1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 基准评估\n", "\n", "仅使用OpenAI嵌入进行检索，没有任何重新排序器。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Eval 方法：\n", "1. 遍历测试数据集的每一行：\n", "    1. 对于当前正在迭代的行，使用数据集中的 paper 列中提供的论文文档创建一个向量索引\n", "    2. 使用 top_k 值为 3，在没有任何重新排序器的情况下查询向量索引\n", "    3. 使用成对比较评估器比较生成的答案与相应样本的参考答案，并将分数添加到列表中\n", "2. 重复步骤 1，直到遍历完所有行\n", "3. 计算所有样本/行的平均分数\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Response\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Document\n", "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n", "from llama_index.core.evaluation.eval_utils import (\n", "    get_responses,\n", "    get_results_df,\n", ")\n", "\n", "import os\n", "import openai\n", "import pandas as pd\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n", "\n", "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n", "\n", "evaluator_gpt4_pairwise = PairwiseComparisonEvaluator(llm=gpt4)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pairwise_scores_list = []\n", "\n", "no_reranker_dict_list = []\n", "\n", "\n", "# 遍历数据集的行\n", "for index, row in df_test.iterrows():\n", "    documents = [Document(text=row[\"paper\"])]\n", "    query_list = row[\"questions\"]\n", "    reference_answers_list = row[\"answers\"]\n", "    number_of_accepted_queries = 0\n", "    # 为当前迭代的行创建向量索引\n", "    vector_index = VectorStoreIndex.from_documents(documents)\n", "\n", "    # 使用 top_k 值为 3 的向量索引进行查询，不使用任何重新排序器\n", "    query_engine = vector_index.as_query_engine(similarity_top_k=3)\n", "\n", "    assert len(query_list) == len(reference_answers_list)\n", "    pairwise_local_score = 0\n", "\n", "    for index in range(0, len(query_list)):\n", "        query = query_list[index]\n", "        reference = reference_answers_list[index]\n", "\n", "        if reference != \"Unacceptable\":\n", "            number_of_accepted_queries += 1\n", "\n", "            response = str(query_engine.query(query))\n", "\n", "            no_reranker_dict = {\n", "                \"query\": query,\n", "                \"response\": response,\n", "                \"reference\": reference,\n", "            }\n", "            no_reranker_dict_list.append(no_reranker_dict)\n", "\n", "            # 使用两两比较评估器比较生成的答案与相应样本的参考答案，并将分数添加到列表中\n", "\n", "            pairwise_eval_result = await evaluator_gpt4_pairwise.aevaluate(\n", "                query, response=response, reference=reference\n", "            )\n", "\n", "            pairwise_score = pairwise_eval_result.score\n", "\n", "            pairwise_local_score += pairwise_score\n", "\n", "        else:\n", "            pass\n", "\n", "    if number_of_accepted_queries > 0:\n", "        avg_pairwise_local_score = (\n", "            pairwise_local_score / number_of_accepted_queries\n", "        )\n", "        pairwise_scores_list.append(avg_pairwise_local_score)\n", "\n", "\n", "overal_pairwise_average_score = sum(pairwise_scores_list) / len(\n", "    pairwise_scores_list\n", ")\n", "\n", "df_responses = pd.DataFrame(no_reranker_dict_list)\n", "df_responses.to_csv(\"No_Reranker_Responses.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>name</th>\n", "      <th>pairwise score</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Without Reranker</td>\n", "      <td>0.553788</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["               name  pairwise score\n", "0  Without Reranker        0.553788"]}, "metadata": {}, "output_type": "display_data"}], "source": ["results_dict = {\n", "    \"name\": [\"Without Reranker\"],\n", "    \"pairwise score\": [overal_pairwise_average_score],\n", "}\n", "results_df = pd.DataFrame(results_dict)\n", "display(results_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 使用基本重新排序器进行评估\n", "\n", "OpenAI Embeddings + `cross-encoder/ms-marco-MiniLM-L-12-v2` 作为重新排序器\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Eval 方法:-\n", "1. 遍历测试数据集的每一行:-\n", "    1. 对于当前正在迭代的行，使用数据集中的paper列中提供的论文文档创建一个向量索引\n", "    2. 使用top_k值为5查询向量索引。\n", "    3. 使用cross-encoder/ms-marco-MiniLM-L-12-v2作为重新排序器，作为NodePostprocessor获取8个节点中的前3个节点的top_k值\n", "    4. 使用Pairwise Comparison Evaluator比较生成的答案与相应样本的参考答案，并将分数添加到列表中\n", "5. 重复步骤1，直到遍历完所有行\n", "6. 计算所有样本/行的平均分数\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "4cd72d8ca9ab45548335b59e673c1ab6", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)lve/main/config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "43fcf99b246e45dcab91746fdad3eb43", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "01f043960f8248b48f7db3dfe765bf7b", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)okenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1c27a842b2964548898ca3f1152756b4", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "8ae5a97aea424b4a93d70b7a1e75c7f3", "version_major": 2, "version_minor": 0}, "text/plain": ["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from llama_index.core.postprocessor import SentenceTransformerRerank\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Response\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Document\n", "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n", "import os\n", "import openai\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n", "\n", "rerank = SentenceTransformerRerank(\n", "    model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", top_n=3\n", ")\n", "\n", "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n", "\n", "evaluator_gpt4_pairwise = PairwiseComparisonEvaluator(llm=gpt4)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pairwise_scores_list = []\n", "\n", "base_reranker_dict_list = []\n", "\n", "\n", "# 遍历数据集的行\n", "for index, row in df_test.iterrows():\n", "    documents = [Document(text=row[\"paper\"])]\n", "    query_list = row[\"questions\"]\n", "    reference_answers_list = row[\"answers\"]\n", "\n", "    number_of_accepted_queries = 0\n", "    # 为当前迭代的行创建向量索引\n", "    vector_index = VectorStoreIndex.from_documents(documents)\n", "\n", "    # 使用reranker作为cross-encoder/ms-marco-MiniLM-L-12-v2，查询具有top_k值为8的节点的向量索引\n", "    query_engine = vector_index.as_query_engine(\n", "        similarity_top_k=8, node_postprocessors=[rerank]\n", "    )\n", "\n", "    assert len(query_list) == len(reference_answers_list)\n", "    pairwise_local_score = 0\n", "\n", "    for index in range(0, len(query_list)):\n", "        query = query_list[index]\n", "        reference = reference_answers_list[index]\n", "\n", "        if reference != \"Unacceptable\":\n", "            number_of_accepted_queries += 1\n", "\n", "            response = str(query_engine.query(query))\n", "\n", "            base_reranker_dict = {\n", "                \"query\": query,\n", "                \"response\": response,\n", "                \"reference\": reference,\n", "            }\n", "            base_reranker_dict_list.append(base_reranker_dict)\n", "\n", "            # 使用Pairwise Comparison Evaluator比较生成的答案与相应样本的参考答案，并将分数添加到列表中\n", "\n", "            pairwise_eval_result = await evaluator_gpt4_pairwise.aevaluate(\n", "                query=query, response=response, reference=reference\n", "            )\n", "\n", "            pairwise_score = pairwise_eval_result.score\n", "\n", "            pairwise_local_score += pairwise_score\n", "\n", "        else:\n", "            pass\n", "\n", "    if number_of_accepted_queries > 0:\n", "        avg_pairwise_local_score = (\n", "            pairwise_local_score / number_of_accepted_queries\n", "        )\n", "        pairwise_scores_list.append(avg_pairwise_local_score)\n", "\n", "overal_pairwise_average_score = sum(pairwise_scores_list) / len(\n", "    pairwise_scores_list\n", ")\n", "\n", "df_responses = pd.DataFrame(base_reranker_dict_list)\n", "df_responses.to_csv(\"Base_Reranker_Responses.csv\")\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>name</th>\n", "      <th>pairwise score</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>With base cross-encoder/ms-marco-MiniLM-L-12-v...</td>\n", "      <td>0.556818</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                                                name  pairwise score\n", "0  With base cross-encoder/ms-marco-MiniLM-L-12-v...        0.556818"]}, "metadata": {}, "output_type": "display_data"}], "source": ["results_dict = {\n", "    \"name\": [\"With base cross-encoder/ms-marco-MiniLM-L-12-v2 as Reranker\"],\n", "    \"pairwise score\": [overal_pairwise_average_score],\n", "}\n", "results_df = pd.DataFrame(results_dict)\n", "display(results_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 使用经过微调的重新排序器进行评估\n", "\n", "OpenAI Embeddings + `bpHigh/Cross-Encoder-LLamaIndex-Demo-v2` 作为重新排序器\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Eval 方法:-\n", "1. 遍历测试数据集的每一行:-\n", "    1. 对于当前正在迭代的行，使用数据集中的 paper 列中提供的论文文档创建一个向量索引\n", "    2. 使用 top_k 值为 5 查询向量索引。\n", "    3. 使用经过微调的 cross-encoder/ms-marco-MiniLM-L-12-v2，保存为 bpHigh/Cross-Encoder-LLamaIndex-Demo 作为一个重新排序器，作为 NodePostprocessor 从 8 个节点中获取 top 3 个节点的 top_k 值\n", "    4. 使用成对比较评估器将生成的答案与各个样本的参考答案进行比较，并将分数添加到列表中\n", "5. 重复步骤 1，直到遍历完所有行\n", "6. 计算所有样本/行的平均分数\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.postprocessor import SentenceTransformerRerank\n", "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Response\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Document\n", "from llama_index.core.evaluation import PairwiseComparisonEvaluator\n", "import os\n", "import openai\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n", "\n", "rerank = SentenceTransformerRerank(\n", "    model=\"bpHigh/Cross-Encoder-LLamaIndex-Demo-v2\", top_n=3\n", ")\n", "\n", "\n", "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n", "\n", "evaluator_gpt4_pairwise = PairwiseComparisonEvaluator(llm=gpt4)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pairwise_scores_list = []\n", "\n", "finetuned_reranker_dict_list = []\n", "\n", "# 遍历数据集的行\n", "for index, row in df_test.iterrows():\n", "    documents = [Document(text=row[\"paper\"])]\n", "    query_list = row[\"questions\"]\n", "    reference_answers_list = row[\"answers\"]\n", "\n", "    number_of_accepted_queries = 0\n", "    # 为当前迭代的行创建向量索引\n", "    vector_index = VectorStoreIndex.from_documents(documents)\n", "\n", "    # 使用reranker作为cross-encoder/ms-marco-MiniLM-L-12-v2，查询具有top_k值为8的节点的向量索引\n", "    query_engine = vector_index.as_query_engine(\n", "        similarity_top_k=8, node_postprocessors=[rerank]\n", "    )\n", "\n", "    assert len(query_list) == len(reference_answers_list)\n", "    pairwise_local_score = 0\n", "\n", "    for index in range(0, len(query_list)):\n", "        query = query_list[index]\n", "        reference = reference_answers_list[index]\n", "\n", "        if reference != \"Unacceptable\":\n", "            number_of_accepted_queries += 1\n", "\n", "            response = str(query_engine.query(query))\n", "\n", "            finetuned_reranker_dict = {\n", "                \"query\": query,\n", "                \"response\": response,\n", "                \"reference\": reference,\n", "            }\n", "            finetuned_reranker_dict_list.append(finetuned_reranker_dict)\n", "\n", "            # 使用Pairwise Comparison Evaluator比较生成的答案与相应样本的参考答案，并将得分添加到列表中\n", "            pairwise_eval_result = await evaluator_gpt4_pairwise.aevaluate(\n", "                query, response=response, reference=reference\n", "            )\n", "\n", "            pairwise_score = pairwise_eval_result.score\n", "\n", "            pairwise_local_score += pairwise_score\n", "\n", "        else:\n", "            pass\n", "\n", "    if number_of_accepted_queries > 0:\n", "        avg_pairwise_local_score = (\n", "            pairwise_local_score / number_of_accepted_queries\n", "        )\n", "        pairwise_scores_list.append(avg_pairwise_local_score)\n", "\n", "overal_pairwise_average_score = sum(pairwise_scores_list) / len(\n", "    pairwise_scores_list\n", ")\n", "df_responses = pd.DataFrame(finetuned_reranker_dict_list)\n", "df_responses.to_csv(\"Finetuned_Reranker_Responses.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>name</th>\n", "      <th>pairwise score</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>With fine-tuned cross-encoder/ms-marco-MiniLM-...</td>\n", "      <td>0.6</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                                                name  pairwise score\n", "0  With fine-tuned cross-encoder/ms-marco-MiniLM-...             0.6"]}, "metadata": {}, "output_type": "display_data"}], "source": ["results_dict = {\n", "    \"name\": [\"With fine-tuned cross-encoder/ms-marco-MiniLM-L-12-v2\"],\n", "    \"pairwise score\": [overal_pairwise_average_score],\n", "}\n", "results_df = pd.DataFrame(results_dict)\n", "display(results_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 结果\n", "\n", "正如我们所看到的，我们使用微调的交叉编码器获得了最高的成对分数。\n", "\n", "虽然我想指出，基于命中的重新排序评估是一种比成对比较评估更健壮的指标，因为我已经看到分数存在不一致性，并且在使用GPT-4进行评估时也存在许多固有偏见。\n"]}], "metadata": {"colab": {"provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}