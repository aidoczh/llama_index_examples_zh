{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/finetuning/gradient/gradient_structured.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 使用Gradient和LlamaIndex对Llama2进行微调以获得更好的结构化输出\n", "\n", "在这个笔记本中，我们将向您展示如何微调llama2-7b以更好地输出结构化结果。\n", "\n", "我们将使用[gradient.ai](https://gradient.ai)来实现这一目标。\n", "\n", "这与我们的[OpenAI函数微调笔记本](https://docs.llamaindex.ai/en/latest/examples/finetuning/openai_fine_tuning_functions.html)的格式类似。\n", "\n", "**注意**：这是我们关于使用Modal对llama2-7b进行微调的另一种选择的仓库/指南：https://github.com/run-llama/modal_finetune_sql\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-gradient\n", "%pip install llama-index-llms-openai\n", "%pip install llama-index-readers-file pymupdf\n", "%pip install llama-index-finetuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index gradientai -q"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from llama_index.llms.gradient import GradientBaseModelLLM\n", "from llama_index.finetuning import GradientFinetuneEngine"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["os.environ[\"GRADIENT_ACCESS_TOKEN\"] = os.getenv(\"GRADIENT_API_KEY\")\n", "os.environ[\"GRADIENT_WORKSPACE_ID\"] = \"<insert_workspace_id>\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 使用GPT-4 Pydantic程序进行微调\n", "\n", "在本节中，我们将展示如何通过我们的低级Pydantic程序模块记录输入 + GPT-4生成的输出。我们将使用该数据集对llama2进行微调。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pydantic import BaseModel\n", "\n", "\n", "class Album(BaseModel):\n", "    \"\"\"专辑的数据模型。\"\"\"\n", "\n", "    name: str\n", "    artist: str"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.llms.gradient import GradientBaseModelLLM\n", "from llama_index.core.program import LLMTextCompletionProgram\n", "from llama_index.core.output_parsers import PydanticOutputParser\n", "\n", "openai_handler = LlamaDebugHandler()\n", "openai_callback = CallbackManager([openai_handler])\n", "openai_llm = OpenAI(model=\"gpt-4\", callback_manager=openai_callback)\n", "\n", "gradient_handler = LlamaDebugHandler()\n", "gradient_callback = CallbackManager([gradient_handler])\n", "base_model_slug = \"llama2-7b-chat\"\n", "gradient_llm = GradientBaseModelLLM(\n", "    base_model_slug=base_model_slug,\n", "    max_tokens=300,\n", "    callback_manager=gradient_callback,\n", "    is_chat_model=True,\n", ")\n", "# HACK: set chat model\n", "from llama_index.core.llms import LLMMetadata\n", "\n", "# gradient_llm.metadata = LLMMetadata(\n", "#     context_window=1024,\n", "#     num_output=gradient_llm.max_tokens or 20,\n", "#     is_chat_model=True,\n", "#     is_function_calling_model=False,\n", "#     model_name=gradient_llm._model.id,\n", "# )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 尝试通过LLMTextCompletionProgram运行两者\n", "\n", "prompt_template_str = \"\"\"\\\n", "生成一个示例专辑，包括一个艺术家和一组歌曲。\\\n", "以电影 {movie_name} 为灵感。\\\n", "\"\"\"\n", "openai_program = LLMTextCompletionProgram.from_defaults(\n", "    output_parser=PydanticOutputParser(Album),\n", "    prompt_template_str=prompt_template_str,\n", "    llm=openai_llm,\n", "    verbose=True,\n", ")\n", "gradient_program = LLMTextCompletionProgram.from_defaults(\n", "    output_parser=PydanticOutputParser(Album),\n", "    prompt_template_str=prompt_template_str,\n", "    llm=gradient_llm,\n", "    verbose=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response = openai_program(movie_name=\"The Shining\")\n", "print(str(response))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tmp = openai_handler.get_llm_inputs_outputs()\n", "print(tmp[0][0].payload[\"messages\"][0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 打印tmp[0][1] "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response = gradient_program(movie_name=\"The Shining\")\n", "print(str(response))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tmp = gradient_handler.get_llm_inputs_outputs()\n", "print(tmp[0][0].payload[\"messages\"][0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 定义 Pydantic 模型 + 程序\n", "\n", "在这里，我们定义了由 GPT-4 提供支持的函数调用程序，该程序将生成结构化输出到一个 Pydantic 对象（相册）中。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.program import LLMTextCompletionProgram\n", "from pydantic import BaseModel\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core.callbacks import GradientAIFineTuningHandler\n", "from llama_index.core.callbacks import CallbackManager\n", "from llama_index.core.output_parsers import PydanticOutputParser\n", "from typing import List\n", "\n", "\n", "class Song(BaseModel):\n", "    \"\"\"歌曲的数据模型。\"\"\"\n", "\n", "    title: str\n", "    length_seconds: int\n", "\n", "\n", "class Album(BaseModel):\n", "    \"\"\"专辑的数据模型。\"\"\"\n", "\n", "    name: str\n", "    artist: str\n", "    songs: List[Song]\n", "\n", "\n", "finetuning_handler = GradientAIFineTuningHandler()\n", "callback_manager = CallbackManager([finetuning_handler])\n", "\n", "llm_gpt4 = OpenAI(model=\"gpt-4\", callback_manager=callback_manager)\n", "\n", "\n", "prompt_template_str = \"\"\"\\\n", "生成一个示例专辑，包括一个艺术家和一组歌曲。\\\n", "以电影 {movie_name} 为灵感。\\\n", "\"\"\"\n", "openai_program = LLMTextCompletionProgram.from_defaults(\n", "    output_parser=PydanticOutputParser(Album),\n", "    prompt_template_str=prompt_template_str,\n", "    llm=llm_gpt4,\n", "    verbose=True,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 记录输入/输出\n", "\n", "我们定义一些样本电影名称作为输入，并通过函数调用程序记录输出。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 注意：我们需要至少10部电影来使用梯度微调\n", "电影名称 = [\n", "    \"闪灵\",\n", "    \"无间道\",\n", "    \"泰坦尼克号\",\n", "    \"盗亦有道\",\n", "    \"风月\",\n", "    \"小鬼当家\",\n", "    \"铁笼狂怒\",\n", "    \"剪刀手爱德华\",\n", "    \"全面回忆\",\n", "    \"幽灵\",\n", "    \"震撼\",\n", "    \"机器战警\",\n", "    \"洛基5\",\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tqdm.notebook import tqdm\n", "\n", "for movie_name in tqdm(movie_names):\n", "    output = openai_program(movie_name=movie_name)\n", "    print(output.json())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["events = finetuning_handler.get_finetuning_events()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["events"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Wrote 14 examples to mock_finetune_songs.jsonl\n"]}], "source": ["finetuning_handler.save_finetuning_events(\"mock_finetune_songs.jsonl\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!cat mock_finetune_songs.jsonl"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 在数据集上进行微调\n", "\n", "现在我们定义一个微调引擎，并在模拟数据集上进行微调。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 定义基础模型\n", "base_model_slug = \"llama2-7b-chat\"\n", "base_llm = GradientBaseModelLLM(\n", "    base_model_slug=base_model_slug, max_tokens=500, is_chat_model=True\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.finetuning import GradientFinetuneEngine\n", "\n", "finetune_engine = GradientFinetuneEngine(\n", "    base_model_slug=base_model_slug,\n", "    # model_adapter_id='805c6fd6-daa8-4fc8-a509-bebb2f2c1024_model_adapter',\n", "    name=\"movies_structured\",\n", "    data_path=\"mock_finetune_songs.jsonl\",\n", "    verbose=True,\n", "    max_steps=200,\n", "    batch_size=1,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["'1f810f84-c4b8-43b0-b6b0-10d2cbdaf92f_model_adapter'"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["finetune_engine.model_adapter_id"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 根据需要调整epochs\n", "epochs = 2\n", "for i in range(epochs):\n", "    print(f\"** EPOCH {i} **\")\n", "    finetune_engine.finetune()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ft_llm = finetune_engine.get_finetuned_model(\n", "    max_tokens=500, is_chat_model=True\n", ")\n", "\n", "# # 注意：与执行以下操作相同\n", "from llama_index.llms.gradient import GradientModelAdapterLLM\n", "\n", "# ft_llm = GradientModelAdapterLLM(\n", "#     model_adapter_id=finetune_engine.model_adapter_id,\n", "#     max_tokens=500\n", "# )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 试一试！\n", "\n", "我们获得了经过微调的LLM，并将其与Pydantic程序一起使用。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 尝试稍微修改prompt_template_str\n", "new_prompt_template_str = \"\"\"\\\n", "生成一个示例专辑，包括一个艺术家和一组歌曲。\\\n", "以电影 {movie_name} 为灵感。\\\n", "请只生成一个专辑。\n", "\"\"\"\n", "\n", "gradient_program = LLMTextCompletionProgram.from_defaults(\n", "    output_parser=PydanticOutputParser(Album),\n", "    # prompt_template_str=prompt_template_str,\n", "    prompt_template_str=new_prompt_template_str,\n", "    llm=ft_llm,\n", "    verbose=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["Album(name='Wiseguy Melodies', artist='Tommy DeVito & The Gangsters', songs=[Song(title='Life in the Fast Lane', length_seconds=210), Song(title='Money and Power', length_seconds=240), Song(title='Goodfellas', length_seconds=270), Song(title='Betrayal', length_seconds=200), Song(title='Downfall', length_seconds=180)])"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["gradient_program(movie_name=\"Goodfellas\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gradient_program(movie_name=\"Chucky\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 你无法通过普通的llama2-7b获得这个！\n", "base_gradient_program = LLMTextCompletionProgram.from_defaults(\n", "    output_parser=PydanticOutputParser(Album),\n", "    prompt_template_str=prompt_template_str,\n", "    llm=base_llm,\n", "    verbose=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 抛出一个错误\n", "base_gradient_program(movie_name=\"Goodfellas\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 通过RAG系统对结构化输出进行微调\n", "\n", "函数调用的一个用例是通过RAG系统获取结构化输出。\n", "\n", "在这里，我们展示如何创建一个训练数据集，其中包括上下文增强的输入和未结构化文档上的结构化输出。然后，我们可以对LLM进行微调，并将其插入到RAG系统中，以执行检索和输出提取。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir data && wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from pydantic import Field\n", "from typing import List\n", "\n", "\n", "class Citation(BaseModel):\n", "    \"\"\"引文类。\"\"\"\n", "\n", "    author: str = Field(\n", "        ..., description=\"推断的第一作者（通常是姓氏）\"\n", "    )\n", "    year: int = Field(..., description=\"推断的年份\")\n", "    desc: str = Field(\n", "        ...,\n", "        description=(\n", "            \"从作者被引用的作品文本中推断的描述\"\n", "        ),\n", "    )\n", "\n", "\n", "class Response(BaseModel):\n", "    \"\"\"作者引文列表。\n", "\n", "    从非结构化文本中提取。\n", "\n", "    \"\"\"\n", "\n", "    citations: List[Citation] = Field(\n", "        ...,\n", "        description=(\n", "            \"作者引文列表（按作者、年份和描述组织）。\"\n", "        ),\n", "    )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 读取数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.readers.file import PyMuPDFReader\n", "from llama_index.core import Document\n", "from llama_index.core.node_parser import SimpleNodeParser\n", "from pathlib import Path\n", "from llama_index.core.callbacks import GradientAIFineTuningHandler"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["loader = PyMuPDFReader()\n", "docs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n", "metadata = {\n", "    \"paper_title\": \"Llama 2: Open Foundation and Fine-Tuned Chat Models\"\n", "}\n", "docs = [Document(text=doc_text, metadata=metadata)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chunk_size = 1024\n", "node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size)\n", "nodes = node_parser.get_nodes_from_documents(docs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["89"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["len(nodes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 设置 GPT-4 上下文 - 生成给定查询的“ground-truth”数据\n", "finetuning_handler = GradientAIFineTuningHandler()  # 微调处理程序\n", "callback_manager = CallbackManager([finetuning_handler])  # 回调管理器\n", "llm_gpt4 = OpenAI(model=\"gpt-4-0613\", temperature=0.3)  # 使用OpenAI的GPT-4模型\n", "llm_gpt4.pydantic_program_mode = \"llm\"  # 设置pydantic程序模式为“llm”\n", "\n", "# 设置 gradient.ai 上下文\n", "base_model_slug = \"llama2-7b-chat\"  # 基础模型标识\n", "base_llm = GradientBaseModelLLM(\n", "    base_model_slug=base_model_slug, max_tokens=500, is_chat_model=True\n", ")  # 基础LLM模型\n", "base_llm.pydantic_program_mode = \"llm\"  # 设置pydantic程序模式为“llm”\n", "\n", "# 设置评估上下文（用于问题生成）\n", "eval_llm = OpenAI(model=\"gpt-4-0613\", temperature=0)  # 使用OpenAI的GPT-4模型"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 生成数据集\n", "\n", "这里我们展示如何在这些非结构化的块/节点上生成一个训练数据集。\n", "\n", "我们生成问题来提取不同上下文中的引用。我们通过一个GPT-4 RAG pipeline运行这些问题，提取结构化的输出，并记录输入/输出。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:27<00:00,  1.41it/s]\n"]}], "source": ["# 设置数据集生成器\n", "from llama_index.core.evaluation import DatasetGenerator\n", "from llama_index.core import SummaryIndex\n", "from llama_index.core import PromptTemplate\n", "from tqdm.notebook import tqdm\n", "from tqdm.asyncio import tqdm_asyncio\n", "\n", "\n", "fp = open(\"data/qa_pairs.jsonl\", \"w\")\n", "\n", "question_gen_prompt = PromptTemplate(\n", "    \"\"\"\n", "{query_str}\n", "\n", "Context:\n", "{context_str}\n", "\n", "Questions:\n", "\"\"\"\n", ")\n", "\n", "question_gen_query = \"\"\"\\\n", "以下是给定的研究论文摘录。它包含引用。\n", "请从文本中生成关于这些引用的问题。\n", "\n", "例如，以下是一些示例问题：\n", "哪些引用对应于变压器模型的相关工作？\n", "告诉我关于推进 RLHF 的作者。\n", "你能告诉我所有计算机视觉工作对应的引用吗？\\\n", "\"\"\"\n", "\n", "qr_pairs = []\n", "node_questions_tasks = []\n", "for idx, node in enumerate(nodes[:39]):\n", "    num_questions = 1  # 更改此数字以增加节点数量\n", "    dataset_generator = DatasetGenerator(\n", "        [node],\n", "        question_gen_query=question_gen_query,\n", "        text_question_template=question_gen_prompt,\n", "        llm=eval_llm,\n", "        metadata_mode=\"all\",\n", "        num_questions_per_chunk=num_questions,\n", "    )\n", "\n", "    task = dataset_generator.agenerate_questions_from_nodes(num=num_questions)\n", "    node_questions_tasks.append(task)\n", "node_questions_lists = await tqdm_asyncio.gather(*node_questions_tasks)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["39"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["len(node_questions_lists)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["['Which citations are mentioned in the section about RLHF Results?']"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["node_questions_lists[1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# [可选] 保存\n", "import pickle\n", "\n", "pickle.dump(node_questions_lists, open(\"llama2_questions.pkl\", \"wb\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# [可选] 加载问题\n", "node_questions_lists = pickle.load(open(\"llama2_questions.pkl\", \"rb\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "\n", "gpt4_index = VectorStoreIndex(nodes[:39], callback_manager=callback_manager)\n", "gpt4_query_engine = gpt4_index.as_query_engine(\n", "    output_cls=Response, llm=llm_gpt4, similarity_top_k=1\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from json import JSONDecodeError\n", "\n", "for idx, node in enumerate(tqdm(nodes[:39])):\n", "    node_questions_0 = node_questions_lists[idx]\n", "    for question in node_questions_0:\n", "        try:\n", "            # 注意：我们不需要使用响应，事件通过微调处理程序记录\n", "            gpt4_query_engine.query(question)\n", "        except Exception as e:\n", "            print(f\"问题 {question} 出错, {repr(e)}\")\n", "            pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Wrote 39 examples to llama2_citation_events.jsonl\n"]}], "source": ["finetuning_handler.save_finetuning_events(\"llama2_citation_events.jsonl\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 设置微调\n", "\n", "我们开始对生成的数据集进行微调。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.finetuning import GradientFinetuneEngine\n", "\n", "finetune_engine = GradientFinetuneEngine(\n", "    base_model_slug=base_model_slug,\n", "    # model_adapter_id='23a71710-47b3-43be-9be2-58a3efbccf2b_model_adapter',\n", "    name=\"llama2_structured\",\n", "    data_path=\"llama2_citation_events.jsonl\",\n", "    verbose=True,\n", "    max_steps=200,\n", "    batch_size=1,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["'23a71710-47b3-43be-9be2-58a3efbccf2b_model_adapter'"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["# 保存这个以备将来运行\n", "finetune_engine.model_adapter_id"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 根据需要调整epochs\n", "epochs = 2\n", "for i in range(epochs):\n", "    print(f\"** EPOCH {i} **\")\n", "    finetune_engine.finetune()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 在RAG管道中使用\n", "\n", "让我们将经过微调的LLM插入到一个完整的RAG管道中，以输出结构化的结果。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ft_llm = finetune_engine.get_finetuned_model(max_tokens=500)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "\n", "vector_index = VectorStoreIndex(nodes)\n", "query_engine = vector_index.as_query_engine(\n", "    output_cls=Response, llm=ft_llm, similarity_top_k=1\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 将基线设置为\n", "base_index = VectorStoreIndex(nodes)\n", "base_query_engine = base_index.as_query_engine(\n", "    output_cls=Response, llm=base_llm, similarity_top_k=1\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_str = \"在RLHF结果部分提到了哪些引用？\"\n", "# query_str = \"\"\"\\\n", "# 哪个引用对应于在RLHF中代表经验抽样人类偏好的数据收集概念？\\\n", "# \"\"\"\n", "# query_str = \"论文中讨论了Llama 2的开发和发布的哪些引用？\"\n", "# query_str = \"在RLHF结果部分提到了哪些引用？\"\n", "# query_str = \"哪个引用讨论了与AI硬件生产相关的碳排放？\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response = query_engine.query(query_str)\n", "print(str(response))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们来看一下资源\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 查看源代码\n", "print(response.source_nodes[0].get_content())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["让我们与基准模型（基础llama2-7b模型）进行比较。请注意，查询引擎抛出了一个错误！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 抛出一个错误！\n", "base_response = base_query_engine.query(query_str)\n", "print(str(base_response))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["作为参考，让我们也与gpt-4进行比较。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 作为参考，请查看GPT-4的响应\n", "gpt4_response = gpt4_query_engine.query(query_str)\n", "print(str(gpt4_response))"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}