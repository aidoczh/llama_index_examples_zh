{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "72f0b224", "metadata": {}, "source": ["\n", "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/low_level/router.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "645ef724-bba9-4d9d-89b0-a470dbfb1713", "metadata": {}, "source": ["# 从零开始构建路由器\n", "\n", "在本教程中，我们将向您展示如何构建一个LLM驱动的路由器模块，该模块可以将用户查询路由到子模块。\n", "\n", "路由器是一种简单但有效的自动决策形式，可以让您对数据执行动态检索/查询。\n", "\n", "在LlamaIndex中，我们通过我们的[路由器模块](https://gpt-index.readthedocs.io/en/latest/core_modules/query_modules/router/root.html)将其抽象出来。\n", "\n", "要构建一个路由器，我们将按照以下步骤进行：\n", "- 制定一个初始提示来选择一组选项\n", "- 强制结构化输出（用于文本完成端点）\n", "- 尝试与本机函数调用端点集成。\n", "\n", "然后，我们将将其插入到RAG流水线中，以动态地进行问答和总结的决策。\n"]}, {"cell_type": "markdown", "id": "ac3b2770-a695-48dc-88e8-371cf0a7d7d0", "metadata": {}, "source": ["## 1. 设置基本路由提示\n", "\n", "在其核心，路由器是一个模块，它接受一组选择。给定用户查询，它会“选择”一个相关的选项。\n", "\n", "为简单起见，我们将以一组字符串作为选择开始。\n"]}, {"cell_type": "code", "execution_count": null, "id": "d174f805", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-readers-file pymupdf\n", "%pip install llama-index-program-openai\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "7d8342c9-0f43-4898-9abd-96c92c177ac3", "metadata": {}, "outputs": [], "source": ["from llama_index.core import PromptTemplate\n", "\n", "choices = [\n", "    \"Useful for questions related to apples\",\n", "    \"Useful for questions related to oranges\",\n", "]\n", "\n", "\n", "def get_choice_str(choices):\n", "    choices_str = \"\\n\\n\".join(\n", "        [f\"{idx+1}. {c}\" for idx, c in enumerate(choices)]\n", "    )\n", "    return choices_str\n", "\n", "\n", "choices_str = get_choice_str(choices)"]}, {"cell_type": "code", "execution_count": null, "id": "5a2da874-9176-4eec-b3fc-f22cb64bb6f2", "metadata": {}, "outputs": [], "source": ["router_prompt0 = PromptTemplate(\n", "    \"Some choices are given below. It is provided in a numbered list (1 to\"\n", "    \" {num_choices}), where each item in the list corresponds to a\"\n", "    \" summary.\\n---------------------\\n{context_list}\\n---------------------\\nUsing\"\n", "    \" only the choices above and not prior knowledge, return the top choices\"\n", "    \" (no more than {max_outputs}, but only select what is needed) that are\"\n", "    \" most relevant to the question: '{query_str}'\\n\"\n", ")"]}, {"cell_type": "markdown", "id": "fe20cf42-8f36-4b65-a4c2-3f0bd6895383", "metadata": {}, "source": ["让我们尝试在一组玩具问题上使用这个提示，看看输出会是什么样的。\n"]}, {"cell_type": "code", "execution_count": null, "id": "929a238e-1271-4a18-9f3c-da8a9cd9b5e5", "metadata": {}, "outputs": [], "source": ["from llama_index.llms.openai import OpenAI\n", "\n", "llm = OpenAI(model=\"gpt-3.5-turbo\")"]}, {"cell_type": "code", "execution_count": null, "id": "8367510f-91e5-4509-a228-4becdb222edc", "metadata": {}, "outputs": [], "source": ["def get_formatted_prompt(query_str):\n", "    fmt_prompt = router_prompt0.format(\n", "        num_choices=len(choices),\n", "        max_outputs=2,\n", "        context_list=choices_str,\n", "        query_str=query_str,\n", "    )\n", "    return fmt_prompt"]}, {"cell_type": "code", "execution_count": null, "id": "64c8fc9c-5fdc-4ab6-90c5-f36031420d01", "metadata": {}, "outputs": [], "source": ["query_str = \"Can you tell me more about the amount of Vitamin C in apples\"\n", "fmt_prompt = get_formatted_prompt(query_str)\n", "response = llm.complete(fmt_prompt)"]}, {"cell_type": "code", "execution_count": null, "id": "47800333-92ea-4cb3-8e11-2d795a635fdc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1. Useful for questions related to apples\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "id": "3c111863-ea12-409c-9777-d3d387126601", "metadata": {}, "outputs": [], "source": ["query_str = \"What are the health benefits of eating orange peels?\"\n", "fmt_prompt = get_formatted_prompt(query_str)\n", "response = llm.complete(fmt_prompt)"]}, {"cell_type": "code", "execution_count": null, "id": "6fc70519-1805-4899-a4d4-a22cd49847c0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["2. Useful for questions related to oranges\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "id": "100f6664-3bef-4f5a-89e8-6c66a5d687b0", "metadata": {}, "outputs": [], "source": ["query_str = (\n", "    \"Can you tell me more about the amount of Vitamin C in apples and oranges.\"\n", ")\n", "fmt_prompt = get_formatted_prompt(query_str)\n", "response = llm.complete(fmt_prompt)"]}, {"cell_type": "code", "execution_count": null, "id": "1548044f-a0cd-4b3e-8678-4e49a97acdbb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["1. Useful for questions related to apples\n", "2. Useful for questions related to oranges\n"]}], "source": ["print(str(response))"]}, {"cell_type": "markdown", "id": "cb39f574-af06-495a-83e0-50866aca9caf", "metadata": {}, "source": ["**观察**：虽然响应对应于正确的选择，但解析为结构化输出（例如单个整数）可能有些巧妙。我们需要对选择进行一些字符串解析，以提取出单个数字，并使其能够应对故障模式。\n"]}, {"cell_type": "markdown", "id": "f7a4cfd8-a515-4613-9f43-92563c14c846", "metadata": {}, "source": ["## 2. 可以生成结构化输出的路由器提示\n", "\n", "因此，下一步是尝试提示模型输出更结构化的表示（JSON）。\n", "\n", "我们定义一个输出解析器类（`RouterOutputParser`）。这个输出解析器将负责格式化提示，并将结果解析为一个结构化对象（一个`Answer`）。\n", "\n", "然后，我们在LLM调用周围应用输出解析器的`format`和`parse`方法，以使用路由器提示生成结构化输出。\n"]}, {"cell_type": "markdown", "id": "fe2e2fb0-aa18-45c1-a8fe-0ff7124eddc1", "metadata": {}, "source": ["### 2.a 导入Answer类\n", "\n", "我们从代码库中加载Answer类。这是一个非常简单的数据类，有两个字段：`choice` 和 `reason`。\n"]}, {"cell_type": "code", "execution_count": null, "id": "fd0b6408-da38-494e-be00-d2ab0f1a9791", "metadata": {}, "outputs": [], "source": ["from dataclasses import fields\n", "from pydantic import BaseModel\n", "import json"]}, {"cell_type": "code", "execution_count": null, "id": "2ecea17b-bbf8-4a33-9252-dc53c74cbbde", "metadata": {}, "outputs": [], "source": ["class Answer(BaseModel):\n", "    choice: int\n", "    reason: str"]}, {"cell_type": "code", "execution_count": null, "id": "6fa49cf5-b892-4834-a783-0f107cbafcb7", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{\n", "  \"title\": \"Answer\",\n", "  \"type\": \"object\",\n", "  \"properties\": {\n", "    \"choice\": {\n", "      \"title\": \"Choice\",\n", "      \"type\": \"integer\"\n", "    },\n", "    \"reason\": {\n", "      \"title\": \"Reason\",\n", "      \"type\": \"string\"\n", "    }\n", "  },\n", "  \"required\": [\n", "    \"choice\",\n", "    \"reason\"\n", "  ]\n", "}\n"]}], "source": ["print(json.dumps(Answer.schema(), indent=2))"]}, {"cell_type": "markdown", "id": "f4e23bc5-0fc1-4da5-b5bc-4220f205d0e1", "metadata": {}, "source": ["### 2.b 定义路由器输出解析器\n"]}, {"cell_type": "code", "execution_count": null, "id": "585a9a25-0e9f-4da0-aa80-562fd0bb17f5", "metadata": {}, "outputs": [], "source": ["from llama_index.core.types import BaseOutputParser"]}, {"cell_type": "code", "execution_count": null, "id": "166b80df-5736-432c-b49b-03f7450ed524", "metadata": {}, "outputs": [], "source": ["FORMAT_STR = \"\"\"输出应格式化为符合以下JSON模式的JSON实例。\n", "\n", "以下是输出模式：\n", "{\n", "  \"type\": \"array\",\n", "  \"items\": {\n", "    \"type\": \"object\",\n", "    \"properties\": {\n", "      \"choice\": {\n", "        \"type\": \"integer\"\n", "      },\n", "      \"reason\": {\n", "        \"type\": \"string\"\n", "      }\n", "    },\n", "    \"required\": [\n", "      \"choice\",\n", "      \"reason\"\n", "    ],\n", "    \"additionalProperties\": false\n", "  }\n", "}\n", "\"\"\""]}, {"cell_type": "markdown", "id": "a089d844-e989-44ea-9908-846bc6e7d584", "metadata": {}, "source": ["如果我们想将 `FORMAT_STR` 作为提示模板的一部分放入 f-string 中，那么我们需要转义大括号，以防它们被视为模板变量。\n"]}, {"cell_type": "code", "execution_count": null, "id": "74614e7a-2314-4702-8918-47cd4ec378bd", "metadata": {}, "outputs": [], "source": ["def _escape_curly_braces(input_string: str) -> str:\n", "    # 将'{'替换为'{{'，将'}'替换为'}}'，以转义大括号\n", "    escaped_string = input_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n", "    return escaped_string"]}, {"cell_type": "markdown", "id": "8657b5d2-ac58-46bd-a3c2-bd0021788814", "metadata": {}, "source": ["现在我们定义一个简单的解析函数，从LLM响应中提取出JSON字符串（通过搜索方括号）。\n"]}, {"cell_type": "code", "execution_count": null, "id": "2a3c0c59-f72a-4003-b850-58bd81b5bb54", "metadata": {}, "outputs": [], "source": ["def _marshal_output_to_json(output: str) -> str:\n", "    output = output.strip()\n", "    left = output.find(\"[\")\n", "    right = output.find(\"]\")\n", "    output = output[left : right + 1]\n", "    return output"]}, {"cell_type": "markdown", "id": "c8d8db4d-475a-4fbf-8025-b2eae468b028", "metadata": {}, "source": ["我们将这些内容放在我们的`RouterOutputParser`中。\n"]}, {"cell_type": "code", "execution_count": null, "id": "c04a83fe-624e-48fd-9dc3-f4aa85fb335c", "metadata": {}, "outputs": [], "source": ["\n", "from typing import List\n", "\n", "\n", "class RouterOutputParser(BaseOutputParser):\n", "    def parse(self, output: str) -> List[Answer]:\n", "        \"\"\"解析字符串。\"\"\"\n", "        json_output = _marshal_output_to_json(output)\n", "        json_dicts = json.loads(json_output)\n", "        answers = [Answer.from_dict(json_dict) for json_dict in json_dicts]\n", "        return answers\n", "\n", "    def format(self, prompt_template: str) -> str:\n", "        return prompt_template + \"\\n\\n\" + _escape_curly_braces(FORMAT_STR)"]}, {"cell_type": "markdown", "id": "4c20c8fe-9f73-41a7-9760-b2559d475769", "metadata": {}, "source": ["### 2.c 试一试\n", "\n", "我们创建一个名为 `route_query` 的函数，它将接受输出解析器、llm 和提示模板，并输出一个结构化的答案。\n"]}, {"cell_type": "code", "execution_count": null, "id": "46126685-a108-4404-9307-73de5f51a9fd", "metadata": {}, "outputs": [], "source": ["output_parser = RouterOutputParser()"]}, {"cell_type": "code", "execution_count": null, "id": "9e67dfd6-42a0-4818-8642-6999748843bb", "metadata": {}, "outputs": [], "source": ["from typing import List\n", "\n", "\n", "def route_query(\n", "    query_str: str, choices: List[str], output_parser: RouterOutputParser\n", "):\n", "    choices_str\n", "\n", "    fmt_base_prompt = router_prompt0.format(\n", "        num_choices=len(choices),\n", "        max_outputs=len(choices),\n", "        context_list=choices_str,\n", "        query_str=query_str,\n", "    )\n", "    fmt_json_prompt = output_parser.format(fmt_base_prompt)\n", "\n", "    raw_output = llm.complete(fmt_json_prompt)\n", "    parsed = output_parser.parse(str(raw_output))\n", "\n", "    return parsed"]}, {"cell_type": "markdown", "id": "e97c0b4b-336e-4550-a4b6-950a141f1b57", "metadata": {}, "source": ["## 3. 使用函数调用端点执行路由\n", "\n", "在上一节中，我们展示了如何构建一个带有文本完成端点的路由器。这包括格式化提示以鼓励模型输出结构化的JSON，以及一个解析函数来加载JSON。\n", "\n", "这个过程可能有点混乱。函数调用端点（例如OpenAI）通过允许模型原生输出结构化函数来抽象化这种复杂性。这消除了手动提示+解析输出的需要。\n", "\n", "LlamaIndex提供了一个称为`PydanticProgram`的抽象，它与函数端点集成以生成一个结构化的Pydantic对象。我们与OpenAI和Guidance集成。\n"]}, {"cell_type": "markdown", "id": "3ceb461a-5c64-4efe-b36b-f02bef414e9e", "metadata": {}, "source": ["我们使用注释重新定义了`Answer`类，并创建了一个包含答案列表的`Answers`类。\n"]}, {"cell_type": "code", "execution_count": null, "id": "5a62bb36-b64f-4900-a89c-ddfc39afbeac", "metadata": {}, "outputs": [], "source": ["from pydantic import Field\n", "\n", "\n", "class Answer(BaseModel):\n", "    \"表示带有原因的单个选择。\"\n", "    choice: int\n", "    reason: str\n", "\n", "\n", "class Answers(BaseModel):\n", "    \"\"\"表示答案列表。\"\"\"\n", "\n", "    answers: List[Answer]"]}, {"cell_type": "code", "execution_count": null, "id": "c32cc480-4d10-4a67-90f0-ee8cc002a559", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'title': 'Answers',\n", " 'description': 'Represents a list of answers.',\n", " 'type': 'object',\n", " 'properties': {'answers': {'title': 'Answers',\n", "   'type': 'array',\n", "   'items': {'$ref': '#/definitions/Answer'}}},\n", " 'required': ['answers'],\n", " 'definitions': {'Answer': {'title': 'Answer',\n", "   'description': 'Represents a single choice with a reason.',\n", "   'type': 'object',\n", "   'properties': {'choice': {'title': 'Choice', 'type': 'integer'},\n", "    'reason': {'title': 'Reason', 'type': 'string'}},\n", "   'required': ['choice', 'reason']}}}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["Answers.schema()"]}, {"cell_type": "code", "execution_count": null, "id": "6c65d01c-b56e-49ae-89c6-268d29bb25c0", "metadata": {}, "outputs": [], "source": ["from llama_index.program.openai import OpenAIPydanticProgram"]}, {"cell_type": "code", "execution_count": null, "id": "8d6f7879-b3f6-4f65-94df-e2ac2edc0a3b", "metadata": {}, "outputs": [], "source": ["router_prompt1 = router_prompt0.partial_format(\n", "    num_choices=len(choices),\n", "    max_outputs=len(choices),\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "acae0e8c-442e-43f5-9753-bd1afff12ae5", "metadata": {}, "outputs": [], "source": ["program = OpenAIPydanticProgram.from_defaults(\n", "    output_cls=Answers,\n", "    prompt=router_prompt1,\n", "    verbose=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "a44e079d-545c-4a75-abc2-409e8fd5c85e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Function call: Answers with args: {\n", "  \"answers\": [\n", "    {\n", "      \"choice\": 2,\n", "      \"reason\": \"Orange peels are related to oranges\"\n", "    }\n", "  ]\n", "}\n"]}], "source": ["query_str = \"What are the health benefits of eating orange peels?\"\n", "output = program(context_list=choices_str, query_str=query_str)"]}, {"cell_type": "code", "execution_count": null, "id": "d914bf0e-a1ec-49cf-adfd-3636c536908e", "metadata": {}, "outputs": [{"data": {"text/plain": ["Answers(answers=[Answer(choice=2, reason='Orange peels are related to oranges')])"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["output"]}, {"cell_type": "markdown", "id": "7bc1e1ce-6075-4655-9c6c-3882806da483", "metadata": {}, "source": ["## 4. 将路由器模块作为RAG管道的一部分\n", "\n", "在本节中，我们将把路由器模块用于RAG管道中。我们将使用它动态决定是执行问答还是摘要。我们可以通过我们的向量索引轻松获得一个问答查询引擎，而摘要则是通过我们的摘要索引执行的。每个查询引擎被描述为我们的路由器的一个“选择”，我们将整个内容组合成一个单一的查询引擎。\n"]}, {"cell_type": "markdown", "id": "34382f7b-11ff-44ef-95a3-63eb4723f9b7", "metadata": {}, "source": ["### 设置：加载数据\n", "\n", "我们将Llama 2论文作为数据加载。\n"]}, {"cell_type": "code", "execution_count": null, "id": "a1d16ec2-c293-4d58-850d-80bc90b01a38", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["mkdir: data: File exists\n", "--2023-09-17 23:37:11--  https://arxiv.org/pdf/2307.09288.pdf\n", "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n", "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 13661300 (13M) [application/pdf]\n", "Saving to: ‘data/llama2.pdf’\n", "\n", "data/llama2.pdf     100%[===================>]  13.03M  1.50MB/s    in 9.5s    \n", "\n", "2023-09-17 23:37:22 (1.37 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n"]}], "source": ["!mkdir data\n", "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""]}, {"cell_type": "code", "execution_count": null, "id": "fcd9b401-1212-4c33-a18a-14f39ecb9989", "metadata": {}, "outputs": [], "source": ["from pathlib import Path\n", "from llama_index.readers.file import PyMuPDFReader"]}, {"cell_type": "code", "execution_count": null, "id": "e538688b-c578-4807-a78b-c0db5ad722ca", "metadata": {}, "outputs": [], "source": ["loader = PyMuPDFReader()\n", "documents = loader.load(file_path=\"./data/llama2.pdf\")"]}, {"cell_type": "markdown", "id": "ce59ee6c-9197-4709-9208-a0e3450a641d", "metadata": {}, "source": ["### 设置：定义索引\n", "\n", "在这份数据上定义一个向量索引和一个摘要索引。\n"]}, {"cell_type": "code", "execution_count": null, "id": "8615db4e-7572-4828-b55f-f846d653aa5c", "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "from llama_index.core import SummaryIndex\n", "from llama_index.core.node_parser import SentenceSplitter\n", "\n", "splitter = SentenceSplitter(chunk_size=1024)\n", "vector_index = VectorStoreIndex.from_documents(\n", "    documents, transformations=[splitter]\n", ")\n", "summary_index = SummaryIndex.from_documents(\n", "    documents, transformations=[splitter]\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "23103269-6e9f-4e68-b964-4b7a1238a7f4", "metadata": {}, "outputs": [], "source": ["vector_query_engine = vector_index.as_query_engine(llm=llm)\n", "summary_query_engine = summary_index.as_query_engine(llm=llm)"]}, {"cell_type": "markdown", "id": "85126015-7873-4540-a977-8ac351a0f427", "metadata": {}, "source": ["### 定义RouterQueryEngine\n", "\n", "我们通过对`CustomQueryEngine`进行子类化来定义一个自定义路由器。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6cdf6b37-31de-4c23-9e85-9cbfb732b591", "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_engine import CustomQueryEngine, BaseQueryEngine\n", "from llama_index.core.response_synthesizers import TreeSummarize"]}, {"cell_type": "code", "execution_count": null, "id": "99ec5310-5385-48a6-863a-ced4f2aeb303", "metadata": {}, "outputs": [], "source": ["class RouterQueryEngine(CustomQueryEngine):\n", "    \"\"\"使用我们的Pydantic程序执行路由。\"\"\"\n", "\n", "    query_engines: List[BaseQueryEngine]\n", "    choice_descriptions: List[str]\n", "    verbose: bool = False\n", "    router_prompt: PromptTemplate\n", "    llm: OpenAI\n", "    summarizer: TreeSummarize = Field(default_factory=TreeSummarize)\n", "\n", "    def custom_query(self, query_str: str):\n", "        \"\"\"定义自定义查询。\"\"\"\n", "\n", "        program = OpenAIPydanticProgram.from_defaults(\n", "            output_cls=Answers,\n", "            prompt=router_prompt1,\n", "            verbose=self.verbose,\n", "            llm=self.llm,\n", "        )\n", "\n", "        choices_str = get_choice_str(self.choice_descriptions)\n", "        output = program(context_list=choices_str, query_str=query_str)\n", "        # 打印选择和原因，并查询底层引擎\n", "        if self.verbose:\n", "            print(f\"选择的选项：\")\n", "            for answer in output.answers:\n", "                print(f\"选择：{answer.choice}，原因：{answer.reason}\")\n", "\n", "        responses = []\n", "        for answer in output.answers:\n", "            choice_idx = answer.choice - 1\n", "            query_engine = self.query_engines[choice_idx]\n", "            response = query_engine.query(query_str)\n", "            responses.append(response)\n", "\n", "        # 如果选择了单个选项，我们可以直接返回该响应\n", "        if len(responses) == 1:\n", "            return responses[0]\n", "        else:\n", "            # 如果选择了多个选项，我们可以选择一个摘要生成器\n", "            response_strs = [str(r) for r in responses]\n", "            result_response = self.summarizer.get_response(\n", "                query_str, response_strs\n", "            )\n", "            return result_response"]}, {"cell_type": "code", "execution_count": null, "id": "153701ce-cd1a-4318-b575-afc508e10f4d", "metadata": {}, "outputs": [], "source": ["choices = [\n", "    (\n", "        \"Useful for answering questions about specific sections of the Llama 2\"\n", "        \" paper\"\n", "    ),\n", "    \"Useful for questions that ask for a summary of the whole paper\",\n", "]\n", "\n", "router_query_engine = RouterQueryEngine(\n", "    query_engines=[vector_query_engine, summary_query_engine],\n", "    choice_descriptions=choices,\n", "    verbose=True,\n", "    router_prompt=router_prompt1,\n", "    llm=OpenAI(model=\"gpt-4\"),\n", ")"]}, {"cell_type": "markdown", "id": "865f15fa-710c-41eb-a2a2-745a8e3c0ced", "metadata": {}, "source": ["### 尝试我们构建的路由查询引擎\n", "\n", "让我们来试试我们自己构建的路由查询引擎！我们提出一个问题，将其路由到向量查询引擎，还有另一个问题将被路由到摘要引擎。\n"]}, {"cell_type": "code", "execution_count": null, "id": "7769a3ec-f6b6-44ec-acab-ed8a14a6b32b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Function call: Answers with args: {\n", "  \"answers\": [\n", "    {\n", "      \"choice\": 1,\n", "      \"reason\": \"This question is asking for specific information about the Llama 2 model and its comparison to GPT-4 in the experimental results. Therefore, the summary that is useful for answering questions about specific sections of the paper would be most relevant.\"\n", "    }\n", "  ]\n", "}\n", "Selected choice(s):\n", "Choice: 1, Reason: This question is asking for specific information about the Llama 2 model and its comparison to GPT-4 in the experimental results. Therefore, the summary that is useful for answering questions about specific sections of the paper would be most relevant.\n"]}], "source": ["response = router_query_engine.query(\n", "    \"How does the Llama 2 model compare to GPT-4 in the experimental results?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "27448520-0727-47e5-9888-eb4cdefbc890", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The Llama 2 model performs better than GPT-4 in the experimental results.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "id": "6bfc9df7-c48b-477e-a7ac-a0f8ca4a9e21", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Function call: Answers with args: {\n", "  \"answers\": [\n", "    {\n", "      \"choice\": 2,\n", "      \"reason\": \"This choice is directly related to providing a summary of the whole paper, which is what the question asks for.\"\n", "    }\n", "  ]\n", "}\n", "Selected choice(s):\n", "Choice: 2, Reason: This choice is directly related to providing a summary of the whole paper, which is what the question asks for.\n"]}], "source": ["response = router_query_engine.query(\"Can you give a summary of this paper?\")"]}, {"cell_type": "code", "execution_count": null, "id": "ab3c9756-81ca-4b4f-8ae1-0e37d9e1464b", "metadata": {}, "outputs": [], "source": ["print(str(response))"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}