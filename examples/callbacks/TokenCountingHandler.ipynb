{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/callbacks/TokenCountingHandler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Tokenè®¡æ•°å¤„ç†ç¨‹åº\n", "\n", "æœ¬ç¬”è®°æœ¬ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨TokenCountingHandlerä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒæ¥è·Ÿè¸ªæ‚¨çš„æç¤ºã€å®Œæˆå’ŒåµŒå…¥æ ‡è®°çš„ä½¿ç”¨æƒ…å†µã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## è®¾ç½®\n", "\n", "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¾ç½®å›è°ƒå’ŒæœåŠ¡ä¸Šä¸‹æ–‡ã€‚æˆ‘ä»¬è®¾ç½®å…¨å±€è®¾ç½®ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…æ‹…å¿ƒå°†å…¶ä¼ é€’ç»™ç´¢å¼•å’ŒæŸ¥è¯¢ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tiktoken\n", "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Settings\n", "\n", "\n", "token_counter = TokenCountingHandler(\n", "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n", ")\n", "\n", "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n", "Settings.callback_manager = CallbackManager([token_counter])"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## ä»¤ç‰Œè®¡æ•°\n", "\n", "ä»¤ç‰Œè®¡æ•°å™¨å°†è·Ÿè¸ªåµŒå…¥ã€æç¤ºå’Œå®Œæˆä»¤ç‰Œçš„ä½¿ç”¨æƒ…å†µã€‚ä»¤ç‰Œè®¡æ•°æ˜¯__ç´¯ç§¯__çš„ï¼Œåªæœ‰åœ¨æ‚¨é€‰æ‹©è¿™æ ·åšæ—¶æ‰ä¼šé‡ç½®ï¼Œä½¿ç”¨`token_counter.reset_counts()`ã€‚\n", "\n", "### åµŒå…¥ä»¤ç‰Œä½¿ç”¨æƒ…å†µ\n", "\n", "ç°åœ¨è®¾ç½®å¥½äº†è®¾ç½®ï¼Œè®©æˆ‘ä»¬æ¥è·Ÿè¸ªæˆ‘ä»¬çš„åµŒå…¥ä»¤ç‰Œä½¿ç”¨æƒ…å†µã€‚\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## ä¸‹è½½æ•°æ®\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "\n", "index = VectorStoreIndex.from_documents(documents)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["20723\n"]}], "source": ["print(token_counter.total_embedding_token_count)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["çœ‹èµ·æ¥æ²¡é—®é¢˜ï¼åœ¨ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬é‡ç½®è®¡æ•°ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["token_counter.reset_counts()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### LLM + åµŒå…¥å¼ä»¤ç‰Œç”¨æ³•\n", "\n", "æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸ªæŸ¥è¯¢ï¼Œçœ‹çœ‹è®¡æ•°æ˜¯ä»€ä¹ˆæ ·å­ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(similarity_top_k=4)\n", "response = query_engine.query(\"What did the author do growing up?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Embedding Tokens:  8 \n", " LLM Prompt Tokens:  4518 \n", " LLM Completion Tokens:  45 \n", " Total LLM Token Count:  4563 \n", "\n"]}], "source": ["print(\n", "    \"Embedding Tokens: \",\n", "    token_counter.total_embedding_token_count,\n", "    \"\\n\",\n", "    \"LLM Prompt Tokens: \",\n", "    token_counter.prompt_llm_token_count,\n", "    \"\\n\",\n", "    \"LLM Completion Tokens: \",\n", "    token_counter.completion_llm_token_count,\n", "    \"\\n\",\n", "    \"Total LLM Token Count: \",\n", "    token_counter.total_llm_token_count,\n", "    \"\\n\",\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Token Counting + Streamingï¼ˆä»¤ç‰Œè®¡æ•° + æµå¤„ç†ï¼‰ï¼\n", "\n", "ä»¤ç‰Œè®¡æ•°å¤„ç†ç¨‹åºè¿˜å¯ä»¥åœ¨æµå¤„ç†æœŸé—´è¿›è¡Œä»¤ç‰Œè®¡æ•°ã€‚\n", "\n", "åœ¨è¿™é‡Œï¼Œä»¤ç‰Œè®¡æ•°åªä¼šåœ¨æµå®Œæˆåè¿›è¡Œã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["token_counter.reset_counts()", "", "query_engine = index.as_query_engine(similarity_top_k=4, streaming=True)", "response = query_engine.query(\"Interleafå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ\")", "", "# å®Œæˆæµå¼å¤„ç†", "for token in response.response_gen:", "    # print(token, end=\"\", flush=True)", "    continue"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Embedding Tokens:  6 \n", " LLM Prompt Tokens:  4563 \n", " LLM Completion Tokens:  123 \n", " Total LLM Token Count:  4686 \n", "\n"]}], "source": ["print(\n", "    \"Embedding Tokens: \",\n", "    token_counter.total_embedding_token_count,\n", "    \"\\n\",\n", "    \"LLM Prompt Tokens: \",\n", "    token_counter.prompt_llm_token_count,\n", "    \"\\n\",\n", "    \"LLM Completion Tokens: \",\n", "    token_counter.completion_llm_token_count,\n", "    \"\\n\",\n", "    \"Total LLM Token Count: \",\n", "    token_counter.total_llm_token_count,\n", "    \"\\n\",\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## é«˜çº§ç”¨æ³•\n", "\n", "ä»¤ç‰Œè®¡æ•°å™¨è·Ÿè¸ªæ¯ä¸ªä»¤ç‰Œä½¿ç”¨äº‹ä»¶ï¼Œå­˜å‚¨åœ¨ä¸€ä¸ªåä¸º`TokenCountingEvent`çš„å¯¹è±¡ä¸­ã€‚è¯¥å¯¹è±¡å…·æœ‰ä»¥ä¸‹å±æ€§ï¼š\n", "\n", "- prompt -> å‘é€åˆ°LLMæˆ–åµŒå…¥æ¨¡å‹çš„æç¤ºå­—ç¬¦ä¸²\n", "- prompt_token_count -> LLMæç¤ºçš„ä»¤ç‰Œè®¡æ•°\n", "- completion -> ä»LLMæ¥æ”¶åˆ°çš„å­—ç¬¦ä¸²å®Œæˆï¼ˆåµŒå…¥æ¨¡å‹ä¸ä½¿ç”¨ï¼‰\n", "- completion_token_count -> LLMå®Œæˆçš„ä»¤ç‰Œè®¡æ•°ï¼ˆåµŒå…¥æ¨¡å‹ä¸ä½¿ç”¨ï¼‰\n", "- total_token_count -> äº‹ä»¶çš„æç¤º+å®Œæˆä»¤ç‰Œçš„æ€»æ•°\n", "- event_id -> äº‹ä»¶çš„å­—ç¬¦ä¸²IDï¼Œä¸å…¶ä»–å›è°ƒå¤„ç†ç¨‹åºå¯¹é½\n", "\n", "è¿™äº›äº‹ä»¶åœ¨ä»¤ç‰Œè®¡æ•°å™¨ä¸Šä»¥ä¸¤ä¸ªåˆ—è¡¨è¿›è¡Œè·Ÿè¸ªï¼š\n", "\n", "- llm_token_counts\n", "- embedding_token_counts\n", "\n", "è®©æˆ‘ä»¬æ¥çœ‹çœ‹å®ƒä»¬æ˜¯ä»€ä¹ˆæ ·å­ï¼\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Num LLM token count events:  2\n", "Num Embedding token count events:  1\n"]}], "source": ["print(\"Num LLM token count events: \", len(token_counter.llm_token_counts))\n", "print(\n", "    \"Num Embedding token count events: \",\n", "    len(token_counter.embedding_token_counts),\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["è¿™å¾ˆæœ‰é“ç†ï¼ä¹‹å‰çš„æŸ¥è¯¢åµŒå…¥äº†æŸ¥è¯¢æ–‡æœ¬ï¼Œç„¶åè¿›è¡Œäº†2æ¬¡LLMè°ƒç”¨ï¼ˆå› ä¸ºtop kä¸º4ï¼Œè€Œé»˜è®¤çš„åˆ†å—å¤§å°ä¸º1024ï¼Œæ‰€ä»¥éœ€è¦è¿›è¡Œä¸¤æ¬¡å•ç‹¬çš„è°ƒç”¨ï¼Œä»¥ä¾¿LLMå¯ä»¥è¯»å–æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æœ¬ï¼‰ã€‚\n", "\n", "æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿçœ‹ä¸€ä¸‹å•ä¸ªäº‹ä»¶çš„è¿™äº›äº‹ä»¶æ˜¯ä»€ä¹ˆæ ·å­ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["prompt:  system: You are an expert Q&A system that is trusted around the world.\n", "Always answer the query using ...\n", "\n", "prompt token count:  3873 \n", "\n", "completion:  assistant: At Interleaf, the company had added a scripting language inspired by Emacs and made it a  ...\n", "\n", "completion token count:  95 \n", "\n", "total token count 3968\n"]}], "source": ["print(\"prompt: \", token_counter.llm_token_counts[0].prompt[:100], \"...\\n\")\n", "print(\n", "    \"prompt token count: \",\n", "    token_counter.llm_token_counts[0].prompt_token_count,\n", "    \"\\n\",\n", ")\n", "\n", "print(\n", "    \"completion: \", token_counter.llm_token_counts[0].completion[:100], \"...\\n\"\n", ")\n", "print(\n", "    \"completion token count: \",\n", "    token_counter.llm_token_counts[0].completion_token_count,\n", "    \"\\n\",\n", ")\n", "\n", "print(\"total token count\", token_counter.llm_token_counts[0].total_token_count)"]}], "metadata": {"kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}