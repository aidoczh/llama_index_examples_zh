{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/callbacks/TokenCountingHandler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Token计数处理程序\n", "\n", "本笔记本介绍了如何使用TokenCountingHandler以及如何使用它来跟踪您的提示、完成和嵌入标记的使用情况。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 设置\n", "\n", "在这里，我们设置回调和服务上下文。我们设置全局设置，这样我们就不必担心将其传递给索引和查询。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tiktoken\n", "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Settings\n", "\n", "\n", "token_counter = TokenCountingHandler(\n", "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n", ")\n", "\n", "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n", "Settings.callback_manager = CallbackManager([token_counter])"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 令牌计数\n", "\n", "令牌计数器将跟踪嵌入、提示和完成令牌的使用情况。令牌计数是__累积__的，只有在您选择这样做时才会重置，使用`token_counter.reset_counts()`。\n", "\n", "### 嵌入令牌使用情况\n", "\n", "现在设置好了设置，让我们来跟踪我们的嵌入令牌使用情况。\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "\n", "index = VectorStoreIndex.from_documents(documents)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["20723\n"]}], "source": ["print(token_counter.total_embedding_token_count)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["看起来没问题！在继续之前，让我们重置计数。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["token_counter.reset_counts()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### LLM + 嵌入式令牌用法\n", "\n", "接下来，让我们测试一个查询，看看计数是什么样子。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(similarity_top_k=4)\n", "response = query_engine.query(\"What did the author do growing up?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Embedding Tokens:  8 \n", " LLM Prompt Tokens:  4518 \n", " LLM Completion Tokens:  45 \n", " Total LLM Token Count:  4563 \n", "\n"]}], "source": ["print(\n", "    \"Embedding Tokens: \",\n", "    token_counter.total_embedding_token_count,\n", "    \"\\n\",\n", "    \"LLM Prompt Tokens: \",\n", "    token_counter.prompt_llm_token_count,\n", "    \"\\n\",\n", "    \"LLM Completion Tokens: \",\n", "    token_counter.completion_llm_token_count,\n", "    \"\\n\",\n", "    \"Total LLM Token Count: \",\n", "    token_counter.total_llm_token_count,\n", "    \"\\n\",\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### Token Counting + Streaming（令牌计数 + 流处理）！\n", "\n", "令牌计数处理程序还可以在流处理期间进行令牌计数。\n", "\n", "在这里，令牌计数只会在流完成后进行。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["token_counter.reset_counts()", "", "query_engine = index.as_query_engine(similarity_top_k=4, streaming=True)", "response = query_engine.query(\"Interleaf发生了什么？\")", "", "# 完成流式处理", "for token in response.response_gen:", "    # print(token, end=\"\", flush=True)", "    continue"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Embedding Tokens:  6 \n", " LLM Prompt Tokens:  4563 \n", " LLM Completion Tokens:  123 \n", " Total LLM Token Count:  4686 \n", "\n"]}], "source": ["print(\n", "    \"Embedding Tokens: \",\n", "    token_counter.total_embedding_token_count,\n", "    \"\\n\",\n", "    \"LLM Prompt Tokens: \",\n", "    token_counter.prompt_llm_token_count,\n", "    \"\\n\",\n", "    \"LLM Completion Tokens: \",\n", "    token_counter.completion_llm_token_count,\n", "    \"\\n\",\n", "    \"Total LLM Token Count: \",\n", "    token_counter.total_llm_token_count,\n", "    \"\\n\",\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 高级用法\n", "\n", "令牌计数器跟踪每个令牌使用事件，存储在一个名为`TokenCountingEvent`的对象中。该对象具有以下属性：\n", "\n", "- prompt -> 发送到LLM或嵌入模型的提示字符串\n", "- prompt_token_count -> LLM提示的令牌计数\n", "- completion -> 从LLM接收到的字符串完成（嵌入模型不使用）\n", "- completion_token_count -> LLM完成的令牌计数（嵌入模型不使用）\n", "- total_token_count -> 事件的提示+完成令牌的总数\n", "- event_id -> 事件的字符串ID，与其他回调处理程序对齐\n", "\n", "这些事件在令牌计数器上以两个列表进行跟踪：\n", "\n", "- llm_token_counts\n", "- embedding_token_counts\n", "\n", "让我们来看看它们是什么样子！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Num LLM token count events:  2\n", "Num Embedding token count events:  1\n"]}], "source": ["print(\"Num LLM token count events: \", len(token_counter.llm_token_counts))\n", "print(\n", "    \"Num Embedding token count events: \",\n", "    len(token_counter.embedding_token_counts),\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["这很有道理！之前的查询嵌入了查询文本，然后进行了2次LLM调用（因为top k为4，而默认的分块大小为1024，所以需要进行两次单独的调用，以便LLM可以读取所有检索到的文本）。\n", "\n", "接下来，让我们快速看一下单个事件的这些事件是什么样子。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["prompt:  system: You are an expert Q&A system that is trusted around the world.\n", "Always answer the query using ...\n", "\n", "prompt token count:  3873 \n", "\n", "completion:  assistant: At Interleaf, the company had added a scripting language inspired by Emacs and made it a  ...\n", "\n", "completion token count:  95 \n", "\n", "total token count 3968\n"]}], "source": ["print(\"prompt: \", token_counter.llm_token_counts[0].prompt[:100], \"...\\n\")\n", "print(\n", "    \"prompt token count: \",\n", "    token_counter.llm_token_counts[0].prompt_token_count,\n", "    \"\\n\",\n", ")\n", "\n", "print(\n", "    \"completion: \", token_counter.llm_token_counts[0].completion[:100], \"...\\n\"\n", ")\n", "print(\n", "    \"completion token count: \",\n", "    token_counter.llm_token_counts[0].completion_token_count,\n", "    \"\\n\",\n", ")\n", "\n", "print(\"total token count\", token_counter.llm_token_counts[0].total_token_count)"]}], "metadata": {"kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}