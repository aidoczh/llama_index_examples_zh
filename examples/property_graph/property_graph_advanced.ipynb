{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 使用预定义模式构建属性图\n", "\n", "在这个笔记本中，我们将演示如何使用Neo4j、Ollama和Huggingface来构建一个属性图。\n", "\n", "具体来说，我们将使用`SchemaLLMPathExtractor`，它允许我们指定一个精确的模式，其中包含可能的实体类型、关系类型，并定义它们如何连接在一起。\n", "\n", "当你想要构建一个特定的图形，并且想要限制LLM正在预测的内容时，这将非常有用。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index\n", "%pip install llama-index-llms-ollama\n", "%pip install llama-index-embeddings-huggingface\n", "%pip install llama-index-graph-stores-neo4j"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 加载数据\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["首先，让我们下载一些示例数据来进行操作。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/loganmarkewich/Library/Caches/pypoetry/virtualenvs/llama-index-bXUwlEfH-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n"]}], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 图构建\n", "\n", "为了构建我们的图，我们将利用`SchemaLLMPathExtractor`来构建我们的图。\n", "\n", "给定图的某些模式，我们可以提取遵循该模式的实体和关系，而不是让LLM随机决定实体和关系。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from typing import Literal", "from llama_index.llms.ollama import Ollama", "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor", "", "# 最佳实践是使用大写", "entities = Literal[\"PERSON\", \"PLACE\", \"ORGANIZATION\"]", "relations = Literal[\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"]", "", "# 定义实体可以具有哪些关系", "validation_schema = {", "    \"PERSON\": [\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"],", "    \"PLACE\": [\"HAS\", \"PART_OF\", \"WORKED_AT\"],", "    \"ORGANIZATION\": [\"HAS\", \"PART_OF\", \"WORKED_WITH\"],", "}", "", "kg_extractor = SchemaLLMPathExtractor(", "    llm=Ollama(model=\"llama3\", json_mode=True, request_timeout=3600),", "    possible_entities=entities,", "    possible_relations=relations,", "    kg_validation_schema=validation_schema,", "    # 如果为false，则允许超出模式的值", "    # 用于将模式用作建议时很有用", "    strict=True,", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["要在本地启动Neo4j，首先确保已安装了Docker。然后，您可以使用以下Docker命令启动数据库：\n", "\n", "```bash\n", "docker run \\\n", "    -p 7474:7474 -p 7687:7687 \\\n", "    -v $PWD/data:/data -v $PWD/plugins:/plugins \\\n", "    --name neo4j-apoc \\\n", "    -e NEO4J_apoc_export_file_enabled=true \\\n", "    -e NEO4J_apoc_import_file_enabled=true \\\n", "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n", "    -e NEO4JLABS_PLUGINS=\\[\\\"apoc\\\"\\] \\\n", "    neo4j:latest\n", "```\n", "\n", "从这里，您可以在 [http://localhost:7474/](http://localhost:7474/) 打开数据库。在该页面上，您将被要求登录。使用默认的用户名/密码 `neo4j` 和 `neo4j`。\n", "\n", "第一次登录后，您将被要求更改密码。\n", "\n", "之后，您就可以准备创建您的第一个属性图了！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.graph_stores.neo4j import Neo4jPGStore\n", "\n", "graph_store = Neo4jPGStore(\n", "    username=\"neo4j\",\n", "    password=\"<password>\",\n", "    url=\"bolt://localhost:7687\",\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**注意：** 使用本地模型进行提取比使用API模型要慢。本地模型（如Ollama）通常只能进行顺序处理。在M2 Max 上可能需要大约10分钟。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import PropertyGraphIndex\n", "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n", "\n", "index = PropertyGraphIndex.from_documents(\n", "    documents,\n", "    kg_extractors=[kg_extractor],\n", "    embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n", "    property_graph_store=graph_store,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["如果我们检查创建的图表，我们会发现它只包括我们定义的关系和实体类型！\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![本地图](./local_kg.png)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["有关所有`kg_extractors`的信息，请参阅[文档](../../module_guides/indexing/lpg_index_guide.md#construction)。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 查询\n", "\n", "现在我们已经创建了图，我们可以对其进行查询。\n", "\n", "与本笔记本的主题一致，我们将使用较低级别的API，并自己构建所有的检索器！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.indices.property_graph import (\n", "    LLMSynonymRetriever,\n", "    VectorContextRetriever,\n", ")\n", "\n", "\n", "llm_synonym = LLMSynonymRetriever(\n", "    index.property_graph_store,\n", "    llm=Ollama(model=\"llama3\", request_timeout=3600),\n", "    include_text=False,\n", ")\n", "vector_context = VectorContextRetriever(\n", "    index.property_graph_store,\n", "    embed_model=HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"),\n", "    include_text=False,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["retriever = index.as_retriever(\n", "    sub_retrievers=[\n", "        llm_synonym,\n", "        vector_context,\n", "    ]\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Paul Graham -> WORKED_AT -> Interleaf\n", "Paul Graham -> WORKED_AT -> Yahoo\n", "Paul Graham -> WORKED_AT -> Cambridge\n", "Tom Cheatham -> WORKED_AT -> Cambridge\n", "Kevin Hale -> WORKED_AT -> Viaweb\n", "Paul Graham -> WORKED_AT -> Viaweb\n", "Paul Graham -> WORKED_ON -> Viaweb\n", "Paul Graham -> PART_OF -> Viaweb\n"]}], "source": ["nodes = retriever.retrieve(\"What happened at Interleaf?\")\n", "\n", "for node in nodes:\n", "    print(node.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们也可以使用类似的语法创建一个查询引擎。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Paul Graham worked at Interleaf.\n"]}], "source": ["query_engine = index.as_query_engine(\n", "    sub_retrievers=[\n", "        llm_synonym,\n", "        vector_context,\n", "    ],\n", "    llm=Ollama(model=\"llama3\", request_timeout=3600),\n", ")\n", "\n", "response = query_engine.query(\"What happened at Interleaf?\")\n", "\n", "print(str(response))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["有关所有检索器的更多信息，请参阅[完整指南](../../module_guides/indexing/lpg_index_guide.md#retrieval-and-querying)。\n"]}], "metadata": {"kernelspec": {"display_name": "llama-index-bXUwlEfH-py3.11", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}