{"cells": [{"cell_type": "markdown", "id": "331f5d8c-3041-4d69-a957-93a79b335562", "metadata": {}, "source": ["# 查询转换手册\n", "\n", "用户查询在作为RAG查询引擎、代理或任何其他流水线的一部分执行之前，可以以多种方式进行转换和分解。\n", "\n", "在本指南中，我们将向您展示不同的转换和分解查询的方式，并找到相关工具集。每种技术可能适用于不同的用例！\n", "\n", "为了命名的目的，我们将基础流水线定义为“工具”。以下是不同的查询转换：\n", "\n", "1. **路由**：保留查询，但识别查询适用的工具的相关子集。将这些工具作为相关选择输出。\n", "2. **查询重写**：保留工具，但以各种不同的方式重写查询，以针对相同的工具执行。\n", "3. **子问题**：将查询分解为针对不同工具（通过它们的元数据标识）的多个子问题。\n", "4. **ReAct代理工具选择**：给定初始查询，识别1）要选择的工具，和2）要在工具上执行的查询。\n", "\n", "本指南的目标是向您展示如何将这些查询转换作为**模块化**组件使用。当然，这些组件中的每一个都可以插入到更大的系统中（例如，子问题生成器是我们的`SubQuestionQueryEngine`的一部分）-每个组件的指南都在下面链接中。\n", "\n", "看一看，让我们知道您的想法！\n"]}, {"cell_type": "code", "execution_count": null, "id": "8c100403", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-question-gen-openai\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "30451d5d-91a8-4148-bdf8-a4b4fd8ab0fc", "metadata": {}, "outputs": [], "source": ["from IPython.display import Markdown, display", "", "# 定义显示提示的函数", "def display_prompt_dict(prompts_dict):", "    for k, p in prompts_dict.items():", "        text_md = f\"**提示键**：{k}<br>\" f\"**文本：**<br>\"", "        display(Markdown(text_md))", "        print(p.get_template())", "        display(Markdown(\"<br><br>\"))"]}, {"cell_type": "markdown", "id": "c0023e21-1bd2-46cb-b829-c697d1132862", "metadata": {}, "source": ["## 路由\n", "\n", "在这个例子中，我们展示了如何使用查询来选择一组相关的工具选项。\n", "\n", "我们使用我们的 `selector` 抽象来选择相关的工具 - 它可以是单个工具，也可以是多个工具，这取决于抽象的定义。\n", "\n", "我们有四个选择器：(LLM 或函数调用) x (单选或多选) 的组合。\n"]}, {"cell_type": "code", "execution_count": null, "id": "ec907783-a091-4ae8-b8a5-d81ae3ede92b", "metadata": {}, "outputs": [], "source": ["from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n", "from llama_index.core.selectors import (\n", "    PydanticMultiSelector,\n", "    PydanticSingleSelector,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "6f5000dc-17db-4a1b-8859-b6651a2a9465", "metadata": {}, "outputs": [], "source": ["# pydantic选择器将pydantic对象提供给调用API的函数", "# 单个选择器（pydantic，函数调用）", "# selector = PydanticSingleSelector.from_defaults()", "", "# 多个选择器（pydantic，函数调用）", "# selector = PydanticMultiSelector.from_defaults()", "", "# LLM选择器使用文本补全端点", "# 单个选择器（LLM）", "# selector = LLMSingleSelector.from_defaults()", "# 多个选择器（LLM）", "selector = LLMMultiSelector.from_defaults()"]}, {"cell_type": "code", "execution_count": null, "id": "21954a93-e2f2-460b-8073-f6a8a88ad1d2", "metadata": {}, "outputs": [], "source": ["from llama_index.core.tools import ToolMetadata\n", "\n", "tool_choices = [\n", "    ToolMetadata(\n", "        name=\"covid_nyt\",\n", "        description=(\"This tool contains a NYT news article about COVID-19\"),\n", "    ),\n", "    ToolMetadata(\n", "        name=\"covid_wiki\",\n", "        description=(\"This tool contains the Wikipedia page about COVID-19\"),\n", "    ),\n", "    ToolMetadata(\n", "        name=\"covid_tesla\",\n", "        description=(\"This tool contains the Wikipedia page about apples\"),\n", "    ),\n", "]"]}, {"cell_type": "code", "execution_count": null, "id": "2a559269-5129-4474-aade-ee6039ff4066", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: prompt<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\n", "---------------------\n", "{context_list}\n", "---------------------\n", "Using only the choices above and not prior knowledge, return the top choices (no more than {max_outputs}, but only select what is needed) that are most relevant to the question: '{query_str}'\n", "\n", "\n", "The output should be ONLY JSON formatted as a JSON instance.\n", "\n", "Here is an example:\n", "[\n", "    {{\n", "        choice: 1,\n", "        reason: \"<insert reason for choice>\"\n", "    }},\n", "    ...\n", "]\n", "\n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["display_prompt_dict(selector.get_prompts())"]}, {"cell_type": "code", "execution_count": null, "id": "cec2fba5-8e5e-4125-9061-56ae9c707329", "metadata": {}, "outputs": [], "source": ["selector_result = selector.select(\n", "    tool_choices, query=\"Tell me more about COVID-19\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "1cb4b53a-0861-4300-b593-ad63edb53e96", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SingleSelection(index=0, reason='This tool contains a NYT news article about COVID-19'),\n", " SingleSelection(index=1, reason='This tool contains the Wikipedia page about COVID-19')]"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["selector_result.selections"]}, {"cell_type": "markdown", "id": "f7a8668c-7cd3-439d-965f-3ab9e1347f24", "metadata": {}, "source": ["了解更多关于我们的路由抽象，请访问我们的[专门的路由器页面](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html)。\n"]}, {"cell_type": "markdown", "id": "cc787575-e60c-4c06-8b39-b33230438e00", "metadata": {}, "source": ["## 查询重写\n", "\n", "在本节中，我们将向您展示如何将查询重写为多个查询。然后，您可以针对检索器执行所有这些查询。\n", "\n", "这是高级检索技术中的关键步骤。通过进行查询重写，您可以为[集成检索]和[融合]生成多个查询，从而获得更高质量的检索结果。\n", "\n", "与子问题生成器不同，这只是一个提示调用，独立于工具存在。\n"]}, {"cell_type": "markdown", "id": "b38c5be2-fd99-4987-8412-2eda597da412", "metadata": {}, "source": ["### 查询重写（自定义）\n", "\n", "在这里，我们将向您展示如何使用提示来生成多个查询，使用我们的LLM和提示抽象。\n"]}, {"cell_type": "code", "execution_count": null, "id": "d969dfcb-0a2b-4178-87ac-1d0444981f49", "metadata": {}, "outputs": [], "source": ["from llama_index.core import PromptTemplate", "from llama_index.llms.openai import OpenAI", "", "query_gen_str = \"\"\"\\", "您是一个乐于助人的助手，根据单个输入查询生成多个搜索查询。生成{num_queries}个搜索查询，每行一个，与以下输入查询相关：", "查询：{query}", "查询：", "\"\"\"", "query_gen_prompt = PromptTemplate(query_gen_str)", "", "llm = OpenAI(model=\"gpt-3.5-turbo\")", "", "", "def generate_queries(query: str, llm, num_queries: int = 4):", "    response = llm.predict(", "        query_gen_prompt, num_queries=num_queries, query=query", "    )", "    # 假设LLM适当地将每个查询放在一行上", "    queries = response.split(\"\\n\")", "    queries_str = \"\\n\".join(queries)", "    print(f\"生成的查询：\\n{queries_str}\")", "    return queries"]}, {"cell_type": "code", "execution_count": null, "id": "4a4f1ed7-cbd8-4177-9c29-a6ecb2e8a530", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Generated queries:\n", "1. What were the major events or milestones in the history of Interleaf and Viaweb?\n", "2. Who were the founders and key figures involved in the development of Interleaf and Viaweb?\n", "3. What were the products or services offered by Interleaf and Viaweb?\n", "4. Are there any notable success stories or failures associated with Interleaf and Viaweb?\n"]}], "source": ["queries = generate_queries(\"What happened at Interleaf and Viaweb?\", llm)"]}, {"cell_type": "code", "execution_count": null, "id": "f416f6ba-efc8-4e30-84ea-e20466168758", "metadata": {}, "outputs": [{"data": {"text/plain": ["['1. What were the major events or milestones in the history of Interleaf and Viaweb?',\n", " '2. Who were the founders and key figures involved in the development of Interleaf and Viaweb?',\n", " '3. What were the products or services offered by Interleaf and Viaweb?',\n", " '4. Are there any notable success stories or failures associated with Interleaf and Viaweb?']"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["queries"]}, {"cell_type": "markdown", "id": "a8c6947d-8909-40f1-b7f5-565a1ab1c991", "metadata": {}, "source": ["For more details about an e2e implementation with a retriever, check out our guides on our fusion retriever:\n", "- [Module Guide](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion.html)\n", "- [Build a Fusion Retriever from Scratch](https://docs.llamaindex.ai/en/latest/examples/low_level/fusion_retriever.html)\n"]}, {"cell_type": "markdown", "id": "535b3f78-1036-4878-b0fe-44f5330fdb1e", "metadata": {}, "source": ["### 查询重写（使用QueryTransform）\n", "\n", "在本节中，我们将向您展示如何使用我们的QueryTransform类进行查询转换。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3ac0639a-65ae-43cf-a68a-c9705cf7cb12", "metadata": {}, "outputs": [], "source": ["from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n", "from llama_index.llms.openai import OpenAI"]}, {"cell_type": "code", "execution_count": null, "id": "480d58fc-b5c6-41d0-b497-d147a2d1e471", "metadata": {}, "outputs": [], "source": ["hyde = HyDEQueryTransform(include_original=True)\n", "llm = OpenAI(model=\"gpt-3.5-turbo\")\n", "\n", "query_bundle = hyde.run(\"What is Bel?\")"]}, {"cell_type": "markdown", "id": "8e85b339-3279-43e3-83d9-b3d16ce21016", "metadata": {}, "source": ["这将生成一个查询包，其中包含原始查询，还包括表示应该被嵌入的查询的`custom_embedding_strs`。\n"]}, {"cell_type": "code", "execution_count": null, "id": "61994183-0b39-4d9a-ae74-b78f68b8dcfc", "metadata": {}, "outputs": [{"data": {"text/plain": ["['Bel is a term that has multiple meanings and can be interpreted in various ways depending on the context. In ancient Mesopotamian mythology, Bel was a prominent deity and one of the chief gods of the Babylonian pantheon. He was often associated with the sky, storms, and fertility. Bel was considered to be the father of the gods and held great power and authority over the other deities.\\n\\nIn addition to its mythological significance, Bel is also a title that was used to address rulers and leaders in ancient Babylon. It was a term of respect and reverence, similar to the modern-day title of \"king\" or \"emperor.\" The title of Bel was bestowed upon those who held significant political and military power, and it symbolized their authority and dominion over their subjects.\\n\\nFurthermore, Bel is also a common given name in various cultures around the world. It can be found in different forms and variations, such as Belinda, Isabel, or Bella. As a personal name, Bel often carries connotations of beauty, grace, and strength.\\n\\nIn summary, Bel can refer to a powerful deity in ancient Mesopotamian mythology, a title of respect for rulers and leaders, or a personal name with positive attributes. The meaning of Bel can vary depending on the specific context in which it is used.',\n", " 'What is Bel?']"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["new_query.custom_embedding_strs"]}, {"cell_type": "markdown", "id": "f4ee4457-e8dc-4527-b305-9b09fb89c1f7", "metadata": {}, "source": ["## 子问题\n", "\n", "给定一组工具和用户查询，决定要生成的子问题集合，以及每个子问题应该运行的工具。\n", "\n", "我们将通过使用`OpenAIQuestionGenerator`进行示例运行，它依赖于函数调用，还有`LLMQuestionGenerator`，它依赖于提示。\n"]}, {"cell_type": "code", "execution_count": null, "id": "423c9cc6-20eb-4ae3-bca0-c01e606f865a", "metadata": {}, "outputs": [], "source": ["from llama_index.core.question_gen import LLMQuestionGenerator\n", "from llama_index.question_gen.openai import OpenAIQuestionGenerator\n", "from llama_index.llms.openai import OpenAI"]}, {"cell_type": "code", "execution_count": null, "id": "59f47121-fc91-45cd-a071-c3c6208726b7", "metadata": {}, "outputs": [], "source": ["llm = OpenAI()\n", "question_gen = OpenAIQuestionGenerator.from_defaults(llm=llm)"]}, {"cell_type": "code", "execution_count": null, "id": "effdbaca-4087-4199-bc85-ceea00d6edf4", "metadata": {}, "outputs": [{"data": {"text/markdown": ["**Prompt Key**: question_gen_prompt<br>**Text:** <br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["You are a world class state of the art agent.\n", "\n", "You have access to multiple tools, each representing a different data source or API.\n", "Each of the tools has a name and a description, formatted as a JSON dictionary.\n", "The keys of the dictionary are the names of the tools and the values are the descriptions.\n", "Your purpose is to help answer a complex user question by generating a list of sub questions that can be answered by the tools.\n", "\n", "These are the guidelines you consider when completing your task:\n", "* Be as specific as possible\n", "* The sub questions should be relevant to the user question\n", "* The sub questions should be answerable by the tools provided\n", "* You can generate multiple sub questions for each tool\n", "* Tools must be specified by their name, not their description\n", "* You don't need to use a tool if you don't think it's relevant\n", "\n", "Output the list of sub questions by calling the SubQuestionList function.\n", "\n", "## Tools\n", "```json\n", "{tools_str}\n", "```\n", "\n", "## User Question\n", "{query_str}\n", "\n"]}, {"data": {"text/markdown": ["<br><br>"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["display_prompt_dict(question_gen.get_prompts())"]}, {"cell_type": "code", "execution_count": null, "id": "586cf84d-276d-40f3-bb5e-2f5f774b84a1", "metadata": {}, "outputs": [], "source": ["from llama_index.core.tools import ToolMetadata\n", "\n", "tool_choices = [\n", "    ToolMetadata(\n", "        name=\"uber_2021_10k\",\n", "        description=(\n", "            \"Provides information about Uber financials for year 2021\"\n", "        ),\n", "    ),\n", "    ToolMetadata(\n", "        name=\"lyft_2021_10k\",\n", "        description=(\n", "            \"Provides information about Lyft financials for year 2021\"\n", "        ),\n", "    ),\n", "]"]}, {"cell_type": "code", "execution_count": null, "id": "3bf94cf9-a869-420f-a6c2-3f2a0b11365e", "metadata": {}, "outputs": [], "source": ["from llama_index.core import QueryBundle\n", "\n", "query_str = \"Compare and contrast Uber and Lyft\"\n", "choices = question_gen.generate(tool_choices, QueryBundle(query_str=query_str))"]}, {"cell_type": "markdown", "id": "11696baf-4767-4b16-bf41-ea4fe9a3722e", "metadata": {}, "source": ["输出是`SubQuestion` Pydantic对象。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3cc4baa8-db8e-4357-b3fe-d9975f8c7fa1", "metadata": {}, "outputs": [{"data": {"text/plain": ["[SubQuestion(sub_question='What are the financials of Uber for the year 2021?', tool_name='uber_2021_10k'),\n", " SubQuestion(sub_question='What are the financials of Lyft for the year 2021?', tool_name='lyft_2021_10k')]"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["choices"]}, {"cell_type": "markdown", "id": "ff6638a9-92d6-4d99-b024-32c56787e621", "metadata": {}, "source": ["有关如何以更加打包的方式将其插入到您的RAG流水线中的详细信息，请查看我们的[SubQuestionQueryEngine](https://docs.llamaindex.ai/en/latest/examples/query_engine/sub_question_query_engine.html)。\n"]}, {"cell_type": "markdown", "id": "81c9cdc5-e224-4fe8-87c1-06e05d9b5aef", "metadata": {}, "source": ["## 使用ReAct提示进行查询转换\n", "\n", "ReAct是一个流行的代理框架，这里我们展示了如何使用核心ReAct提示来转换查询。\n", "\n", "我们使用`ReActChatFormatter`来获取用于LLM的输入消息集合。\n"]}, {"cell_type": "code", "execution_count": null, "id": "cea5bba4-121f-440c-9407-3416f05028fb", "metadata": {}, "outputs": [], "source": ["from llama_index.core.agent import ReActChatFormatter\n", "from llama_index.core.agent.react.output_parser import ReActOutputParser\n", "from llama_index.core.tools import FunctionTool\n", "from llama_index.core.llms import ChatMessage"]}, {"cell_type": "code", "execution_count": null, "id": "ce0575c5-75aa-4f8d-bd15-2383769d7401", "metadata": {}, "outputs": [], "source": ["def execute_sql(sql: str) -> str:", "    \"\"\"给定一个SQL输入字符串，执行它。\"\"\"", "    # 注意：这是一个模拟函数", "    return f\"执行了 {sql}\"", "", "", "def add(a: int, b: int) -> int:", "    \"\"\"两个数字相加。\"\"\"", "    return a + b", "", "", "tool1 = FunctionTool.from_defaults(fn=execute_sql)", "tool2 = FunctionTool.from_defaults(fn=add)", "tools = [tool1, tool2]"]}, {"cell_type": "markdown", "id": "6d04fb7a-261f-4220-8b3f-821886c0a0c9", "metadata": {}, "source": ["在这里，我们获取输入提示消息，以便传递给LLM。看一下！\n"]}, {"cell_type": "code", "execution_count": null, "id": "3b5f2fa3-ad0f-4685-b9e8-704e77934775", "metadata": {}, "outputs": [{"data": {"text/plain": ["[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Tools\\nYou have access to a wide variety of tools. You are responsible for using\\nthe tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools\\nto complete each subtask.\\n\\nYou have access to the following tools:\\n> Tool Name: execute_sql\\nTool Description: execute_sql(sql: str) -> str\\nGiven a SQL input string, execute it.\\nTool Args: {\\'title\\': \\'execute_sql\\', \\'type\\': \\'object\\', \\'properties\\': {\\'sql\\': {\\'title\\': \\'Sql\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'sql\\']}\\n\\n> Tool Name: add\\nTool Description: add(a: int, b: int) -> int\\nAdd two numbers.\\nTool Args: {\\'title\\': \\'add\\', \\'type\\': \\'object\\', \\'properties\\': {\\'a\\': {\\'title\\': \\'A\\', \\'type\\': \\'integer\\'}, \\'b\\': {\\'title\\': \\'B\\', \\'type\\': \\'integer\\'}}, \\'required\\': [\\'a\\', \\'b\\']}\\n\\n\\n## Output Format\\nTo answer the question, please use the following format.\\n\\n```\\nThought: I need to use a tool to help me answer the question.\\nAction: tool name (one of execute_sql, add) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {\\'input\\': \\'hello world\\', \\'num_beams\\': 5}.\\n\\nIf this format is used, the user will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format until you have enough information\\nto answer the question without using any more tools. At that point, you MUST respond\\nin the one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools.\\nAnswer: [your answer here]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: Sorry, I cannot answer your query.\\n```\\n\\n## Current Conversation\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n\\n', additional_kwargs={}),\n", " ChatMessage(role=<MessageRole.USER: 'user'>, content='Can you find the top three rows from the table named `revenue_years`', additional_kwargs={})]"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["chat_formatter = ReActChatFormatter()\n", "output_parser = ReActOutputParser()\n", "input_msgs = chat_formatter.format(\n", "    tools,\n", "    [\n", "        ChatMessage(\n", "            content=\"Can you find the top three rows from the table named `revenue_years`\",\n", "            role=\"user\",\n", "        )\n", "    ],\n", ")\n", "input_msgs"]}, {"cell_type": "markdown", "id": "4f6ed9b3-aeb8-453b-825a-f711d907cb5a", "metadata": {}, "source": ["接下来我们从模型中获取输出。\n"]}, {"cell_type": "code", "execution_count": null, "id": "f4685858-92d6-4dd9-b23a-5394c97991a8", "metadata": {}, "outputs": [], "source": ["llm = OpenAI(model=\"gpt-4-1106-preview\")"]}, {"cell_type": "code", "execution_count": null, "id": "0f2330aa-7ac7-418f-bb22-cdd1eb081626", "metadata": {}, "outputs": [], "source": ["response = llm.chat(input_msgs)"]}, {"cell_type": "markdown", "id": "785cf896-1dff-49ba-97ba-a7ed6bae0c1d", "metadata": {}, "source": ["最后，我们使用我们的ReActOutputParser将内容解析为结构化输出，并分析动作输入。\n"]}, {"cell_type": "code", "execution_count": null, "id": "1ab1439f-ac69-4e36-97d6-befb3bd6b40e", "metadata": {}, "outputs": [], "source": ["reasoning_step = output_parser.parse(response.message.content)"]}, {"cell_type": "code", "execution_count": null, "id": "bf3ee527-5f34-41e7-8309-2bc4db785f1c", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'sql': 'SELECT * FROM revenue_years ORDER BY revenue DESC LIMIT 3'}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["reasoning_step.action_input"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}