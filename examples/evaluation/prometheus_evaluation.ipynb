{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "7e5f9aa4", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/evaluation/prometheus_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "7dc07d6a", "metadata": {}, "source": ["# 使用 [Prometheus](https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ) 模型进行评估\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "c142b766", "metadata": {}, "source": ["评估是迭代RAG（检索增强生成）管道的关键方面。这个过程严重依赖于GPT-4。然而，一个名为[Prometheus](https://arxiv.org/abs/2310.08491)的新开源模型最近出现作为评估的替代选择。\n", "\n", "在这个笔记本中，我们将演示如何利用Prometheus模型进行评估，并将其与LlamaIndex抽象集成。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "63e301f5", "metadata": {}, "source": ["如果你对Prometheus模型不熟悉，你可能会发现Andrei准备的论文摘要很有启发性。需要注意的是，该模型要求在提示中包含分数以进行有效评估。要获取更详细的信息，你可以参考笔记本中概述的具体提示。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "eb248085", "metadata": {}, "source": ["![Prometheus Paper Card](../data/images/prometheus_paper_card.png)\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "36fc6fe6", "metadata": {}, "source": ["我们将使用Llama数据集中的两个数据集，使用Prometheus模型来演示正确性评估。如果您还没有探索过Llama数据集，我建议花些时间阅读关于它们的信息[这里](https://blog.llamaindex.ai/introducing-llama-datasets-aadb9994ad9e)。\n", "\n", "1. Paul Graham的文章\n", "2. Llama2\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "92616d65", "metadata": {}, "source": ["### 注意：我们在这里展示的是原始的[Prometheus模型](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)用于分析。您可以使用[模型的量化版本](https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ)重新运行分析。\n"]}, {"cell_type": "code", "execution_count": null, "id": "56ae649e", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai\n", "%pip install llama-index-llms-huggingface"]}, {"cell_type": "code", "execution_count": null, "id": "2500d100", "metadata": {}, "outputs": [], "source": ["# 附加到相同的事件循环", "import nest_asyncio", "", "nest_asyncio.apply()"]}, {"attachments": {}, "cell_type": "markdown", "id": "17449837", "metadata": {}, "source": ["## 下载数据集\n"]}, {"cell_type": "code", "execution_count": null, "id": "e70b44ed", "metadata": {}, "outputs": [], "source": ["from llama_index.core.llama_dataset import download_llama_dataset\n", "\n", "paul_graham_rag_dataset, paul_graham_documents = download_llama_dataset(\n", "    \"PaulGrahamEssayDataset\", \"./data/paul_graham\"\n", ")\n", "\n", "llama2_rag_dataset, llama2_documents = download_llama_dataset(\n", "    \"Llama2PaperDataset\", \"./data/llama2\"\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "f08c135c", "metadata": {}, "source": ["## 定义托管在HuggingFace上的Prometheus LLM模型。\n", "\n", "我们使用Nvidia A10G GPU在HF推理端点上托管了该模型。\n"]}, {"cell_type": "code", "execution_count": null, "id": "0e165956", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n"]}], "source": ["from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n", "\n", "HF_TOKEN = \"YOUR HF TOKEN\"\n", "HF_ENDPOINT_URL = (\n", "    \"https://q3yljc2cypyrvw3i.us-east-1.aws.endpoints.huggingface.cloud\"\n", ")\n", "\n", "prometheus_llm = HuggingFaceInferenceAPI(\n", "    model_name=HF_ENDPOINT_URL,\n", "    token=HF_TOKEN,\n", "    temperature=0.1,\n", "    do_sample=True,\n", "    top_p=0.95,\n", "    top_k=40,\n", "    repetition_penalty=1.1,\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "c0679504", "metadata": {}, "source": ["## 提示模板\n", "\n", "我们将使用相同的提示模板来对比Prometheus模型和GPT-4的性能，以确保一致性。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "e522fbf1", "metadata": {}, "source": ["请按照以下要求评估翻译的准确性：\n", "\n", "1. 确保翻译的内容准确无误。\n", "2. 保持原始文档的格式和结构不变。\n"]}, {"cell_type": "code", "execution_count": null, "id": "f8648503", "metadata": {}, "outputs": [], "source": ["prometheus_correctness_eval_prompt_template = \"\"\"###任务描述：提供了一条指令（可能包括其中的一个输入）、一个查询、一个待评估的响应、一个得分为5的参考答案，以及代表评估标准的得分规则。", "\t\t\t1. 撰写一份详细的反馈，严格基于给定的得分规则评估响应的质量，不进行一般性评估。", "\t\t\t2. 撰写反馈后，给出一个得分，可以是1或2或3或4或5。您应参考得分规则。", "\t\t\t3. 输出格式应如下所示：\"反馈：（为标准撰写反馈）[结果]（1或2或3或4或5）\"", "\t\t\t4. 请不要生成任何其他的开头、结尾和解释。", "            5. 只评估生成的答案与参考答案之间的共同点。不要评估参考答案中存在但生成的答案中不存在的内容。", "", "\t\t\t###评估指令：您的任务是评估查询的生成答案和参考答案", "\t\t\t", "            ###待评估的生成答案：{generated_answer} ", "", "            ###参考答案（得分5）：{reference_answer}", "            ", "    \t\t###得分规则： ", "            得分1：如果生成的答案与用户查询和参考答案不相关。", "            得分2：如果生成的答案符合参考答案但与用户查询不相关。", "            得分3：如果生成的答案与用户查询和参考答案相关，但包含错误。", "    \t\t得分4：如果生成的答案与用户查询相关，并且与参考答案具有完全相同的度量，但不够简洁。", "            得分5：如果生成的答案与用户查询相关，并且根据参考答案完全正确。", "    ", "    \t\t###反馈：\"\"\""]}, {"cell_type": "code", "execution_count": null, "id": "b7dd8d3c", "metadata": {}, "outputs": [], "source": ["prometheus_correctness_eval_prompt_template = \"\"\"###任务描述：给定一条指令（可能包括其中的输入），一个查询，一个要评估的响应，一个得分为5的参考答案，以及代表评估标准的得分规则。", "\t\t\t1. 撰写一份详细的反馈，严格基于给定的得分规则评估响应的质量，而不是一般性评估。", "\t\t\t2. 撰写反馈后，给出一个得分，可以是1或2或3或4或5。您应参考得分规则。", "\t\t\t3. 输出格式应如下所示：\"反馈：（为标准撰写反馈）[结果]（1或2或3或4或5）\"", "\t\t\t4. 请不要生成任何其他开头、结尾和解释。", "            5. 只评估生成的答案和参考答案之间的共同之处。不要评估参考答案中存在但生成的答案中不存在的内容。", "", "\t\t\t###评估指令：您的任务是评估查询的生成答案和参考答案", "\t\t\t", "            ###要评估的生成答案：{generated_answer} ", "", "            ###参考答案（得分5）：{reference_answer}", "            ", "    \t\t###得分规则： ", "            得分1：如果生成的答案与用户查询和参考答案不相关。", "            得分2：如果生成的答案根据参考答案是正确的，但与用户查询不相关。", "            得分3：如果生成的答案与用户查询相关，并根据参考答案是正确的，但在事实上有一些错误。", "    \t\t得分4：如果生成的答案与用户查询相关，与参考答案具有完全相同的度量和正确性，但不够简洁。", "            得分5：如果生成的答案与用户查询相关，并且根据参考答案完全正确。", "", "    \t\t###反馈：\"\"\""]}, {"attachments": {}, "cell_type": "markdown", "id": "240e82ad", "metadata": {}, "source": ["### 信实性评估提示\n"]}, {"cell_type": "code", "execution_count": null, "id": "a11597ef", "metadata": {}, "outputs": [], "source": ["prometheus_faithfulness_eval_prompt_template = \"\"\"###任务描述：提供了一条指示（可能包括其中的一个输入）、一条信息、一个上下文，以及代表评估标准的评分规则。", "\t        1. 你将根据信息和上下文提供的评估任务，使用评分规则给出结果。", "            2. 基于评估任务和给定的评分规则撰写详细反馈，而不是一般性评估。", "\t\t\t3. 撰写反馈后，写出一个YES或NO的评分。你应参考评分规则。", "            4. 输出格式应如下所示：“反馈：（为标准撰写反馈）[结果]（YES或NO）”", "            5. 请不要生成任何其他开头、结尾和解释。", "", "        ###评估指示：你的任务是评估所给信息是否得到上下文的支持。", "", "        ###信息：{query_str} ", "", "        ###上下文：{context_str}", "            ", "        ###评分规则： ", "        YES得分：如果所给信息得到上下文的支持。", "        NO得分：如果所给信息未得到上下文的支持。", "    ", "        ###反馈：\"\"\"", "", "prometheus_faithfulness_refine_prompt_template = \"\"\"###任务描述：提供了一条指示（可能包括其中的一个输入）、一条信息、一个上下文信息、一个现有答案，以及代表评估标准的评分规则。", "\t\t\t1. 你将根据信息、上下文信息和现有答案提供的评估任务，使用评分规则给出结果。", "            2. 基于评估任务和给定的评分规则撰写详细反馈，而不是一般性评估。", "\t\t\t3. 撰写反馈后，写出一个YES或NO的评分。你应参考评分规则。", "\t\t\t4. 输出格式应如下所示：“反馈：（为标准撰写反馈）[结果]（YES或NO）”", "\t\t\t5. 请不要生成任何其他开头、结尾和解释。", "", "\t\t\t###评估指示：如果信息在上下文中存在，并且提供了一个现有答案。", "", "\t\t\t###现有答案：{existing_answer} ", "", "            ###信息：{query_str}", "", "            ###上下文：{context_msg}", "            ", "    \t\t###评分规则： ", "            YES得分：如果现有答案已经是YES，或者信息在上下文中存在。", "            NO得分：如果现有答案是NO，并且信息不在上下文中。", "    ", "    \t\t###反馈：\"\"\""]}, {"attachments": {}, "cell_type": "markdown", "id": "00b50daa", "metadata": {}, "source": ["请评估以下内容的相关性：\n", "\n", "给定一个包含学生考试成绩的数据集，评估每个学生的数学成绩与语文成绩之间的相关性。您可以使用任何适当的统计方法来评估这种相关性。\n"]}, {"cell_type": "code", "execution_count": null, "id": "a47f1341", "metadata": {}, "outputs": [], "source": ["prometheus_relevancy_eval_prompt_template = \"\"\"###任务描述：提供了一条指令（可能包括其中的输入）、一个带有响应的查询、上下文和表示评估标准的分数规则。", "            1. 通过查询和响应以及上下文提供的评估任务。", "            2. 基于评估任务和给定的分数规则撰写详细反馈，而不是一般性评估。", "\t\t\t3. 撰写反馈后，写出一个YES或NO的分数。您应参考分数规则。", "            4. 输出格式应如下所示：“反馈：（为标准撰写反馈）[结果]（YES或NO）”", "            5. 请不要生成任何其他开头、结尾和解释。", "", "        ###评估指令：您的任务是评估查询的响应是否符合提供的上下文信息。", "", "        ###查询和响应：{query_str}", "", "        ###上下文：{context_str}", "            ", "        ###分数规则： ", "        分数YES：如果查询的响应与提供的上下文信息一致。", "        分数NO：如果查询的响应与提供的上下文信息不一致。", "    ", "        ###反馈：\"\"\"", "", "prometheus_relevancy_refine_prompt_template = \"\"\"###任务描述：提供了一条指令（可能包括其中的输入）、一个带有响应的查询、上下文、现有答案和表示评估标准的分数规则。", "\t\t\t1. 通过查询和响应以及上下文和现有答案提供的评估任务。", "            2. 基于评估任务和给定的分数规则撰写详细反馈，而不是一般性评估。", "\t\t\t3. 撰写反馈后，写出一个YES或NO的分数。您应参考分数规则。", "\t\t\t4. 输出格式应如下所示：“反馈：（为标准撰写反馈）[结果]（YES或NO）”", "\t\t\t5. 请不要生成任何其他开头、结尾和解释。", "", "\t\t\t###评估指令：您的任务是评估查询的响应是否符合提供的上下文信息。", "", "\t\t\t###查询和响应：{query_str}", "", "            ###上下文：{context_str}", "            ", "    \t\t###分数规则： ", "            分数YES：如果现有答案已经是YES或者查询的响应与提供的上下文信息一致。", "            分数NO：如果现有答案是NO，并且查询的响应与提供的上下文信息一致。", "    ", "    \t\t###反馈：\"\"\""]}, {"attachments": {}, "cell_type": "markdown", "id": "2fb0de17", "metadata": {}, "source": ["请按照以下步骤设置OpenAI密钥以进行索引：\n", "\n", "1. 打开OpenAI网站并登录到您的帐户。\n", "2. 转到API密钥管理页面。\n", "3. 创建一个新的API密钥或使用现有的API密钥。\n", "4. 将API密钥复制粘贴到您的Python文件中的相应位置。\n", "5. 保存文件并重新运行以确保密钥已设置。\n"]}, {"cell_type": "code", "execution_count": null, "id": "c2f9b190", "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"YOUR OPENAI API KEY\"\n", "\n", "from llama_index.llms.openai import OpenAI\n", "\n", "gpt4_llm = OpenAI(\"gpt-4\")"]}, {"attachments": {}, "cell_type": "markdown", "id": "30f967ea", "metadata": {}, "source": ["## 定义解析器函数\n", "\n", "它将用于正确性评估器中。\n"]}, {"cell_type": "code", "execution_count": null, "id": "15db93bd", "metadata": {}, "outputs": [], "source": ["from typing import Tuple", "import re", "", "", "def parser_function(output_str: str) -> Tuple[float, str]:", "    # 用于匹配反馈和响应的模式", "    # 此模式查找以'[RESULT]'结尾的任何文本，后面跟着一个数字", "    pattern = r\"(.+?) \\[RESULT\\] (\\d)\"", "", "    # 使用正则表达式查找所有匹配项", "    matches = re.findall(pattern, output_str)", "", "    # 检查是否找到任何匹配项", "    if matches:", "        # 假设文本中只有一个匹配项，提取反馈和响应", "        feedback, score = matches[0]", "        score = float(score.strip()) if score is not None else score", "        return score, feedback.strip()", "    else:", "        return None, None"]}, {"attachments": {}, "cell_type": "markdown", "id": "c72df564", "metadata": {}, "source": ["## 定义正确性、忠实度、相关性评估器\n"]}, {"cell_type": "code", "execution_count": null, "id": "ea574259", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import (", "    CorrectnessEvaluator,  # 正确性评估器", "    FaithfulnessEvaluator,  # 忠实度评估器", "    RelevancyEvaluator,  # 相关性评估器", ")", "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler", "import tiktoken", "", "", "# 使用Prometheus模型的CorrectnessEvaluator", "prometheus_correctness_evaluator = CorrectnessEvaluator(", "    llm=prometheus_llm,", "    parser_function=parser_function,", "    eval_template=prometheus_correctness_eval_prompt_template,", ")", "", "# 使用Prometheus模型的FaithfulnessEvaluator", "prometheus_faithfulness_evaluator = FaithfulnessEvaluator(", "    llm=prometheus_llm,", "    eval_template=prometheus_faithfulness_eval_prompt_template,", "    refine_template=prometheus_faithfulness_refine_prompt_template,", ")", "", "# 使用Prometheus模型的RelevancyEvaluator", "prometheus_relevancy_evaluator = RelevancyEvaluator(", "    llm=prometheus_llm,", "    eval_template=prometheus_relevancy_eval_prompt_template,", "    refine_template=prometheus_relevancy_refine_prompt_template,", ")", "", "# 将编码模型设置为`gpt-4`以进行标记计数", "token_counter = TokenCountingHandler(", "    tokenizer=tiktoken.encoding_for_model(\"gpt-4\").encode", ")", "", "callback_manager = CallbackManager([token_counter])", "gpt4_llm.callback_manager = callback_manager", "", "# 使用GPT-4模型的CorrectnessEvaluator", "gpt4_correctness_evaluator = CorrectnessEvaluator(", "    llm=gpt4_llm,", "    # parser_function=parser_function,", ")", "", "# 使用GPT-4模型的FaithfulnessEvaluator", "gpt4_faithfulness_evaluator = FaithfulnessEvaluator(", "    llm=gpt4_llm,", "    eval_template=prometheus_faithfulness_eval_prompt_template,", "    refine_template=prometheus_faithfulness_refine_prompt_template,", ")", "", "# 使用GPT-4模型的RelevancyEvaluator", "gpt4_relevancy_evaluator = RelevancyEvaluator(", "    llm=gpt4_llm,", "    eval_template=prometheus_relevancy_eval_prompt_template,", "    refine_template=prometheus_relevancy_refine_prompt_template,", ")", "", "# 创建评估器字典", "prometheus_evaluators = {", "    \"correctness\": prometheus_correctness_evaluator,", "    \"faithfulness\": prometheus_faithfulness_evaluator,", "    \"relevancy\": prometheus_relevancy_evaluator,", "}", "", "gpt4_evaluators = {", "    \"correctness\": gpt4_correctness_evaluator,", "    \"faithfulness\": gpt4_faithfulness_evaluator,", "    \"relevancy\": gpt4_relevancy_evaluator,", "}"]}, {"attachments": {}, "cell_type": "markdown", "id": "0d14dac0", "metadata": {}, "source": ["## 让我们创建一个函数来为不同的数据集创建`query_engine`和`rag_dataset`。\n"]}, {"cell_type": "code", "execution_count": null, "id": "b7c99c1f", "metadata": {}, "outputs": [], "source": ["from llama_index.core.llama_dataset import LabelledRagDataset\n", "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n", "\n", "\n", "def create_query_engine_rag_dataset(dataset_path):\n", "    rag_dataset = LabelledRagDataset.from_json(\n", "        f\"{dataset_path}/rag_dataset.json\"\n", "    )\n", "    documents = SimpleDirectoryReader(\n", "        input_dir=f\"{dataset_path}/source_files\"\n", "    ).load_data()\n", "\n", "    index = VectorStoreIndex.from_documents(documents=documents)\n", "    query_engine = index.as_query_engine()\n", "\n", "    return query_engine, rag_dataset"]}, {"attachments": {}, "cell_type": "markdown", "id": "03925a73", "metadata": {}, "source": ["## 运行批量评估的函数定义\n", "\n", "这个函数用于在定义的评估器上运行批量评估。\n"]}, {"cell_type": "code", "execution_count": null, "id": "7a106a98", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import BatchEvalRunner\n", "\n", "\n", "async def batch_eval_runner(\n", "    evaluators, query_engine, questions, reference=None, num_workers=8\n", "):\n", "    batch_runner = BatchEvalRunner(\n", "        evaluators, workers=num_workers, show_progress=True\n", "    )\n", "\n", "    eval_results = await batch_runner.aevaluate_queries(\n", "        query_engine, queries=questions, reference=reference\n", "    )\n", "\n", "    return eval_results"]}, {"attachments": {}, "cell_type": "markdown", "id": "a5a5b50e", "metadata": {}, "source": ["## 检查分数分布的函数\n"]}, {"cell_type": "code", "execution_count": null, "id": "2c5d39f8", "metadata": {}, "outputs": [], "source": ["from collections import Counter", "from typing import List, Dict", "", "", "def get_scores_distribution(scores: List[float]) -> Dict[str, float]:", "    # 统计每个分数的出现次数", "    score_counts = Counter(scores)", "", "    # 分数的总数", "    total_scores = len(scores)", "", "    # 计算百分比分布", "    percentage_distribution = {", "        score: (count / total_scores) * 100", "        for score, count in score_counts.items()", "    }", "", "    return percentage_distribution"]}, {"attachments": {}, "cell_type": "markdown", "id": "4c8a631a", "metadata": {}, "source": ["## 用于检查正确性、忠实度和相关性评分的函数\n"]}, {"cell_type": "code", "execution_count": null, "id": "206f150e", "metadata": {}, "outputs": [], "source": ["def get_eval_results(key, eval_results):\n", "    results = eval_results[key]\n", "    correct = 0\n", "    for result in results:\n", "        if result.passing:\n", "            correct += 1\n", "    score = correct / len(results)\n", "    print(f\"{key} Score: {round(score, 2)}\")\n", "    return score"]}, {"attachments": {}, "cell_type": "markdown", "id": "379f5bbd", "metadata": {}, "source": ["## 计算`汉明距离`的函数。\n"]}, {"cell_type": "code", "execution_count": null, "id": "f066e8a5", "metadata": {}, "outputs": [], "source": ["def hamming_distance(list1, list2):\n", "    if len(list1) != len(list2):\n", "        raise ValueError(\"Lists must be of the same length\")\n", "    return sum(el1 != el2 for el1, el2 in zip(list1, list2))"]}, {"attachments": {}, "cell_type": "markdown", "id": "2300f843", "metadata": {}, "source": ["## 对PaulGraham文章文本的评估\n"]}, {"cell_type": "code", "execution_count": null, "id": "4dae4d9b-66d8-4e30-be53-aae3b4d01343", "metadata": {}, "outputs": [], "source": ["query_engine, rag_dataset = create_query_engine_rag_dataset(\n", "    \"./data/paul_graham\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "bec19307", "metadata": {}, "outputs": [], "source": ["# 获取用于评估的问题", "questions = [example.query for example in rag_dataset.examples]", "", "# 获取用于评估的参考答案", "reference = [[example.reference_answer] for example in rag_dataset.examples]"]}, {"attachments": {}, "cell_type": "markdown", "id": "16b9ea6e", "metadata": {}, "source": ["### 计算正确性、忠实度和相关性评估\n"]}, {"cell_type": "code", "execution_count": null, "id": "59d81f10", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|██████████| 44/44 [00:30<00:00,  1.43it/s]\n", "100%|██████████| 132/132 [01:56<00:00,  1.13it/s]\n"]}], "source": ["prometheus_eval_results = await batch_eval_runner(\n", "    prometheus_evaluators, query_engine, questions, reference\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "57d67e63", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|██████████| 44/44 [00:26<00:00,  1.66it/s]\n", "100%|██████████| 132/132 [02:32<00:00,  1.16s/it]\n"]}], "source": ["gpt4_eval_results = await batch_eval_runner(\n", "    gpt4_evaluators, query_engine, questions, reference\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "cf51af00", "metadata": {}, "source": ["### 使用Prometheus评估器进行正确性评估得分分布。\n"]}, {"cell_type": "code", "execution_count": null, "id": "4c06baac", "metadata": {}, "outputs": [{"data": {"text/plain": ["{3.0: 50.0,\n", " 1.0: 43.18181818181818,\n", " 5.0: 2.272727272727273,\n", " 4.0: 4.545454545454546}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["prometheus_scores = [\n", "    result.score for result in prometheus_eval_results[\"correctness\"]\n", "]\n", "get_scores_distribution(prometheus_scores)"]}, {"attachments": {}, "cell_type": "markdown", "id": "e616d8ce", "metadata": {}, "source": ["### 使用GPT-4评估器进行正确性评估得分分布。\n"]}, {"cell_type": "code", "execution_count": null, "id": "e958e2ab", "metadata": {}, "outputs": [{"data": {"text/plain": ["{4.5: 50.0,\n", " 5.0: 34.090909090909086,\n", " 2.5: 9.090909090909092,\n", " 4.0: 2.272727272727273,\n", " 3.5: 4.545454545454546}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["gpt4_scores = [result.score for result in gpt4_eval_results[\"correctness\"]]\n", "get_scores_distribution(gpt4_scores)"]}, {"attachments": {}, "cell_type": "markdown", "id": "1b61b75d", "metadata": {}, "source": ["### Prometheus和GPT-4的反馈比较\n", "\n", "在这里，我们将比较Prometheus和GPT-4两个系统的反馈机制。\n", "\n", "#### Prometheus反馈\n", "- Prometheus是一个开源的监控系统，它提供了丰富的数据模型和查询语言，可以用于实时监控和警报。\n", "- Prometheus的反馈主要集中在系统的性能指标和运行状况，例如CPU利用率、内存使用情况、请求延迟等。\n", "- Prometheus通过收集和分析这些指标数据，可以帮助用户了解系统的运行情况，并及时发现和解决问题。\n", "\n", "#### GPT-4反馈\n", "- GPT-4是由OpenAI开发的自然语言处理模型，具有强大的文本生成能力，可以用于生成文章、对话等。\n", "- GPT-4的反馈主要体现在生成的文本质量和逻辑连贯性上，用户可以根据生成的内容来评估模型的表现。\n", "- GPT-4通过不断的训练和优化，可以提供更准确、更自然的文本生成，从而改善用户的体验。\n", "\n", "总的来说，Prometheus和GPT-4的反馈机制针对的是不同的应用场景，分别关注系统性能和文本生成质量，用户可以根据自己的需求选择合适的系统来满足特定的需求。\n"]}, {"cell_type": "code", "execution_count": null, "id": "116c132e", "metadata": {}, "outputs": [], "source": ["查询 = prometheus_eval_results[\"correctness\"][0].查询", "响应 = prometheus_eval_results[\"correctness\"][0].响应", "参考答案 = 参考[0][0]", "", "# prometheus反馈和得分", "prometheus反馈 = prometheus_eval_results[\"correctness\"][0].反馈", "prometheus得分 = prometheus_eval_results[\"correctness\"][0].得分", "", "# GPT4反馈和得分", "gpt4反馈 = gpt4_eval_results[\"correctness\"][0].反馈", "gpt4得分 = gpt4_eval_results[\"correctness\"][0].得分"]}, {"cell_type": "code", "execution_count": null, "id": "a44412ed", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Query: In the essay, the author mentions his early experiences with programming. Describe the first computer he used for programming, the language he used, and the challenges he faced. \n", "\n", "\n", "Generated Answer: The author mentions that the first computer he used for programming was the IBM 1401, which was located in the basement of his junior high school. He used an early version of Fortran as the programming language. The author faced challenges in figuring out what to do with the computer, as the only form of input was data stored on punched cards, and he didn't have any. Additionally, he didn't know enough math to do anything interesting with the computer. \n", "\n", "\n", "Reference Answer: The first computer the author used for programming was the IBM 1401, which was used by his school district for data processing. He started using it in 9th grade, around the age of 13 or 14. The programming language he used was an early version of Fortran. The author faced several challenges while using this computer. The only form of input to programs was data stored on punched cards, and he didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but he didn't know enough math to do anything interesting of that type. Therefore, he couldn't figure out what to do with it and in retrospect, he believes there's not much he could have done with it. \n", "\n", "\n", "Prometheus Feedback: The generated response is relevant to the user query and correctly describes the first computer the author used for programming, the programming language he used, and the challenges he faced. However, it has some inaccuracies in the details. The author did not use the IBM 1401 in the basement of his junior high school, but rather in 9th grade, around the age of 13 or 14. The author did not have any data stored on punched cards, but the only form of input was data stored on punched cards. The author did not know enough math to do anything interesting with the computer, but he didn't know enough math to do anything interesting of that type. So the overall score is 3. \n", "\n", " 3.0 \n", "\n", "\n", "GPT-4 Feedback: The generated answer is highly relevant and almost completely accurate. It correctly identifies the first computer the author used (IBM 1401), the programming language (Fortran), and the challenges he faced (lack of input data and insufficient math knowledge). However, it omits the detail about the author's age and grade level when he started programming, which was included in the reference answer. \n", "\n", " 4.5\n"]}], "source": ["print(f\"Query: {query} \\n\\n\")\n", "print(f\"Generated Answer: {response} \\n\\n\")\n", "print(f\"Reference Answer: {reference_answer} \\n\\n\")\n", "print(\n", "    f\"Prometheus Feedback: {prometheus_feedback} \\n\\n {prometheus_score} \\n\\n\"\n", ")\n", "print(f\"GPT-4 Feedback: {gpt4_feedback} \\n\\n {gpt4_score}\")"]}, {"attachments": {}, "cell_type": "markdown", "id": "efb16b02", "metadata": {}, "source": ["#### 观察：\n", "\n", "普罗米修斯的反馈更加详细，指出生成的回复中省略了某些具体细节，导致得分为`3.0`。相比之下，GPT-4的反馈更加宽泛，不太具体，尽管缺少一些细节，但仍给出了`5.0`的评分。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "2d7cdab4", "metadata": {}, "source": ["### Prometheus忠实度和相关性评分。\n"]}, {"cell_type": "code", "execution_count": null, "id": "bd9fb039", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["faithfulness Score: 0.75\n", "relevancy Score: 0.86\n"]}], "source": ["_ = get_eval_results(\"faithfulness\", prometheus_eval_results)\n", "\n", "_ = get_eval_results(\"relevancy\", prometheus_eval_results)"]}, {"attachments": {}, "cell_type": "markdown", "id": "026371fa", "metadata": {}, "source": ["### GPT-4忠实度和相关性评分。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6bf36c1a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["faithfulness Score: 0.98\n", "relevancy Score: 0.95\n"]}], "source": ["_ = get_eval_results(\"faithfulness\", gpt4_eval_results)\n", "\n", "_ = get_eval_results(\"relevancy\", gpt4_eval_results)"]}, {"attachments": {}, "cell_type": "markdown", "id": "13154a8b", "metadata": {}, "source": ["### 普罗米修斯和GPT-4之间的汉明距离比较\n", "\n", "（数值越低越好）\n"]}, {"cell_type": "code", "execution_count": null, "id": "0b4bbb3e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Faithfulness Hamming Distance: 10\n", "Relevancy Hamming Distance: 8\n"]}], "source": ["prometheus_faithfulness_scores = [\n", "    result.score for result in prometheus_eval_results[\"faithfulness\"]\n", "]\n", "prometheus_relevancy_scores = [\n", "    result.score for result in prometheus_eval_results[\"relevancy\"]\n", "]\n", "\n", "gpt4_faithfulness_scores = [\n", "    result.score for result in gpt4_eval_results[\"faithfulness\"]\n", "]\n", "gpt4_relevancy_scores = [\n", "    result.score for result in gpt4_eval_results[\"relevancy\"]\n", "]\n", "\n", "faithfulness_hamming_distance = hamming_distance(\n", "    prometheus_faithfulness_scores, gpt4_faithfulness_scores\n", ")\n", "relevancy_hamming_distance = hamming_distance(\n", "    prometheus_relevancy_scores, gpt4_relevancy_scores\n", ")\n", "\n", "print(f\"Faithfulness Hamming Distance: {faithfulness_hamming_distance}\")\n", "print(f\"Relevancy Hamming Distance: {relevancy_hamming_distance}\")"]}, {"attachments": {}, "cell_type": "markdown", "id": "7c73b2fc", "metadata": {}, "source": ["#### 观察：\n", "\n", "比较显示，在信实性和相关性方面，Prometheus和GPT-4评估中约`77%`和`81%`的分数是相同的。这表明在信实性和相关性评分方面，Prometheus和GPT-4模型之间存在相当的相关性。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "04b94dfe", "metadata": {}, "source": ["### GPT-4 成本分析\n"]}, {"cell_type": "code", "execution_count": null, "id": "38fda419", "metadata": {}, "outputs": [], "source": ["prompt_token_count = token_counter.prompt_llm_token_count\n", "completion_token_count = token_counter.completion_llm_token_count\n", "\n", "total_cost_paul_graham_essay = (\n", "    prompt_token_count * 0.03 + completion_token_count * 0.06\n", ") / 1000\n", "\n", "token_counter.reset_counts()"]}, {"attachments": {}, "cell_type": "markdown", "id": "d8349f1a", "metadata": {}, "source": ["## 使用 Llama2 论文进行评估\n"]}, {"cell_type": "code", "execution_count": null, "id": "ea3a310a", "metadata": {}, "outputs": [], "source": ["query_engine, rag_dataset = create_query_engine_rag_dataset(\"./data/llama2\")"]}, {"cell_type": "code", "execution_count": null, "id": "48cc5c83", "metadata": {}, "outputs": [], "source": ["questions = [example.query for example in rag_dataset.examples]"]}, {"cell_type": "code", "execution_count": null, "id": "05131d13", "metadata": {}, "outputs": [], "source": ["reference = [[example.reference_answer] for example in rag_dataset.examples]"]}, {"attachments": {}, "cell_type": "markdown", "id": "61c09098", "metadata": {}, "source": ["### 计算正确性、忠实度和相关性评估\n"]}, {"cell_type": "code", "execution_count": null, "id": "757dec6c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|██████████| 100/100 [01:02<00:00,  1.61it/s]\n", "100%|██████████| 300/300 [04:34<00:00,  1.09it/s]\n"]}], "source": ["prometheus_eval_results = await batch_eval_runner(\n", "    prometheus_evaluators, query_engine, questions, reference\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "7214c768", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100%|██████████| 100/100 [01:06<00:00,  1.51it/s]\n", "100%|██████████| 300/300 [06:22<00:00,  1.27s/it]\n"]}], "source": ["gpt4_eval_results = await batch_eval_runner(\n", "    gpt4_evaluators, query_engine, questions, reference\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "id": "a842f4e3", "metadata": {}, "source": ["### 使用Prometheus评估器进行正确性评估得分分布。\n"]}, {"cell_type": "code", "execution_count": null, "id": "4f4e483c", "metadata": {}, "outputs": [{"data": {"text/plain": ["{3.0: 56.00000000000001, 1.0: 26.0, 5.0: 9.0, 4.0: 8.0, 2.0: 1.0}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["prometheus_scores = [\n", "    result.score for result in prometheus_eval_results[\"correctness\"]\n", "]\n", "get_scores_distribution(prometheus_scores)"]}, {"attachments": {}, "cell_type": "markdown", "id": "f26abdfd", "metadata": {}, "source": ["### 使用GPT-4评估器进行正确性评估得分分布。\n"]}, {"cell_type": "code", "execution_count": null, "id": "5dd97ed0", "metadata": {}, "outputs": [{"data": {"text/plain": ["{4.5: 57.99999999999999,\n", " 1.0: 6.0,\n", " 4.0: 12.0,\n", " 5.0: 10.0,\n", " 2.0: 5.0,\n", " 3.5: 5.0,\n", " 2.5: 3.0,\n", " 3.0: 1.0}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["gpt4_scores = [result.score for result in gpt4_eval_results[\"correctness\"]]\n", "get_scores_distribution(gpt4_scores)"]}, {"attachments": {}, "cell_type": "markdown", "id": "b957e0d0", "metadata": {}, "source": ["### 比较Prometheus和GPT-4在正确性方面的反馈。\n"]}, {"cell_type": "code", "execution_count": null, "id": "d110a22c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Query: Based on the abstract of \"Llama 2: Open Foundation and Fine-Tuned Chat Models,\" what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? \n", "\n", "\n", "Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion. \n", "\n", "\n", "Reference Answer: The two primary objectives achieved in the work described in the abstract of \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" are:\n", "\n", "1. The development and release of a collection of pretrained and fine-tuned large language models (LLMs) specifically optimized for dialogue use cases.\n", "2. The demonstration that these fine-tuned LLMs, referred to as Llama 2-Chat, outperform open-source chat models on most benchmarks tested and may be a suitable substitute for closed-source models, particularly in terms of helpfulness and safety based on human evaluations.\n", "\n", "The range of parameters for the large language models developed in this work is from 7 billion to 70 billion parameters. \n", "\n", "\n", "Prometheus Feedback: The generated response is relevant to the user query and correctly identifies the two primary objectives of the work described in the abstract of \"Llama  2: Open Foundation and Fine-Tuned Chat Models.\" However, it does not mention the demonstration of the fine-tuned LLMs outperforming open-source chat models on most benchmarks tested, which is a key point in the reference response. The range of parameters for the large language models developed is correctly identified, but the response does not mention the specific models referred to as Llama 2-Chat. So the overall score is 3. \n", "\n", " 3.0 \n", "\n", "\n", "GPT-4 Feedback: The generated answer is relevant and almost fully correct. It correctly identifies the two primary objectives and the range of parameters for the large language models. However, it misses the detail about Llama 2-Chat outperforming other models on most benchmarks and potentially being a suitable substitute for closed-source models. \n", "\n", " 4.5\n"]}], "source": ["查询 = prometheus_eval_results[\"correctness\"][0].query", "响应 = prometheus_eval_results[\"correctness\"][0].response", "参考答案 = reference[0][0]", "", "# Prometheus反馈和得分", "prometheus反馈 = prometheus_eval_results[\"correctness\"][0].feedback", "prometheus得分 = prometheus_eval_results[\"correctness\"][0].score", "", "# GPT4反馈和得分", "gpt4反馈 = gpt4_eval_results[\"correctness\"][0].feedback", "gpt4得分 = gpt4_eval_results[\"correctness\"][0].score", "", "print(f\"查询: {查询} \\n\\n\")", "print(f\"生成的答案: {响应} \\n\\n\")", "print(f\"参考答案: {参考答案} \\n\\n\")", "print(", "    f\"Prometheus反馈: {prometheus反馈} \\n\\n {prometheus得分} \\n\\n\"", ")", "print(f\"GPT-4反馈: {gpt4反馈} \\n\\n {gpt4得分}\")"]}, {"attachments": {}, "cell_type": "markdown", "id": "2be4aebd", "metadata": {}, "source": ["#### 观察：\n", "\n", "与GPT-4相比，Prometheus的反馈更加精确，它给出了一个`3.0`的惩罚分数，而GPT-4给出了一个`4.5`的分数。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "8ea4b274", "metadata": {}, "source": ["### Prometheus忠实度和相关性评分。\n"]}, {"cell_type": "code", "execution_count": null, "id": "53d4e599", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["faithfulness Score: 0.39\n", "relevancy Score: 0.57\n"]}], "source": ["_ = get_eval_results(\"faithfulness\", prometheus_eval_results)\n", "\n", "_ = get_eval_results(\"relevancy\", prometheus_eval_results)"]}, {"attachments": {}, "cell_type": "markdown", "id": "74c5a9bf", "metadata": {}, "source": ["### GPT-4忠实度和相关性评分。\n"]}, {"cell_type": "code", "execution_count": null, "id": "a3c975d1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["faithfulness Score: 0.93\n", "relevancy Score: 0.98\n"]}], "source": ["_ = get_eval_results(\"faithfulness\", gpt4_eval_results)\n", "\n", "_ = get_eval_results(\"relevancy\", gpt4_eval_results)"]}, {"attachments": {}, "cell_type": "markdown", "id": "8c198e09", "metadata": {}, "source": ["在这个示例中，我们将计算普罗米修斯和 GPT-4 之间的汉明距离。汉明距离是衡量两个等长字符串之间的不同之处的数量的指标。让我们看看普罗米修斯和 GPT-4 之间的汉明距离。\n"]}, {"cell_type": "code", "execution_count": null, "id": "4d9f55cc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Faithfulness Hamming Distance: 58\n", "Relevancy Hamming Distance: 41\n"]}], "source": ["prometheus_faithfulness_scores = [\n", "    result.score for result in prometheus_eval_results[\"faithfulness\"]\n", "]\n", "prometheus_relevancy_scores = [\n", "    result.score for result in prometheus_eval_results[\"relevancy\"]\n", "]\n", "\n", "gpt4_faithfulness_scores = [\n", "    result.score for result in gpt4_eval_results[\"faithfulness\"]\n", "]\n", "gpt4_relevancy_scores = [\n", "    result.score for result in gpt4_eval_results[\"relevancy\"]\n", "]\n", "\n", "faithfulness_hamming_distance = hamming_distance(\n", "    prometheus_faithfulness_scores, gpt4_faithfulness_scores\n", ")\n", "relevancy_hamming_distance = hamming_distance(\n", "    prometheus_relevancy_scores, gpt4_relevancy_scores\n", ")\n", "\n", "print(f\"Faithfulness Hamming Distance: {faithfulness_hamming_distance}\")\n", "print(f\"Relevancy Hamming Distance: {relevancy_hamming_distance}\")"]}, {"attachments": {}, "cell_type": "markdown", "id": "ab1dc805", "metadata": {}, "source": ["#### 观察：\n", "\n", "比较显示，在“忠实度”方面约有`44%`的分数，在“相关性”方面约有`63%`的分数在Prometheus和GPT-4评估中是相同的。这表明在忠实度和相关性评分方面，Prometheus和GPT-4模型之间存在相当大的相关性。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "482e92ba", "metadata": {}, "source": ["### Prometheus和GPT-4的忠实度和相关性反馈比较\n"]}, {"cell_type": "code", "execution_count": null, "id": "0809503d", "metadata": {}, "outputs": [], "source": ["# 获取查询", "query = questions[0]", "", "# 获取查询的响应/生成的答案", "response = prometheus_eval_results[\"faithfulness\"][0].response", "# 获取检索到的上下文，因为它们用于忠实度和相关性", "contexts = prometheus_eval_results[\"faithfulness\"][0].contexts", "", "# 从prometheus模型获取忠实度和相关性反馈", "prometheus_faithfulness_feedback = prometheus_eval_results[\"faithfulness\"][", "    0", "].feedback", "prometheus_relevancy_feedback = prometheus_eval_results[\"relevancy\"][", "    0", "].feedback", "", "# 从gpt4模型获取忠实度和相关性反馈", "gpt4_faithfulness_feedback = gpt4_eval_results[\"faithfulness\"][0].feedback", "gpt4_relevancy_feedback = gpt4_eval_results[\"relevancy\"][0].feedback", "", "# 从prometheus模型获取忠实度和相关性分数", "prometheus_faithfulness_score = prometheus_eval_results[\"faithfulness\"][", "    0", "].score", "prometheus_relevancy_score = prometheus_eval_results[\"relevancy\"][0].score", "", "# 从gpt4模型获取忠实度和相关性分数", "gpt4_faithfulness_score = gpt4_eval_results[\"faithfulness\"][0].score", "gpt4_relevancy_score = gpt4_eval_results[\"relevancy\"][0].score"]}, {"cell_type": "code", "execution_count": null, "id": "9275263f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Query: Based on the abstract of \"Llama 2: Open Foundation and Fine-Tuned Chat Models,\" what are the two primary objectives achieved in this work, and what is the range of parameters for the large language models developed? \n", "\n", "\n", "Generated Answer: The two primary objectives achieved in this work are the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. The range of parameters for the large language models developed is from 7 billion to 70 billion.\n"]}], "source": ["print(f\"Query: {query} \\n\\n\")\n", "print(f\"Generated Answer: {response}\")"]}, {"cell_type": "code", "execution_count": null, "id": "19c3d47e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Context-1: Llama 2 : Open Foundation and Fine-Tuned Chat Models\n", "Hugo Touvron∗Louis Martin†Kevin Stone†\n", "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n", "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n", "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n", "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n", "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n", "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n", "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n", "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n", "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n", "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n", "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n", "Sergey Edunov Thomas Scialom∗\n", "GenAI, Meta\n", "Abstract\n", "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n", "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n", "Our fine-tuned LLMs, called Llama 2-Chat , are optimized for dialogue use cases. Our\n", "models outperform open-source chat models on most benchmarks we tested, and based on\n", "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosed-\n", "source models. We provide a detailed description of our approach to fine-tuning and safety\n", "improvements of Llama 2-Chat in order to enable the community to build on our work and\n", "contribute to the responsible development of LLMs.\n", "∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n", "†Second author\n", "Contributions for all the authors can be found in Section A.1.arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"]}], "source": ["print(f\"Context-1: {contexts[0]}\")"]}, {"cell_type": "code", "execution_count": null, "id": "cd28e3d3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Context-2: (2021)alsoilluminatesthedifficultiestiedtochatbot-oriented\n", "LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes\n", "a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between\n", "potential positive and negative impacts from releasing dialogue models.\n", "InvestigationsintoredteamingrevealspecificchallengesintunedLLMs,withstudiesbyGangulietal.(2022)\n", "and Zhuoet al. (2023) showcasing a variety ofsuccessful attack typesand their effects onthe generation of\n", "harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also\n", "raisedredflagsaroundadvancedemergentmodelbehaviors,cyberthreats,andpotentialmisuseinareaslike\n", "biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an\n", "over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu\n", "andRestrepo,2018;AutorandSalomons,2018;Webb,2019;Shumailovetal.,2023). Wearecommittedto\n", "continuing our work engaging with the broader policy, academic, and industry community on these issues.\n", "7 Conclusion\n", "Inthisstudy,wehaveintroduced Llama 2,anewfamilyofpretrainedandfine-tunedmodelswithscales\n", "of7billionto70billionparameters. Thesemodelshavedemonstratedtheircompetitivenesswithexisting\n", "open-source chat models, as well as competency that is equivalent to some proprietary models on evaluation\n", "setsweexamined,althoughtheystilllagbehindothermodelslikeGPT-4. Wemeticulouslyelaboratedonthe\n", "methodsandtechniquesappliedinachievingourmodels,withaheavyemphasisontheiralignmentwiththe\n", "principlesofhelpfulnessandsafety. Tocontributemoresignificantlytosocietyandfosterthepaceofresearch,\n", "wehaveresponsiblyopenedaccessto Llama 2 andLlama 2-Chat . Aspartofourongoingcommitmentto\n", "transparency and safety, we plan to make further improvements to Llama 2-Chat in future work.\n", "36\n"]}], "source": ["print(f\"Context-2: {contexts[1]}\")"]}, {"cell_type": "code", "execution_count": null, "id": "b80a38b8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Prometheus Faithfulness Feedback: \n", "        The information provided in the context is not supported by the given information. The context is about the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. However, the information provided in the context does not align with the given information. The context does not mention the range of parameters for the large language models developed, which is the primary objective mentioned in the information. The context only talks about the development and release of Llama 2 and its optimization for dialogue use cases, but it does not provide any information about the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO\n", "\n", "\n", "Prometheus Faithfulness Score: 0.0\n", "\n", "\n", "Prometheus Relevancy Feedback: \n", "        The response is not in line with the context information provided. The query asked for the two primary objectives achieved in the work and the range of parameters for the large language models developed. However, the response provided the abstract of the paper and mentioned the authors, which is not relevant to the query. The response also did not mention the two primary objectives achieved in the work or the range of parameters for the large language models developed. So the overall score is NO. [RESULT] NO\n", "\n", "\n", "Prometheus Relevancy Score: 0.0\n"]}], "source": ["print(\n", "    f\"Prometheus Faithfulness Feedback: {prometheus_faithfulness_feedback}\\n\\n\"\n", ")\n", "print(f\"Prometheus Faithfulness Score: {prometheus_faithfulness_score}\\n\\n\")\n", "print(f\"Prometheus Relevancy Feedback: {prometheus_relevancy_feedback}\\n\\n\")\n", "print(f\"Prometheus Relevancy Score: {prometheus_relevancy_score}\")"]}, {"attachments": {}, "cell_type": "markdown", "id": "f0c2eb9e", "metadata": {}, "source": ["如果你比较反馈和上下文，你会发现上下文和回复中提到了一系列参数，但是反馈表示模型找不到这样的信息。\n"]}, {"cell_type": "code", "execution_count": null, "id": "fee1c96e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["GPT-4 Faithfulness Feedback: The given piece of information is well supported by the context. The context clearly states that Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), was developed and released. It also mentions that these models range in scale from 7 billion to 70 billion parameters. Furthermore, the context confirms that these models are optimized for dialogue use cases. Therefore, the information provided is accurate and is corroborated by the context. [RESULT] YES\n", "\n", "\n", "GPT-4 Faithfulness Score: 1.0\n", "\n", "\n", "GPT-4 Relevancy Feedback: The response accurately reflects the context provided. The response correctly identifies the two primary objectives of the work as the development and release of Llama 2, a collection of pretrained and fine-tuned large language models (LLMs), and the optimization of these models for dialogue use cases. This is in line with the information provided in the abstract of the context. The response also correctly states the range of parameters for the large language models developed as being from 7 billion to 70 billion, which is also confirmed in the context. Therefore, the response is in line with the context information provided. [RESULT] YES\n", "\n", "\n", "GPT-4 Relevancy Score: 1.0\n"]}], "source": ["print(f\"GPT-4 Faithfulness Feedback: {gpt4_faithfulness_feedback}\\n\\n\")\n", "print(f\"GPT-4 Faithfulness Score: {gpt4_faithfulness_score}\\n\\n\")\n", "print(f\"GPT-4 Relevancy Feedback: {gpt4_relevancy_feedback}\\n\\n\")\n", "print(f\"GPT-4 Relevancy Score: {gpt4_relevancy_score}\")"]}, {"attachments": {}, "cell_type": "markdown", "id": "3973f128", "metadata": {}, "source": ["#### GPT-4能够正确评估，而Prometheus模型则不能。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "0534ff6c", "metadata": {}, "source": ["### GPT-4 成本分析\n"]}, {"cell_type": "code", "execution_count": null, "id": "0feaad6d", "metadata": {}, "outputs": [], "source": ["prompt_token_count = token_counter.prompt_llm_token_count\n", "completion_token_count = token_counter.completion_llm_token_count\n", "\n", "total_cost_llama2 = (\n", "    prompt_token_count * 0.03 + completion_token_count * 0.06\n", ") / 1000"]}, {"attachments": {}, "cell_type": "markdown", "id": "2b01ea9b", "metadata": {}, "source": ["## 总成本分析\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "8f63d288", "metadata": {}, "source": ["### Prometheus模型 - `$2.167` 用于 `144` 个查询（`44` 用于Paul Graham的文章，`100` 用于Llama2论文），每个查询约为 `$0.015`。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "88a926de", "metadata": {}, "source": ["### GPT4 模型 - `$22`（total_cost_paul_graham_essay + total_cost_llama2）- 每个查询为 `$0.15`。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "dbe28800", "metadata": {}, "source": ["## 观察：\n", "\n", "1. 评估成本（约）：Prometheus模型为`$2.167`，GPT4为`$22`。\n", "2. 尽管Prometheus模型提供比GPT-4更详细的反馈，但偶尔会提供不正确的反馈，因此需要谨慎应用。\n", "3. 如果生成的答案缺少参考答案中的某些事实，Prometheus模型会对分数施加比GPT-4更严格的惩罚。\n", "4. 与GPT-4相比，Prometheus的忠实度和相关性反馈在反馈中显示出更多的幻觉/错误解释。\n", "5. Prometheus和GPT-4的忠实度和相关性分数的共同性在两个数据集中不同，因此在生产中应谨慎使用。\n", "\n", "注意：HF上的端点在AWS Nvidia A100G上提供，配备1个GPU和80GB内存，成本为每小时$6.5。我们在这里使用了[Prometheus模型](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)进行分析。我们还使用了[Prometheus模型](https://huggingface.co/kaist-ai/prometheus-13b-v1.0)的[GPTQ量化版本](https://huggingface.co/TheBloke/prometheus-13B-v1.0-GPTQ)进行了类似的分析，并观察到与原始未量化模型相比，在反馈中出现了更多的幻觉。感谢论文的作者和[Tom Jobbins](https://twitter.com/TheBlokeAI)提供了模型的量化版本。\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}, "vscode": {"interpreter": {"hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}}, "nbformat": 4, "nbformat_minor": 5}