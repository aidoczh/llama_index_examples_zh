{"cells": [{"cell_type": "markdown", "id": "39bbe88b", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/evaluation/mt_bench_single_grading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "a44afb3b-4c42-4985-909b-f6508965fdb5", "metadata": {}, "source": ["# 在Mini MT-Bench上对LLM评估器进行基准测试（单个评分）`LabelledEvaluatorDataset`\n"]}, {"cell_type": "markdown", "id": "53a0ea03-b7b5-47ed-8227-de416791eb6e", "metadata": {}, "source": ["在这个笔记本中，我们将对三种不同的评估器进行评估，它们将评判另一个LLM对用户查询的回复。更具体地说，我们将使用MT-Bench单评分数据集的迷你版本来运行基准测试。在这个版本中，我们只考虑llama2-70b提供的160个问题的答案（即80 x 2，因为有80个两轮对话）。用于此基准测试的参考答案由GPT-4提供。因此，我们对这三个评估器的基准测试将评估它们与GPT-4的接近程度（实际上，对于GPT-4来说是自我一致性）。\n", "\n", "1. GPT-3.5（OpenAI）\n", "2. GPT-4（OpenAI）\n", "3. Gemini-Pro（Google）\n"]}, {"cell_type": "code", "execution_count": null, "id": "ed15bedf", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai\n", "%pip install llama-index-llms-cohere\n", "%pip install llama-index-llms-gemini"]}, {"cell_type": "code", "execution_count": null, "id": "4ccb6e02-8a81-4f3c-8cc7-8d193d3689e3", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "id": "7d739ec7-174a-4282-9d24-f14d9845cf78", "metadata": {}, "outputs": [], "source": ["!pip install \"google-generativeai\" -q"]}, {"cell_type": "markdown", "id": "d86a307b-67c4-4455-9a54-665407a91258", "metadata": {}, "source": ["### 加载评估器数据集\n"]}, {"cell_type": "markdown", "id": "4e0e9014-cfb4-4b03-bfe1-04d75c4f55e9", "metadata": {}, "source": ["让我们从llama-hub加载llama数据集。\n"]}, {"cell_type": "code", "execution_count": null, "id": "2170e0d7-fc3f-45b0-bed0-7c8b8b31ac66", "metadata": {}, "outputs": [], "source": ["from llama_index.core.llama_dataset import download_llama_dataset", "", "# 下载数据集", "evaluator_dataset, _ = download_llama_dataset(", "    \"MiniMtBenchSingleGradingDataset\", \"./mini_mt_bench_data\"", ")"]}, {"cell_type": "code", "execution_count": null, "id": "f2277cdb-c855-46c7-9c7b-eedd09033e51", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>query</th>\n", "      <th>answer</th>\n", "      <th>contexts</th>\n", "      <th>ground_truth_answer</th>\n", "      <th>query_by</th>\n", "      <th>answer_by</th>\n", "      <th>ground_truth_answer_by</th>\n", "      <th>reference_feedback</th>\n", "      <th>reference_score</th>\n", "      <th>reference_evaluation_by</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Compose an engaging travel blog post about a r...</td>\n", "      <td>Title: Discovering the Aloha Spirit: A Recent ...</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>ai (llama2-70b)</td>\n", "      <td>None</td>\n", "      <td>The generated answer is highly relevant to the...</td>\n", "      <td>5.0</td>\n", "      <td>ai (gpt-4)</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Rewrite your previous response. Start every se...</td>\n", "      <td>Absolutely, here's the revised response:\\n\\nAl...</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>ai (llama2-70b)</td>\n", "      <td>None</td>\n", "      <td>The generated answer is highly relevant and co...</td>\n", "      <td>5.0</td>\n", "      <td>ai (gpt-4)</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>Draft a professional email seeking your superv...</td>\n", "      <td>Subject: Seeking Feedback on Quarterly Financi...</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>ai (llama2-70b)</td>\n", "      <td>None</td>\n", "      <td>The generated answer is highly relevant to the...</td>\n", "      <td>5.0</td>\n", "      <td>ai (gpt-4)</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>Take a moment to evaluate and critique your ow...</td>\n", "      <td>My response was:\\n\\n\"Subject: Seeking Feedback...</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>ai (llama2-70b)</td>\n", "      <td>None</td>\n", "      <td>The generated answer is highly relevant to the...</td>\n", "      <td>5.0</td>\n", "      <td>ai (gpt-4)</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>Imagine you are writing a blog post comparing ...</td>\n", "      <td>Sure, here's an outline for a blog post compar...</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>None</td>\n", "      <td>ai (llama2-70b)</td>\n", "      <td>None</td>\n", "      <td>The generated answer is highly relevant to the...</td>\n", "      <td>5.0</td>\n", "      <td>ai (gpt-4)</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                                               query  \\\n", "0  Compose an engaging travel blog post about a r...   \n", "1  Rewrite your previous response. Start every se...   \n", "2  Draft a professional email seeking your superv...   \n", "3  Take a moment to evaluate and critique your ow...   \n", "4  Imagine you are writing a blog post comparing ...   \n", "\n", "                                              answer contexts  \\\n", "0  Title: Discovering the Aloha Spirit: A Recent ...     None   \n", "1  Absolutely, here's the revised response:\\n\\nAl...     None   \n", "2  Subject: Seeking Feedback on Quarterly Financi...     None   \n", "3  My response was:\\n\\n\"Subject: Seeking Feedback...     None   \n", "4  Sure, here's an outline for a blog post compar...     None   \n", "\n", "  ground_truth_answer query_by        answer_by ground_truth_answer_by  \\\n", "0                None     None  ai (llama2-70b)                   None   \n", "1                None     None  ai (llama2-70b)                   None   \n", "2                None     None  ai (llama2-70b)                   None   \n", "3                None     None  ai (llama2-70b)                   None   \n", "4                None     None  ai (llama2-70b)                   None   \n", "\n", "                                  reference_feedback  reference_score  \\\n", "0  The generated answer is highly relevant to the...              5.0   \n", "1  The generated answer is highly relevant and co...              5.0   \n", "2  The generated answer is highly relevant to the...              5.0   \n", "3  The generated answer is highly relevant to the...              5.0   \n", "4  The generated answer is highly relevant to the...              5.0   \n", "\n", "  reference_evaluation_by  \n", "0              ai (gpt-4)  \n", "1              ai (gpt-4)  \n", "2              ai (gpt-4)  \n", "3              ai (gpt-4)  \n", "4              ai (gpt-4)  "]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["evaluator_dataset.to_pandas()[:5]"]}, {"cell_type": "markdown", "id": "3e40453c-1d51-4421-947e-4c3b10fee786", "metadata": {}, "source": ["### 定义我们的评估器\n", "\n", "我们将使用三个评估器来评估我们的模型：\n", "- `accuracy`：用于评估分类模型的准确性。\n", "- `precision`：用于评估分类模型的精确度。\n", "- `recall`：用于评估分类模型的召回率。\n", "\n", "我们将使用这些评估器来评估我们的模型在不同方面的性能表现。\n"]}, {"cell_type": "code", "execution_count": null, "id": "752dffac-5d23-424a-9fe3-b9e5c639602e", "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import CorrectnessEvaluator\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.llms.gemini import Gemini\n", "from llama_index.llms.cohere import Cohere\n", "\n", "llm_gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n", "llm_gpt35 = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n", "llm_gemini = Gemini(model=\"models/gemini-pro\", temperature=0)\n", "\n", "\n", "evaluators = {\n", "    \"gpt-4\": CorrectnessEvaluator(llm=llm_gpt4),\n", "    \"gpt-3.5\": CorrectnessEvaluator(llm=llm_gpt35),\n", "    \"gemini-pro\": CorrectnessEvaluator(llm=llm_gemini),\n", "}"]}, {"cell_type": "markdown", "id": "01d9c8be-8a1e-44fc-b590-3f02b62d5fd2", "metadata": {}, "source": ["### 使用`EvaluatorBenchmarkerPack`进行基准测试（llama-pack）\n", "\n", "使用`LabelledEvaluatorDataset`和`EvaluatorBenchmarkerPack`时，返回的基准测试结果将包含以下指标数值：\n", "\n", "- `number_examples`：数据集包含的示例数量。\n", "- `invalid_predictions`：无法产生最终评估的评估次数（例如，由于无法解析评估输出或LLM评估器抛出异常）。\n", "- `correlation`：提供的评估器分数与参考评估器（在本例中为gpt-4）分数之间的相关性。\n", "- `mae`：提供的评估器分数与参考评估器分数之间的平均绝对误差。\n", "- `hamming`：提供的评估器分数与参考评估器分数之间的汉明距离。\n", "\n", "注意：`correlation`、`mae`和`hamming`都是在无效预测的情况下计算的。因此，实质上这些指标是有条件的，取决于预测是否有效。\n"]}, {"cell_type": "code", "execution_count": null, "id": "e279d1f8-af0f-4557-b836-7a2d3bb6ef59", "metadata": {}, "outputs": [], "source": ["from llama_index.core.llama_pack import download_llama_pack\n", "\n", "EvaluatorBenchmarkerPack = download_llama_pack(\n", "    \"EvaluatorBenchmarkerPack\", \"./pack\"\n", ")"]}, {"cell_type": "markdown", "id": "a0d9a4b7-781b-44ef-ab11-e2328b2a00e8", "metadata": {}, "source": ["GPT 3.5 是 OpenAI 推出的一款自然语言处理模型，是 GPT 系列的最新版本。它具有更强大的语言理解和生成能力，可以用于文本生成、对话系统、语言翻译等多种应用领域。 GPT 3.5 基于大规模的预训练模型，能够理解和生成各种语言表达，并且在多个语言任务上取得了显著的性能提升。\n"]}, {"cell_type": "code", "execution_count": null, "id": "950c29b1-89ca-4ded-91a0-8256da4e8b84", "metadata": {}, "outputs": [], "source": ["evaluator_benchmarker = EvaluatorBenchmarkerPack(\n", "    evaluator=evaluators[\"gpt-3.5\"],\n", "    eval_dataset=evaluator_dataset,\n", "    show_progress=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "5d142745-6881-45e6-ae51-066f7b2ae1a4", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/nerdai/Projects/llama_index/docs/examples/evaluation/pack/base.py:142: UserWarning: You've set a large batch_size (>10). If using OpenAI GPT-4 as  `judge_llm` (which is the default judge_llm), you may experience a RateLimitError. Previous successful eval  responses are cached per batch. So hitting a RateLimitError would mean you'd lose all of the current batches successful  GPT-4 calls.\n", "  warnings.warn(\n", "Batch processing of predictions: 100%|████████████████████| 100/100 [00:05<00:00, 18.88it/s]\n", "Batch processing of predictions: 100%|██████████████████████| 60/60 [00:04<00:00, 12.26it/s]\n"]}], "source": ["gpt_3p5_benchmark_df = await evaluator_benchmarker.arun(\n", "    batch_size=100, sleep_time_in_seconds=0\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "8300c5ce-748f-4ca4-9219-72871806cc5d", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>number_examples</th>\n", "      <th>invalid_predictions</th>\n", "      <th>correlation</th>\n", "      <th>mae</th>\n", "      <th>hamming</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>gpt-3.5</th>\n", "      <td>160</td>\n", "      <td>0</td>\n", "      <td>0.317047</td>\n", "      <td>1.11875</td>\n", "      <td>27</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["         number_examples  invalid_predictions  correlation      mae  hamming\n", "gpt-3.5              160                    0     0.317047  1.11875       27"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["gpt_3p5_benchmark_df.index = [\"gpt-3.5\"]\n", "gpt_3p5_benchmark_df"]}, {"cell_type": "markdown", "id": "90e6cf9d-4848-456f-986f-954396939ad8", "metadata": {}, "source": ["GPT-4\n", "GPT-4是OpenAI推出的第四代通用预训练模型。它是一种基于人工智能的语言模型，可以生成高质量的文本内容。GPT-4在自然语言处理领域具有广泛的应用，可以用于文本生成、对话系统、翻译等多个领域。GPT-4相较于之前的版本在语言理解和生成能力上有所提升，被认为是目前最先进的语言模型之一。\n"]}, {"cell_type": "code", "execution_count": null, "id": "6445b17d-2892-4915-9d5b-e1ad6142d2b1", "metadata": {}, "outputs": [], "source": ["evaluator_benchmarker = EvaluatorBenchmarkerPack(\n", "    evaluator=evaluators[\"gpt-4\"],\n", "    eval_dataset=evaluator_dataset,\n", "    show_progress=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "f4b269e3-9125-4305-acbf-2fdfb9f4222a", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/nerdai/Projects/llama_index/docs/examples/evaluation/pack/base.py:142: UserWarning: You've set a large batch_size (>10). If using OpenAI GPT-4 as  `judge_llm` (which is the default judge_llm), you may experience a RateLimitError. Previous successful eval  responses are cached per batch. So hitting a RateLimitError would mean you'd lose all of the current batches successful  GPT-4 calls.\n", "  warnings.warn(\n", "Batch processing of predictions: 100%|████████████████████| 100/100 [00:13<00:00,  7.26it/s]\n", "Batch processing of predictions: 100%|██████████████████████| 60/60 [00:10<00:00,  5.92it/s]\n"]}], "source": ["gpt_4_benchmark_df = await evaluator_benchmarker.arun(\n", "    batch_size=100, sleep_time_in_seconds=0\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "3af7f475-e7fa-4123-984c-fe722fd6bc08", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>number_examples</th>\n", "      <th>invalid_predictions</th>\n", "      <th>correlation</th>\n", "      <th>mae</th>\n", "      <th>hamming</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>gpt-4</th>\n", "      <td>160</td>\n", "      <td>0</td>\n", "      <td>0.966126</td>\n", "      <td>0.09375</td>\n", "      <td>143</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["       number_examples  invalid_predictions  correlation      mae  hamming\n", "gpt-4              160                    0     0.966126  0.09375      143"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["gpt_4_benchmark_df.index = [\"gpt-4\"]\n", "gpt_4_benchmark_df"]}, {"cell_type": "markdown", "id": "09470187-876f-4919-8d40-7dcabd901036", "metadata": {}, "source": ["Gemini Pro\n"]}, {"cell_type": "code", "execution_count": null, "id": "71831f98-34ff-4175-9603-571b2a72086f", "metadata": {}, "outputs": [], "source": ["evaluator_benchmarker = EvaluatorBenchmarkerPack(\n", "    evaluator=evaluators[\"gemini-pro\"],\n", "    eval_dataset=evaluator_dataset,\n", "    show_progress=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "0f667926-ae4e-48ae-9c42-ecbc70b33536", "metadata": {}, "outputs": [], "source": ["gemini_pro_benchmark_df = await evaluator_benchmarker.arun(\n", "    batch_size=5, sleep_time_in_seconds=0.5\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "6dbf5d22-75a2-48c6-9639-e822146d79b7", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>number_examples</th>\n", "      <th>invalid_predictions</th>\n", "      <th>correlation</th>\n", "      <th>mae</th>\n", "      <th>hamming</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>gemini-pro</th>\n", "      <td>160</td>\n", "      <td>1</td>\n", "      <td>0.295121</td>\n", "      <td>1.220126</td>\n", "      <td>12</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["            number_examples  invalid_predictions  correlation       mae  \\\n", "gemini-pro              160                    1     0.295121  1.220126   \n", "\n", "            hamming  \n", "gemini-pro       12  "]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["gemini_pro_benchmark_df.index = [\"gemini-pro\"]\n", "gemini_pro_benchmark_df"]}, {"cell_type": "code", "execution_count": null, "id": "cf29da98-2e2c-453e-977b-5afae3a102bc", "metadata": {}, "outputs": [], "source": ["evaluator_benchmarker.prediction_dataset.save_json(\n", "    \"mt_sg_gemini_predictions.json\"\n", ")"]}, {"cell_type": "markdown", "id": "da3675c4-65d1-417a-88b2-585f40b5671c", "metadata": {}, "source": ["### 总结\n", "\n", "将所有基线放在一起。\n"]}, {"cell_type": "code", "execution_count": null, "id": "b5231aad-93e3-409a-a84e-9c23857cc7ce", "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>number_examples</th>\n", "      <th>invalid_predictions</th>\n", "      <th>correlation</th>\n", "      <th>mae</th>\n", "      <th>hamming</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>gpt-3.5</th>\n", "      <td>160</td>\n", "      <td>0</td>\n", "      <td>0.317047</td>\n", "      <td>1.118750</td>\n", "      <td>27</td>\n", "    </tr>\n", "    <tr>\n", "      <th>gpt-4</th>\n", "      <td>160</td>\n", "      <td>0</td>\n", "      <td>0.966126</td>\n", "      <td>0.093750</td>\n", "      <td>143</td>\n", "    </tr>\n", "    <tr>\n", "      <th>gemini-pro</th>\n", "      <td>160</td>\n", "      <td>1</td>\n", "      <td>0.295121</td>\n", "      <td>1.220126</td>\n", "      <td>12</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["            number_examples  invalid_predictions  correlation       mae  \\\n", "gpt-3.5                 160                    0     0.317047  1.118750   \n", "gpt-4                   160                    0     0.966126  0.093750   \n", "gemini-pro              160                    1     0.295121  1.220126   \n", "\n", "            hamming  \n", "gpt-3.5          27  \n", "gpt-4           143  \n", "gemini-pro       12  "]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["import pandas as pd\n", "\n", "final_benchmark = pd.concat(\n", "    [\n", "        gpt_3p5_benchmark_df,\n", "        gpt_4_benchmark_df,\n", "        gemini_pro_benchmark_df,\n", "    ],\n", "    axis=0,\n", ")\n", "final_benchmark"]}, {"cell_type": "markdown", "id": "80234260-8f53-4aa9-899b-85ed68bb7cda", "metadata": {}, "source": ["从上面的结果中，我们可以得出以下观察结果：\n", "- GPT-3.5 和 Gemini-Pro 似乎具有类似的结果，也许在接近 GPT-4 的程度上，GPT-3.5 稍微领先一点。\n", "- 不过，两者似乎都不太接近 GPT-4。\n", "- 在这个基准测试中，GPT-4 似乎与自身非常一致。\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}