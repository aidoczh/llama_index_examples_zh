{"cells": [{"cell_type": "markdown", "id": "cd032bcb-fefb-48ec-94da-08d49ac26120", "metadata": {}, "source": ["# 使用异步/并行执行的查询管道\n", "\n", "在这里，我们展示了我们的查询管道，使用异步+并行执行。\n", "\n", "我们通过设置一个RAG管道来实现这一点，该管道执行以下操作：\n", "1. 将查询发送到多个RAG查询引擎。\n", "2. 合并结果。\n", "\n", "在这个过程中，我们还将展示一些很好的抽象概念，用于连接结果（例如我们的`ArgPackComponent()`）。\n"]}, {"cell_type": "markdown", "id": "3531eedc-4f65-457e-8844-55fcc1773154", "metadata": {}, "source": ["## 加载数据\n", "\n", "加载保罗·格雷厄姆的文章作为示例。\n"]}, {"cell_type": "code", "execution_count": null, "id": "462a4f6f", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "2a441905-9007-44d6-b71a-6fc3e5023e49", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2024-01-10 12:31:00--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 75042 (73K) [text/plain]\n", "Saving to: ‘pg_essay.txt’\n", "\n", "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n", "\n", "2024-01-10 12:31:00 (6.32 MB/s) - ‘pg_essay.txt’ saved [75042/75042]\n", "\n"]}], "source": ["!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"]}, {"cell_type": "code", "execution_count": null, "id": "3533149c-4312-4444-9b45-52afe21731ed", "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "reader = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"])\n", "documents = reader.load_data()"]}, {"cell_type": "markdown", "id": "6c1d5ff8-ae04-4ea3-bbe0-2c097af71efd", "metadata": {}, "source": ["## 设置查询管道\n", "\n", "我们设置了一个并行查询管道，可以同时执行多个块大小，并将结果合并。\n"]}, {"cell_type": "markdown", "id": "63caf998-0a88-4c50-b6a4-2a0c412bde5b", "metadata": {}, "source": ["### 定义模块\n", "\n", "这包括：\n", "- LLM\n", "- 块大小\n", "- 查询引擎\n"]}, {"cell_type": "code", "execution_count": null, "id": "01fcbdb2-6747-4e65-b1ce-5d40febccb81", "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_pipeline import (\n", "    QueryPipeline,\n", "    InputComponent,\n", "    ArgPackComponent,\n", ")\n", "from typing import Dict, Any, List, Optional\n", "from llama_index.core.llama_pack import BaseLlamaPack\n", "from llama_index.core.llms import LLM\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core import Document, VectorStoreIndex\n", "from llama_index.core.response_synthesizers import TreeSummarize\n", "from llama_index.core.schema import NodeWithScore, TextNode\n", "from llama_index.core.node_parser import SentenceSplitter\n", "\n", "\n", "llm = OpenAI(model=\"gpt-3.5-turbo\")\n", "chunk_sizes = [128, 256, 512, 1024]\n", "query_engines = {}\n", "for chunk_size in chunk_sizes:\n", "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=0)\n", "    nodes = splitter.get_nodes_from_documents(documents)\n", "    vector_index = VectorStoreIndex(nodes)\n", "    query_engines[str(chunk_size)] = vector_index.as_query_engine(llm=llm)"]}, {"cell_type": "markdown", "id": "7a87a439-88e6-4130-b28f-45268330d3e4", "metadata": {}, "source": ["### 构建查询管道\n", "\n", "将输入连接到多个查询引擎，并将结果进行合并。\n"]}, {"cell_type": "code", "execution_count": null, "id": "ff95be2e-517f-4632-a7b8-a2e0dec11d73", "metadata": {}, "outputs": [], "source": ["", "# 构建查询管道", "p = QueryPipeline(verbose=True)", "module_dict = {", "    **query_engines,", "    \"input\": InputComponent(),  # 输入组件", "    \"summarizer\": TreeSummarize(),  # 摘要组件", "    \"join\": ArgPackComponent(  # 连接组件", "        convert_fn=lambda x: NodeWithScore(node=TextNode(text=str(x)))", "    ),", "}", "p.add_modules(module_dict)", "# 从输入到查询引擎添加链接（由chunk_size标识）", "for chunk_size in chunk_sizes:", "    p.add_link(\"input\", str(chunk_size))", "    p.add_link(str(chunk_size), \"join\", dest_key=str(chunk_size))", "p.add_link(\"join\", \"summarizer\", dest_key=\"nodes\")", "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")"]}, {"cell_type": "markdown", "id": "bda05274-09c5-4b56-b2ba-57f445346e73", "metadata": {}, "source": ["## 尝试查询\n", "\n", "让我们比较异步性能和同步性能！\n", "\n", "在我们的实验中，我们获得了2倍的加速。\n"]}, {"cell_type": "code", "execution_count": null, "id": "d3e161ce-ef10-446f-acfb-f6d3a1d291bb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n", "Module key: input. Input: \n", "input: What did the author do during his time in YC?\n", "\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n", "Module key: 128. Input: \n", "input: What did the author do during his time in YC?\n", "\n", "Module key: 256. Input: \n", "input: What did the author do during his time in YC?\n", "\n", "Module key: 512. Input: \n", "input: What did the author do during his time in YC?\n", "\n", "Module key: 1024. Input: \n", "input: What did the author do during his time in YC?\n", "\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n", "Module key: join. Input: \n", "128: The author worked on solving the problems of startups that were part of the YC program.\n", "256: The author worked on YC's internal software in Arc and also wrote essays during his time in YC.\n", "512: During his time in YC, the author worked on various projects. Initially, he intended to do three things: hack, write essays, and work on YC. However, as YC grew and he became more excited about it, it...\n", "1024: During his time in YC, the author worked on YC's internal software in Arc and wrote essays. He also worked on various projects related to YC, such as helping startups and solving their problems. Addit...\n", "\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n", "Module key: summarizer. Input: \n", "query_str: What did the author do during his time in YC?\n", "nodes: [NodeWithScore(node=TextNode(id_='7e0b0aeb-04e3-4518-b534-2cf68c07ae1f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fe9144af45...\n", "\n", "\n", "\u001b[0mDuring his time in YC, the author worked on various projects, including YC's internal software in Arc and writing essays. He also helped startups and solved their problems, and was involved in disputes between cofounders. Additionally, the author worked hard to ensure the success of YC and dealt with people who maltreated startups.\n", "Time taken: 3.943013906478882\n"]}], "source": ["import time\n", "\n", "start_time = time.time()\n", "response = await p.arun(input=\"What did the author do during his time in YC?\")\n", "print(str(response))\n", "end_time = time.time()\n", "print(f\"Time taken: {end_time - start_time}\")"]}, {"cell_type": "code", "execution_count": null, "id": "2b36ba65-636f-4fe9-8dee-e318cfe9a50d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[1;3;38;2;155;135;227m> Running module input with input: \n", "input: What did the author do during his time in YC?\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 128 with input: \n", "input: What did the author do during his time in YC?\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 256 with input: \n", "input: What did the author do during his time in YC?\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 512 with input: \n", "input: What did the author do during his time in YC?\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 1024 with input: \n", "input: What did the author do during his time in YC?\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module join with input: \n", "128: The author worked on solving the problems of startups that were part of the YC program.\n", "256: The author worked on YC's internal software in Arc and also wrote essays.\n", "512: During his time in YC, the author worked on various projects. Initially, he intended to do three things: hack, write essays, and work on YC. However, as YC grew and he became more excited about it, it...\n", "1024: During his time in YC, the author worked on YC's internal software in Arc, wrote essays, and worked on various projects related to YC. He also engaged in solving the problems faced by startups that we...\n", "\n", "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module summarizer with input: \n", "query_str: What did the author do during his time in YC?\n", "nodes: [NodeWithScore(node=TextNode(id_='4d698e2f-811e-42ce-bd0d-9b5615b0bbfd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fe9144af45...\n", "\n", "\u001b[0mDuring his time in YC, the author worked on YC's internal software in Arc, wrote essays, and worked on various projects related to YC. He also engaged in solving the problems faced by startups that were part of YC's program. Additionally, the author mentioned working on tasks he didn't particularly enjoy, such as resolving disputes between cofounders and dealing with people who mistreated startups.\n", "Time taken: 7.640604019165039\n"]}], "source": ["# 与同步方法进行比较", "", "start_time = time.time()", "response = p.run(input=\"作者在 YC 期间做了什么？\")", "print(str(response))", "end_time = time.time()", "print(f\"花费时间：{end_time - start_time}\")"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}