{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/node_postprocessor/MetadataReplacementDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 元数据替换 + 节点句子窗口\n", "\n", "在这个笔记本中，我们使用`SentenceWindowNodeParser`将文档解析为每个节点一个句子。每个节点还包含一个“窗口”，其中包含节点句子两侧的句子。\n", "\n", "然后，在检索过程中，在将检索到的句子传递给LLM之前，使用`MetadataReplacementNodePostProcessor`将单个句子替换为包含周围句子的窗口。\n", "\n", "这对于大型文档/索引非常有用，因为它有助于检索更精细的细节。\n", "\n", "默认情况下，句子窗口是原始句子两侧的5个句子。\n", "\n", "在这种情况下，不使用块大小设置，而是遵循窗口设置。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-embeddings-openai\n", "%pip install llama-index-embeddings-huggingface\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 设置\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.llms.openai import OpenAI", "from llama_index.embeddings.openai import OpenAIEmbedding", "from llama_index.embeddings.huggingface import HuggingFaceEmbedding", "from llama_index.core.node_parser import SentenceWindowNodeParser", "from llama_index.core.node_parser import SentenceSplitter", "", "# 创建具有默认设置的句子窗口节点解析器", "node_parser = SentenceWindowNodeParser.from_defaults(", "    window_size=3,", "    window_metadata_key=\"window\",", "    original_text_metadata_key=\"original_text\",", ")", "", "# 基本节点解析器是句子分割器", "text_splitter = SentenceSplitter()", "", "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)", "embed_model = HuggingFaceEmbedding(", "    model_name=\"sentence-transformers/all-mpnet-base-v2\", max_length=512", ")", "", "from llama_index.core import Settings", "", "Settings.llm = llm", "Settings.embed_model = embed_model", "Settings.text_splitter = text_splitter"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 加载数据，构建索引\n", "\n", "在这一部分，我们将加载数据并构建向量索引。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 加载数据\n", "\n", "在这里，我们使用最新的IPCC气候报告第3章来构建一个索引。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n", "                                 Dload  Upload   Total   Spent    Left  Speed\n", "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: www..ch\n"]}], "source": ["!curl https://www.ipcc.ch/report/ar6/wg2/downloads/report/IPCC_AR6_WGII_Chapter03.pdf --output IPCC_AR6_WGII_Chapter03.pdf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "documents = SimpleDirectoryReader(\n", "    input_files=[\"./IPCC_AR6_WGII_Chapter03.pdf\"]\n", ").load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 提取节点\n", "\n", "我们提取出将存储在VectorIndex中的节点集。这包括使用句子窗口解析器提取的节点，以及使用标准解析器提取的“基本”节点。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nodes = node_parser.get_nodes_from_documents(documents)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["base_nodes = text_splitter.get_nodes_from_documents(documents)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 构建索引\n", "\n", "我们同时构建句子索引和“基本”索引（使用默认的块大小）。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "\n", "sentence_index = VectorStoreIndex(nodes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["base_index = VectorStoreIndex(base_nodes)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 查询\n", "\n", "### 使用 MetadataReplacementPostProcessor\n", "\n", "在这里，我们现在使用 `MetadataReplacementPostProcessor` 来用周围的上下文替换每个节点中的句子。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["There is low confidence in the quantification of Atlantic Meridional Overturning Circulation (AMOC) changes in the 20th century due to low agreement in quantitative reconstructed and simulated trends. Additionally, direct observational records since the mid-2000s remain too short to determine the relative contributions of internal variability, natural forcing, and anthropogenic forcing to AMOC change. However, it is very likely that AMOC will decline for all SSP scenarios over the 21st century, but it will not involve an abrupt collapse before 2100.\n"]}], "source": ["from llama_index.core.postprocessor import MetadataReplacementPostProcessor", "", "query_engine = sentence_index.as_query_engine(", "    similarity_top_k=2,", "    # 目标键默认为`window`，以匹配node_parser的默认设置", "    node_postprocessors=[", "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")", "    ],", ")", "window_response = query_engine.query(", "    \"What are the concerns surrounding the AMOC?\"", ")", "print(window_response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["我们还可以检查每个节点检索到的原始句子，以及实际发送到LLM的句子窗口。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Window: Nevertheless, projected future annual cumulative upwelling wind \n", "changes at most locations and seasons remain within ±10–20% of \n", "present-day values (medium confidence) (WGI AR6 Section  9.2.3.5; \n", "Fox-Kemper et al., 2021).\n", " Continuous observation of the Atlantic meridional overturning \n", "circulation (AMOC) has improved the understanding of its variability \n", "(Frajka-Williams et  al., 2019), but there is low confidence in the \n", "quantification of AMOC changes in the 20th century because of low \n", "agreement in quantitative reconstructed and simulated trends (WGI \n", "AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., 2021). \n", " Direct observational records since the mid-2000s remain too short to \n", "determine the relative contributions of internal variability, natural \n", "forcing and anthropogenic forcing to AMOC change (high confidence) \n", "(WGI AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., \n", "2021).  Over the 21st century, AMOC will very likely decline for all SSP \n", "scenarios but will not involve an abrupt collapse before 2100 (WGI \n", "AR6 Sections 4.3.2, 9.2.3.1; Fox-Kemper et al., 2021; Lee et al., 2021).\n", " 3.2.2.4 Sea Ice Changes\n", "Sea ice is a key driver of polar marine life, hosting unique ecosystems \n", "and affecting diverse marine organisms and food webs through its \n", "impact on light penetration and supplies of nutrients and organic \n", "matter (Arrigo, 2014).  Since the late 1970s, Arctic sea ice area has \n", "decreased for all months, with an estimated decrease of 2 million km2 \n", "(or 25%) for summer sea ice (averaged for August, September and \n", "October) in 2010–2019 as compared with 1979–1988 (WGI AR6 \n", "Section 9.3.1.1; Fox-Kemper et al., 2021). \n", "------------------\n", "Original Sentence: Over the 21st century, AMOC will very likely decline for all SSP \n", "scenarios but will not involve an abrupt collapse before 2100 (WGI \n", "AR6 Sections 4.3.2, 9.2.3.1; Fox-Kemper et al., 2021; Lee et al., 2021).\n"]}], "source": ["window = window_response.source_nodes[0].node.metadata[\"window\"]\n", "sentence = window_response.source_nodes[0].node.metadata[\"original_text\"]\n", "\n", "print(f\"Window: {window}\")\n", "print(\"------------------\")\n", "print(f\"Original Sentence: {sentence}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 与普通的VectorStoreIndex对比\n", "\n", "在使用`VectorStoreIndex`时，我们需要注意以下几点：\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The concerns surrounding the AMOC are not provided in the given context information.\n"]}], "source": ["query_engine = base_index.as_query_engine(similarity_top_k=2)\n", "vector_response = query_engine.query(\n", "    \"What are the concerns surrounding the AMOC?\"\n", ")\n", "print(vector_response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["嗯，那个方法没起作用。让我们增加前k个！这种方法会比句子窗口索引慢，同时会使用更多的标记。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["There are concerns surrounding the AMOC (Atlantic Meridional Overturning Circulation). The context information mentions that the AMOC will decline over the 21st century, with high confidence but low confidence for quantitative projections.\n"]}], "source": ["query_engine = base_index.as_query_engine(similarity_top_k=5)\n", "vector_response = query_engine.query(\n", "    \"What are the concerns surrounding the AMOC?\"\n", ")\n", "print(vector_response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 分析\n", "\n", "因此，`SentenceWindowNodeParser` + `MetadataReplacementNodePostProcessor` 组合在这里是明显的赢家。但为什么呢？\n", "\n", "句子级别的嵌入似乎捕捉到了更多细粒度的细节，比如单词 `AMOC`。\n", "\n", "我们还可以比较每个索引的检索到的块！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Over the 21st century, AMOC will very likely decline for all SSP \n", "scenarios but will not involve an abrupt collapse before 2100 (WGI \n", "AR6 Sections 4.3.2, 9.2.3.1; Fox-Kemper et al., 2021; Lee et al., 2021).\n", "\n", "--------\n", "Direct observational records since the mid-2000s remain too short to \n", "determine the relative contributions of internal variability, natural \n", "forcing and anthropogenic forcing to AMOC change (high confidence) \n", "(WGI AR6 Sections 2.3.3, 9.2.3.1; Fox-Kemper et al., 2021; Gulev et al., \n", "2021). \n", "--------\n"]}], "source": ["for source_node in window_response.source_nodes:\n", "    print(source_node.node.metadata[\"original_text\"])\n", "    print(\"--------\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["在这里，我们可以看到句子窗口索引轻松检索到了两个讨论AMOC的节点。请记住，这里的嵌入是纯粹基于原始句子的，但LLM实际上最终会阅读周围的上下文！\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["现在，让我们尝试分析一下为什么朴素向量索引失败了。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["AMOC mentioned? False\n", "--------\n", "AMOC mentioned? False\n", "--------\n", "AMOC mentioned? True\n", "--------\n", "AMOC mentioned? False\n", "--------\n", "AMOC mentioned? False\n", "--------\n"]}], "source": ["for node in vector_response.source_nodes:\n", "    print(\"AMOC mentioned?\", \"AMOC\" in node.node.text)\n", "    print(\"--------\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["把索引为[2]的源节点提到了AMOC，但实际上这段文本是什么样的呢？\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["2021; Gulev et al. \n", "2021)The AMOC will decline over the 21st century \n", "(high confidence, but low confidence for \n", "quantitative projections).4.3.2.3, 9.2.3 (Fox-Kemper \n", "et al. 2021; Lee et al. \n", "2021)\n", "Sea ice\n", "Arctic sea ice \n", "changes‘Current Arctic sea ice coverage levels are the \n", "lowest since at least 1850 for both annual mean \n", "and late-summer values (high confidence).’2.3.2.1, 9.3.1 (Fox-Kemper \n", "et al. 2021; Gulev et al. \n", "2021)‘The Arctic will become practically ice-free in \n", "September by the end of the 21st century under \n", "SSP2-4.5, SSP3-7.0 and SSP5-8.5[…](high \n", "confidence).’4.3.2.1, 9.3.1 (Fox-Kemper \n", "et al. 2021; Lee et al. \n", "2021)\n", "Antarctic sea ice \n", "changesThere is no global significant trend in \n", "Antarctic sea ice area from 1979 to 2020 (high \n", "confidence).2.3.2.1, 9.3.2 (Fox-Kemper \n", "et al. 2021; Gulev et al. \n", "2021)There is low confidence in model simulations of \n", "future Antarctic sea ice.9.3.2 (Fox-Kemper et al. \n", "2021)\n", "Ocean chemistry\n", "Changes in salinityThe ‘large-scale, near-surface salinity contrasts \n", "have intensified since at least 1950 […] \n", "(virtually certain).’2.3.3.2, 9.2.2.2 \n", "(Fox-Kemper et al. 2021; \n", "Gulev et al. 2021)‘Fresh ocean regions will continue to get fresher \n", "and salty ocean regions will continue to get \n", "saltier in the 21st century (medium confidence).’9.2.2.2 (Fox-Kemper et al. \n", "2021)\n", "Ocean acidificationOcean surface pH has declined globally over the \n", "past four decades (virtually certain).2.3.3.5, 5.3.2.2 (Canadell \n", "et al. 2021; Gulev et al. \n", "2021)Ocean surface pH will continue to decrease \n", "‘through the 21st century, except for the \n", "lower-emission scenarios SSP1-1.9 and SSP1-2.6 \n", "[…] (high confidence).’4.3.2.5, 4.5.2.2, 5.3.4.1 \n", "(Lee et al. 2021; Canadell \n", "et al. 2021)\n", "Ocean \n", "deoxygenationDeoxygenation has occurred in most open \n", "ocean regions since the mid-20th century (high \n", "confidence).2.3.3.6, 5.3.3.2 (Canadell \n", "et al. 2021; Gulev et al. \n", "2021)Subsurface oxygen content ‘is projected to \n", "transition to historically unprecedented condition \n", "with decline over the 21st century (medium \n", "confidence).’5.3.3.2 (Canadell et al. \n", "2021)\n", "Changes in nutrient \n", "concentrationsNot assessed in WGI Not assessed in WGI\n"]}], "source": ["print(vector_response.source_nodes[2].node.text)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["所以AMOC被讨论了，但遗憾的是它在中间部分。对于LLMs来说，经常观察到检索到的上下文中间的文本往往被忽略或不太有用。最近的一篇论文[\"中间的遗失\"在这里讨论了这个问题](https://arxiv.org/abs/2307.03172)。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## [可选] 评估\n", "\n", "我们将更严格地评估句子窗口检索器相对于基础检索器的工作效果。\n", "\n", "我们定义/加载一个评估基准数据集，然后对其进行不同的评估。\n", "\n", "**警告**：这可能会非常*昂贵*，特别是使用GPT-4。请谨慎使用，并调整样本大小以适应您的预算。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\n", "\n", "from llama_index.llms.openai import OpenAI\n", "import nest_asyncio\n", "import random\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["428"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["len(base_nodes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_nodes_eval = 30", "# 总共有428个节点。取前200个节点生成问题（文档的后半部分都是参考资料）", "sample_eval_nodes = random.sample(base_nodes[:200], num_nodes_eval)", "# 注意：如果数据集尚未保存，则运行此代码", "# 从最大的块（1024）生成问题", "dataset_generator = DatasetGenerator(", "    sample_eval_nodes,", "    llm=OpenAI(model=\"gpt-4\"),", "    show_progress=True,", "    num_questions_per_chunk=2,", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eval_dataset = await dataset_generator.agenerate_dataset_from_nodes()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eval_dataset.save_json(\"data/ipcc_eval_qr_dataset.json\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 可选", "eval_dataset = QueryResponseDataset.from_json(\"data/ipcc_eval_qr_dataset.json\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 比较结果\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import asyncio\n", "import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation import (", "    CorrectnessEvaluator,  # 正确性评估器", "    SemanticSimilarityEvaluator,  # 语义相似性评估器", "    RelevancyEvaluator,  # 相关性评估器", "    FaithfulnessEvaluator,  # 忠实度评估器", "    PairwiseComparisonEvaluator,  # 两两比较评估器", ")", "", "", "from collections import defaultdict", "import pandas as pd", "", "# 注意：可以取消其他评估器的注释", "evaluator_c = CorrectnessEvaluator(llm=OpenAI(model=\"gpt-4\"))  # 正确性评估器", "evaluator_s = SemanticSimilarityEvaluator()  # 语义相似性评估器", "evaluator_r = RelevancyEvaluator(llm=OpenAI(model=\"gpt-4\"))  # 相关性评估器", "evaluator_f = FaithfulnessEvaluator(llm=OpenAI(model=\"gpt-4\"))  # 忠实度评估器", "# pairwise_evaluator = PairwiseComparisonEvaluator(llm=OpenAI(model=\"gpt-4\"))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.evaluation.eval_utils import (", "    get_responses,", "    get_results_df,", ")", "from llama_index.core.evaluation import BatchEvalRunner", "", "max_samples = 30", "", "eval_qs = eval_dataset.questions", "ref_response_strs = [r for (_, r) in eval_dataset.qr_pairs]", "", "# 重新设置基础查询引擎和句子窗口查询引擎", "# 基础查询引擎", "base_query_engine = base_index.as_query_engine(similarity_top_k=2)", "# 句子窗口查询引擎", "query_engine = sentence_index.as_query_engine(", "    similarity_top_k=2,", "    # 目标键默认为`window`，以匹配node_parser的默认设置", "    node_postprocessors=[", "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")", "    ],", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "\n", "base_pred_responses = get_responses(\n", "    eval_qs[:max_samples], base_query_engine, show_progress=True\n", ")\n", "pred_responses = get_responses(\n", "    eval_qs[:max_samples], query_engine, show_progress=True\n", ")\n", "\n", "pred_response_strs = [str(p) for p in pred_responses]\n", "base_pred_response_strs = [str(p) for p in base_pred_responses]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["evaluator_dict = {\n", "    \"correctness\": evaluator_c,\n", "    \"faithfulness\": evaluator_f,\n", "    \"relevancy\": evaluator_r,\n", "    \"semantic_similarity\": evaluator_s,\n", "}\n", "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["运行对忠实度/语义相似性的评估。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eval_results = await batch_runner.aevaluate_responses(\n", "    queries=eval_qs[:max_samples],\n", "    responses=pred_responses[:max_samples],\n", "    reference=ref_response_strs[:max_samples],\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["base_eval_results = await batch_runner.aevaluate_responses(\n", "    queries=eval_qs[:max_samples],\n", "    responses=base_pred_responses[:max_samples],\n", "    reference=ref_response_strs[:max_samples],\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>names</th>\n", "      <th>correctness</th>\n", "      <th>relevancy</th>\n", "      <th>faithfulness</th>\n", "      <th>semantic_similarity</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Sentence Window Retriever</td>\n", "      <td>4.366667</td>\n", "      <td>0.933333</td>\n", "      <td>0.933333</td>\n", "      <td>0.959583</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Base Retriever</td>\n", "      <td>4.216667</td>\n", "      <td>0.900000</td>\n", "      <td>0.933333</td>\n", "      <td>0.958664</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                       names  correctness  relevancy  faithfulness  \\\n", "0  Sentence Window Retriever     4.366667   0.933333      0.933333   \n", "1             Base Retriever     4.216667   0.900000      0.933333   \n", "\n", "   semantic_similarity  \n", "0             0.959583  \n", "1             0.958664  "]}, "metadata": {}, "output_type": "display_data"}], "source": ["results_df = get_results_df(\n", "    [eval_results, base_eval_results],\n", "    [\"Sentence Window Retriever\", \"Base Retriever\"],\n", "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n", ")\n", "display(results_df)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}