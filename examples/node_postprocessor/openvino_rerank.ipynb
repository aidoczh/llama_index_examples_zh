{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/node_postprocessor/openivno_rerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# OpenVINO Rerank\n", "\n", "[OpenVINO™](https://github.com/openvinotoolkit/openvino)是一个用于优化和部署AI推断的开源工具包。OpenVINO™ Runtime支持各种硬件[设备](https://github.com/openvinotoolkit/openvino?tab=readme-ov-file#supported-hardware-matrix)，包括x86和ARM CPU以及Intel GPU。它可以帮助提升计算机视觉、自动语音识别、自然语言处理和其他常见任务中的深度学习性能。\n", "\n", "Hugging Face rerank模型可以通过``OpenVINORerank``类来支持OpenVINO。\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-postprocessor-openvino-rerank\n", "%pip install llama-index-embeddings-openvino"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["## 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex和SimpleDirectoryReader", "", "# 加载文档", "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()", "", "# 构建索引", "index = VectorStoreIndex.from_documents(documents=documents)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 下载嵌入、重新排序模型和LLM\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n", "\n", "OpenVINOEmbedding.create_and_save_openvino_model(\n", "    \"BAAI/bge-small-en-v1.5\", \"./embedding_ov\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.postprocessor.openvino_rerank import OpenVINORerank\n", "\n", "OpenVINORerank.create_and_save_openvino_model(\n", "    \"BAAI/bge-reranker-large\", \"./rerank_ov\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!optimum-cli export openvino --model HuggingFaceH4/zephyr-7b-beta --weight-format int4 llm_ov"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 下载模型\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.llms.openvino import OpenVINOLLM\n", "from llama_index.core import Settings\n", "\n", "\n", "Settings.embed_model = OpenVINOEmbedding(folder_name=\"./embedding_ov\")\n", "Settings.llm = OpenVINOLLM(model_name=\"./llm_ov\", tokenizer_name=\"./llm_ov\")\n", "ov_rerank = OpenVINORerank(model=\"./rerank_ov\", top_n=2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 检索前10个最相关的节点，然后使用OpenVINO重新排序进行过滤\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "from llama_index.postprocessor.openvino_rerank import OpenVINORerank\n", "\n", "\n", "ov_rerank = OpenVINORerank(top_n=2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["index = VectorStoreIndex.from_documents(documents=documents)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(\n", "    similarity_top_k=10,\n", "    node_postprocessors=[ov_rerank],\n", ")\n", "response = query_engine.query(\n", "    \"What did Sam Altman do in this essay?\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "Sam Altman was asked by the author, Paul Graham, to become the president of Y Combinator (YC), a startup accelerator. Initially, Sam declined the offer as he wanted to start a startup to make nuclear reactors. However, the author continued to persuade him, and in October 2013, Sam agreed to take over YC starting with the winter 2014 batch. The author then stepped back from running YC and focused on other activities, including painting and writing essays.\n"]}], "source": ["print(response)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["> Source (Doc id: ae4297fa-670c-403c-a355-6fffe7e16835): Why not organize a summer program where they'd start startups instead? We wouldn't feel guilty for being in a sense fake investors, because they would in a similar sense be fake founders. So while ...\n", "\n", "> Source (Doc id: c55eddb9-33f8-46bb-82a1-cb7fa0c7f5b6): This seemed strange advice, because YC was doing great. But if there was one thing rarer than Rtm offering advice, it was Rtm being wrong. So this set me thinking. It was true that on my current tr...\n"]}], "source": ["print(response.get_formatted_sources(length=200))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 直接检索前两个最相似的节点\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(\n", "    similarity_top_k=2,\n", ")\n", "response = query_engine.query(\n", "    \"What did Sam Altman do in this essay?\",\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["检索到的上下文不相关，回复是虚构的。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "Sam Altman is mentioned in the essay as the person who was asked to become the president of Y Combinator. He initially declined the offer but later agreed to take over starting with the winter 2014 batch. The author also mentions that they left running Y Combinator more and more to Sam, partly so he could learn the job, and partly because they were focused on their mother, who had cancer and passed away in January 2014.\n"]}], "source": ["print(response)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["> Source (Doc id: c55eddb9-33f8-46bb-82a1-cb7fa0c7f5b6): This seemed strange advice, because YC was doing great. But if there was one thing rarer than Rtm offering advice, it was Rtm being wrong. So this set me thinking. It was true that on my current tr...\n", "\n", "> Source (Doc id: 6b2c335f-1390-4e92-9171-3ba5d24b3826): I knew that online essays would be a marginal medium at first. Socially they'd seem more like rants posted by nutjobs on their GeoCities sites than the genteel and beautifully typeset compositions ...\n"]}], "source": ["print(response.get_formatted_sources(length=200))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["有关更多信息，请参考：\n", "\n", "* [OpenVINO LLM指南](https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide.html)。\n", "\n", "* [OpenVINO文档](https://docs.openvino.ai/2024/home.html)。\n", "\n", "* [OpenVINO入门指南](https://www.intel.com/content/www/us/en/content-details/819067/openvino-get-started-guide.html)。\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}