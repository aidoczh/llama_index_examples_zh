{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# NVIDIA NIMs\n", "\n", "> [NVIDIA NIMs](https://ai.nvidia.com)为用户提供了便捷访问NVIDIA托管的AI模型API端点，如Mixtral 8x22B、Llama 3、Stable Diffusion等。这些模型托管在[https://build.nvidia.com](https://build.nvidia.com)，经过优化、测试并托管在NVIDIA AI平台上，使其快速且易于评估，进一步定制，并在任何加速堆栈上无缝运行达到最佳性能。\n", "\n", "> 使用[NVIDIA NIMs](https://ai.nvidia.com)，您可以从在[NVIDIA DGX Cloud](https://www.nvidia.com/en-us/data-center/dgx-cloud/)上运行的完全加速堆栈中快速获得结果。这些模型可以使用[使用NVIDIA AI Enterprise](https://www.nvidia.com/en-us/data-center/products/ai-enterprise/)在任何地方部署，具有企业级安全性、稳定性和支持。\n", "\n", "> 这些模型可以通过[`llama-index-postprocessor-nvidia-rerank`](https://pypi.org/project/llama-index-postprocessor-nvidia-rerank/)包轻松访问，如下所示。\n", "\n", "本示例介绍了如何使用LlamaIndex与支持的[NVIDIA Retrieval QA Ranking Model](https://build.nvidia.com/explore/retrieval)进行交互，以通过`NVIDIARerank`类进行[检索增强生成](https://developer.nvidia.com/blog/build-enterprise-retrieval-augmented-generation-apps-with-nvidia-retrieval-qa-embedding-model/)。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 重新排序\n", "\n", "重新排序是高精度、高效的检索流程中的关键部分。\n", "\n", "两个重要的用例：\n", "- 结合多个数据源的结果\n", "- 提高单个数据源的准确性\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 合并多个来源的结果\n", "\n", "考虑一个包含来自语义存储（如VectorStoreIndex）和BM25存储的数据的流水线。\n", "\n", "每个存储都是独立查询的，并返回各自认为高度相关的结果。确定结果的整体相关性是重新排序发挥作用的地方。\n", "\n", "请参考[高级 - 混合检索器 + 重新排序](https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/#advanced-hybrid-retriever-re-ranking)用例，将[重新排序器](https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/#re-ranker-setup)替换为 -\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install --upgrade --quiet llama-index-postprocessor-nvidia-rerank"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.postprocessor.nvidia_rerank import NVIDIARerank\n", "\n", "reranker = NVIDIARerank(top_n=4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 连接到本地NIMs\n", "\n", "除了连接到托管的[NVIDIA NIMs](https://ai.nvidia.com)之外，此连接器还可用于连接到本地微服务实例。这有助于在必要时将应用程序部署到本地。\n", "\n", "有关设置本地微服务实例的说明，请参阅https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["来自llama_index.postprocessor.nvidia_rerank的NVIDIARerank", "", "# 从上面获取reranker = NVIDIARerank(top_n...) ", "reranker = reranker.mode(\"nim\", base_url=\"http://0.0.0.0:1976/v1\")"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}