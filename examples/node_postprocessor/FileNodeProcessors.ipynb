{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "3f8b002b", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/node_postprocessor/FileNodeProcessors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "46ced011-52fd-4adf-b2ce-c9247e87757d", "metadata": {}, "source": ["# 基于文件的节点解析器\n", "\n", "`SimpleFileNodeParser` 和 `FlatReader` 旨在允许打开各种文件类型，并自动选择最佳的 `NodeParser` 来处理文件。`FlatReader` 以原始文本格式加载文件，并将文件信息附加到元数据中，然后 `SimpleFileNodeParser` 将文件类型映射到 `node_parser/file` 中的节点解析器，选择最适合任务的节点解析器。\n", "\n", "`SimpleFileNodeParser` 不执行基于标记的文本分块，并旨在与标记节点解析器结合使用。\n", "\n", "让我们看一个使用 `FlatReader` 和 `SimpleFileNodeParser` 加载内容的示例。对于 README 文件，我将使用 LlamaIndex 的 README，对于 HTML 文件，我将使用 Stack Overflow 的首页，但是任何 README 和 HTML 文件都可以使用。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "c96e7e3e", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "e33116cb", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-readers-file"]}, {"cell_type": "code", "execution_count": null, "id": "89026f89", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "ae713f58-414a-4dc6-a358-7d07846eddd6", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/adamhofmann/opt/anaconda3/lib/python3.9/site-packages/langchain/__init__.py:24: UserWarning: Importing BasePromptTemplate from langchain root module is no longer supported.\n", "  warnings.warn(\n", "/Users/adamhofmann/opt/anaconda3/lib/python3.9/site-packages/langchain/__init__.py:24: UserWarning: Importing PromptTemplate from langchain root module is no longer supported.\n", "  warnings.warn(\n"]}], "source": ["from llama_index.core.node_parser import SimpleFileNodeParser\n", "from llama_index.readers.file import FlatReader\n", "from pathlib import Path"]}, {"cell_type": "code", "execution_count": null, "id": "46733ed7-54a8-44b1-ad2f-0a5cd0624a71", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'filename': 'stack-overflow.html', 'extension': '.html'}\n", "Doc ID: a6750408-b0fa-466d-be28-ff2fcbcbaa97\n", "Text: <!DOCTYPE html>       <html class=\"html__responsive\n", "html__unpinned-leftnav\" lang=\"en\">      <head>          <title>Stack\n", "Overflow - Where Developers Learn, Share, &amp; Build Careers</title>\n", "<link rel=\"shortcut icon\" href=\"https://cdn.sstatic.net/Sites/stackove\n", "rflow/Img/favicon.ico?v=ec617d715196\">         <link rel=\"apple-touch-\n", "icon\" hr...\n", "----\n", "{'filename': 'README.md', 'extension': '.md'}\n", "Doc ID: 1d872f44-2bb3-4693-a1b8-a59392c23be2\n", "Text: # 🗂️ LlamaIndex 🦙 [![PyPI -\n", "Downloads](https://img.shields.io/pypi/dm/llama-\n", "index)](https://pypi.org/project/llama-index/) [![GitHub contributors]\n", "(https://img.shields.io/github/contributors/jerryjliu/llama_index)](ht\n", "tps://github.com/jerryjliu/llama_index/graphs/contributors) [![Discord\n", "](https://img.shields.io/discord/1059199217496772688)](https:...\n"]}], "source": ["reader = FlatReader()\n", "html_file = reader.load_data(Path(\"./stack-overflow.html\"))\n", "md_file = reader.load_data(Path(\"./README.md\"))\n", "print(html_file[0].metadata)\n", "print(html_file[0])\n", "print(\"----\")\n", "print(md_file[0].metadata)\n", "print(md_file[0])"]}, {"cell_type": "markdown", "id": "d8af0bda-338e-403e-97c7-ce561867bef9", "metadata": {}, "source": ["## 解析文件\n", "\n", "简单的读取器已经将文件的内容加载到文档对象中，以便进一步处理。我们可以看到文件信息保留在元数据中。让我们将这些文档传递给节点解析器，看看解析的结果。\n"]}, {"cell_type": "code", "execution_count": null, "id": "dbf324f2-73b6-416e-b019-744284debf85", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙'}\n", "🗂️ LlamaIndex 🦙\n", "[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\n", "[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\n", "[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)\n", "\n", "\n", "LlamaIndex (GPT Index) is a data framework for your LLM application.\n", "\n", "PyPI: \n", "- LlamaIndex: https://pypi.org/project/llama-index/.\n", "- GPT Index (duplicate): https://pypi.org/project/gpt-index/.\n", "\n", "LlamaIndex.TS (Typescript/Javascript): https://github.com/run-llama/LlamaIndexTS.\n", "\n", "Documentation: https://gpt-index.readthedocs.io/.\n", "\n", "Twitter: https://twitter.com/llama_index.\n", "\n", "Discord: https://discord.gg/dGcwcsnxhU.\n", "{'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 3': 'Ecosystem'}\n", "Ecosystem\n", "\n", "- LlamaHub (community library of data loaders): https://llamahub.ai\n", "- LlamaLab (cutting-edge AGI projects using LlamaIndex): https://github.com/run-llama/llama-lab\n", "----\n", "{'filename': 'stack-overflow.html', 'extension': '.html', 'tag': 'li'}\n", "About\n", "Products\n", "For Teams\n", "Stack Overflow\n", "Public questions & answers\n", "Stack Overflow for Teams\n", "Where developers & technologists share private knowledge with coworkers\n", "Talent\n", "\n", "\t\t\t\t\t\t\t\tBuild your employer brand\n", "Advertising\n", "Reach developers & technologists worldwide\n", "Labs\n", "The future of collective knowledge sharing\n", "About the company\n", "current community\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "            Stack Overflow\n", "        \n", "\n", "\n", "\n", "help\n", "chat\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "            Meta Stack Overflow\n", "        \n", "\n", "\n", "\n", "\n", "\n", "\n", "your communities            \n", "\n", "\n", "\n", "Sign up or log in to customize your list.                \n", "\n", "\n", "more stack exchange communities\n", "\n", "company blog\n"]}], "source": ["parser = SimpleFileNodeParser()\n", "md_nodes = parser.get_nodes_from_documents(md_file)\n", "html_nodes = parser.get_nodes_from_documents(html_file)\n", "print(md_nodes[0].metadata)\n", "print(md_nodes[0].text)\n", "print(md_nodes[1].metadata)\n", "print(md_nodes[1].text)\n", "print(\"----\")\n", "print(html_nodes[0].metadata)\n", "print(html_nodes[0].text)"]}, {"cell_type": "markdown", "id": "1486c88f-8a3a-47e9-b8ec-4212e55c3fa2", "metadata": {}, "source": ["## 进一步处理文件\n", "\n", "我们可以看到，Markdown 和 HTML 文件已根据文档结构被分割成块。Markdown 节点解析器会根据任何标题进行分割，并将标题的层次结构附加到元数据中。HTML 节点解析器从常见文本元素中提取文本，简化了 HTML 文件，并合并相邻的相同元素节点。与处理原始 HTML 相比，这已经在检索有意义的文本内容方面有了很大的改进。\n", "\n", "由于这些文件仅根据文件结构进行了分割，我们可以使用文本分割器进行进一步处理，将内容准备成有限标记长度的节点。\n"]}, {"cell_type": "code", "execution_count": null, "id": "ffe3d6fd-c63a-47ca-b966-75da8b23e4e2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "\n", "HTML parsed nodes: 67\n", "About\n", "Products\n", "For Teams\n", "Stack Overflow\n", "Public questions & answers\n", "Stack Overflow for Teams\n", "Where developers & technologists share private knowledge with coworkers\n", "Talent\n", "\n", "\t\t\t\t\t\t\t\tBuild your employer brand\n", "Advertising\n", "Reach developers & technologists worldwide\n", "Labs\n", "The future of collective knowledge sharing\n", "About the company\n", "current community\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "            Stack Overflow\n", "        \n", "\n", "\n", "\n", "help\n", "chat\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "            Meta Stack Overflow\n", "        \n", "\n", "\n", "\n", "\n", "\n", "\n", "your communities            \n", "\n", "\n", "\n", "Sign up or log in to customize your list.                \n", "\n", "\n", "more stack exchange communities\n", "\n", "company blog\n", "\n", "\n", "HTML chunked nodes: 87\n", "About\n", "Products\n", "For Teams\n", "Stack Overflow\n", "Public questions & answers\n", "Stack Overflow for Teams\n", "Where developers & technologists share private knowledge with coworkers\n", "Talent\n", "\n", "\t\t\t\t\t\t\t\tBuild your employer brand\n", "Advertising\n", "Reach developers & technologists worldwide\n", "Labs\n", "The future of collective knowledge sharing\n", "About the company\n", "current community\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "            Stack Overflow\n", "        \n", "\n", "\n", "\n", "help\n", "chat\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "            Meta Stack Overflow\n", "        \n", "\n", "\n", "\n", "\n", "\n", "\n", "your communities\n", "\n", "\n", "MD parsed nodes: 10\n", "🗂️ LlamaIndex 🦙\n", "[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\n", "[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\n", "[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)\n", "\n", "\n", "LlamaIndex (GPT Index) is a data framework for your LLM application.\n", "\n", "PyPI: \n", "- LlamaIndex: https://pypi.org/project/llama-index/.\n", "- GPT Index (duplicate): https://pypi.org/project/gpt-index/.\n", "\n", "LlamaIndex.TS (Typescript/Javascript): https://github.com/run-llama/LlamaIndexTS.\n", "\n", "Documentation: https://gpt-index.readthedocs.io/.\n", "\n", "Twitter: https://twitter.com/llama_index.\n", "\n", "Discord: https://discord.gg/dGcwcsnxhU.\n", "\n", "\n", "MD chunked nodes: 13\n", "🗂️ LlamaIndex 🦙\n", "[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\n", "[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\n", "[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)\n"]}], "source": ["from llama_index.core.node_parser import SentenceSplitter", "", "# 为了在演示中更清晰，进行小范围且不重叠的分割", "splitting_parser = SentenceSplitter(chunk_size=200, chunk_overlap=0)", "", "html_chunked_nodes = splitting_parser(html_nodes)", "md_chunked_nodes = splitting_parser(md_nodes)", "print(f\"\\n\\nHTML解析节点数：{len(html_nodes)}\")", "print(html_nodes[0].text)", "", "print(f\"\\n\\nHTML分块节点数：{len(html_chunked_nodes)}\")", "print(html_chunked_nodes[0].text)", "", "print(f\"\\n\\nMD解析节点数：{len(md_nodes)}\")", "print(md_nodes[0].text)", "", "print(f\"\\n\\nMD分块节点数：{len(md_chunked_nodes)}\")", "print(md_chunked_nodes[0].text)"]}, {"cell_type": "markdown", "id": "421f1e27-56d1-451c-a7fd-b4ea8b68d970", "metadata": {}, "source": ["## 概要\n", "\n", "我们可以看到文件在`SimpleFileNodeParser`创建的拆分中进一步处理，现在已经准备好被索引或向量存储器摄入。下面的代码单元格展示了从原始文件到分块节点的解析器链式调用：\n"]}, {"cell_type": "code", "execution_count": null, "id": "c86294e0-dd2e-4415-9d70-bfbde29f339f", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[TextNode(id_='e6236169-45a1-4699-9762-c8d3d89f8fa0', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e7bc328f-85c1-430a-9772-425e59909a58', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙'}, hash='e538ad7c04f635f1c707eba290b55618a9f0942211c4b5ca2a4e54e1fdf04973'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='51b40b54-dfd3-48ed-b377-5ca58a0f48a3', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙'}, hash='ca9e3590b951f1fca38687fd12bb43fbccd0133a38020c94800586b3579c3218')}, hash='ec733c85ad1dca248ae583ece341428ee20e4d796bc11adea1618c8e4ed9246a', text='🗂️ LlamaIndex 🦙\\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\\n[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\\n[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='51b40b54-dfd3-48ed-b377-5ca58a0f48a3', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e7bc328f-85c1-430a-9772-425e59909a58', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙'}, hash='e538ad7c04f635f1c707eba290b55618a9f0942211c4b5ca2a4e54e1fdf04973'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e6236169-45a1-4699-9762-c8d3d89f8fa0', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙'}, hash='ec733c85ad1dca248ae583ece341428ee20e4d796bc11adea1618c8e4ed9246a')}, hash='ca9e3590b951f1fca38687fd12bb43fbccd0133a38020c94800586b3579c3218', text='LlamaIndex (GPT Index) is a data framework for your LLM application.\\n\\nPyPI: \\n- LlamaIndex: https://pypi.org/project/llama-index/.\\n- GPT Index (duplicate): https://pypi.org/project/gpt-index/.\\n\\nLlamaIndex.TS (Typescript/Javascript): https://github.com/run-llama/LlamaIndexTS.\\n\\nDocumentation: https://gpt-index.readthedocs.io/.\\n\\nTwitter: https://twitter.com/llama_index.\\n\\nDiscord: https://discord.gg/dGcwcsnxhU.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='ce269047-4718-4a08-b170-34fef19cdafe', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 3': 'Ecosystem'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='953934dc-dd4f-4069-9e2a-326ee8a593bf', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 3': 'Ecosystem'}, hash='ede2843c0f18e0f409ae9e2bb4090bca4409eaa992fe8ca149295406d3d7adac')}, hash='52b03025c73d7218bd4d66b9812f6e1f6fab6ccf64e5660dc31d123bf1caf5be', text='Ecosystem\\n\\n- LlamaHub (community library of data loaders): https://llamahub.ai\\n- LlamaLab (cutting-edge AGI projects using LlamaIndex): https://github.com/run-llama/llama-lab', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='5ef55167-1fa1-4cae-b2b5-4a86beffbef6', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2223925f-93a8-45db-9044-41838633e8cc', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview'}, hash='adc49240ff2bdd007e3462b2c3d3f6b6f3b394abbf043d4c291b1a029302c909')}, hash='dc3f175a9119976866e3e6fb2233a12590e8861dc91c621db131521d84e490c4', text='🚀 Overview\\n\\n**NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='8b8e4778-7943-424c-a160-b7da845dd7da', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Context'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c1ea3027-aad7-4a6f-b8dc-460a8ffbc258', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Context'}, hash='632c76181233b32c03377ccc3d41e458aaec7de845d123a20ace6e3036bbdcd7')}, hash='b867ce7afa1cee176db4e5d0b147276c2e4c724223d590dd5017e68fab3aa29a', text='Context\\n- LLMs are a phenomenonal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.\\n- How do we best augment LLMs with our own private data?\\n\\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='be9d228a-91f6-4c39-845d-b79d3b8fa874', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Proposed Solution'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f57a202a-cb3d-4a74-ab09-70bf93a0bf51', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Proposed Solution'}, hash='4d338f21570da1564e407877e2fceac4dc9e9f8c90cb3b34876507f85d29f41e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a18e1c90-0455-47be-9411-8e098df9c951', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Proposed Solution'}, hash='7b9bbe433d53e727b353864a38ad8a9e78b74c84dbef4ca931422f0f45a4906d')}, hash='b02a43b52686c62c8c4a2f32aa7b8a5bcf2a9e9ea7a033430645ec492f04a4fd', text='Proposed Solution\\n\\nThat\\'s where **LlamaIndex** comes in. LlamaIndex is a \"data framework\" to help you build LLM apps. It provides the following tools:\\n\\n- Offers **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\\n- Provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\\n- Provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\\n- Allows easy integrations with your outer application framework (e.g.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='a18e1c90-0455-47be-9411-8e098df9c951', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Proposed Solution'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f57a202a-cb3d-4a74-ab09-70bf93a0bf51', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Proposed Solution'}, hash='4d338f21570da1564e407877e2fceac4dc9e9f8c90cb3b34876507f85d29f41e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='be9d228a-91f6-4c39-845d-b79d3b8fa874', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🚀 Overview', 'Header 3': 'Proposed Solution'}, hash='b02a43b52686c62c8c4a2f32aa7b8a5bcf2a9e9ea7a033430645ec492f04a4fd')}, hash='7b9bbe433d53e727b353864a38ad8a9e78b74c84dbef4ca931422f0f45a4906d', text='with LangChain, Flask, Docker, ChatGPT, anything else).\\n\\nLlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in\\n5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules),\\nto fit their needs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='b3c6544a-6f68-4060-b3ec-27e5d4b9a599', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💡 Contributing'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6abcec78-98c1-4f74-b57b-d8cae4aa7112', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💡 Contributing'}, hash='cdb950bc1703132df9c05c607702201177c1ad5f8f0de9dcfa3f6154a12a3acd')}, hash='4892fb635ac6b11743ca428676ed492ef7d264e440a205a68a0d752d43e3a19c', text='💡 Contributing\\n\\nInterested in contributing? See our [Contribution Guide](CONTRIBUTING.md) for more details.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='e0fc56d6-ec94-476d-a3e4-c007daa2e405', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '📄 Documentation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f44afbd2-0bf3-46f5-8662-309e0cf7fa9c', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '📄 Documentation'}, hash='b01a7435fcbe2962f9b6a2cb397a07c1fed6632941e06a1814f4c4ea2300dc67')}, hash='f0215c48bf198d05ee1d6dcc74e12f70d9310c43f4b4dcea71452c9aec051612', text='📄 Documentation\\n\\nFull documentation can be found here: https://gpt-index.readthedocs.io/en/latest/. \\n\\nPlease check it out for the most up-to-date tutorials, how-to guides, references, and other resources!', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='b583e1f6-e696-42e3-9c87-fa1a12af5cc9', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💻 Example Usage'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f25c47c0-b8bd-451b-81bf-3879c48c55f4', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💻 Example Usage'}, hash='dfe232d846ceae9f0ccbf96e053b01a00cf24382ff4f49f1380830522d8ae86c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='82fcab04-4346-4fba-86ae-612e95285c8a', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💻 Example Usage'}, hash='fe6196075f613ebae9f64bf5b1e04d8324c239e8f256d4455653ccade1da5541')}, hash='9073dfc928908788a3e174fe06f4689c081a6eeafe002180134a57c28c640c83', text='💻 Example Usage\\n\\n```\\npip install llama-index\\n```\\n\\nExamples are in the `examples` folder. Indices are in the `indices` folder (see list of indices below).\\n\\nTo build a simple vector store index:\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \\'YOUR_OPENAI_API_KEY\\'\\n\\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader\\ndocuments = SimpleDirectoryReader(\\'data\\').load_data()\\nindex = VectorStoreIndex.from_documents(documents)\\n```', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='82fcab04-4346-4fba-86ae-612e95285c8a', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💻 Example Usage'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f25c47c0-b8bd-451b-81bf-3879c48c55f4', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💻 Example Usage'}, hash='dfe232d846ceae9f0ccbf96e053b01a00cf24382ff4f49f1380830522d8ae86c'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b583e1f6-e696-42e3-9c87-fa1a12af5cc9', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '💻 Example Usage'}, hash='9073dfc928908788a3e174fe06f4689c081a6eeafe002180134a57c28c640c83')}, hash='fe6196075f613ebae9f64bf5b1e04d8324c239e8f256d4455653ccade1da5541', text='To query:\\n```python\\nquery_engine = index.as_query_engine()\\nquery_engine.query(\"<question_text>?\")\\n```\\n\\n\\nBy default, data is stored in-memory.\\nTo persist to disk (under `./storage`):\\n\\n```python\\nindex.storage_context.persist()\\n```\\n\\nTo reload from disk:\\n```python\\nfrom llama_index import StorageContext, load_index_from_storage\\n\\n# rebuild storage context\\nstorage_context = StorageContext.from_defaults(persist_dir=\\'./storage\\')\\n# load index\\nindex = load_index_from_storage(storage_context)\\n```', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='b2c3437a-7cef-4990-ab3e-6b3f293f3d9f', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🔧 Dependencies'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0f9e96b7-9a47-4053-8a43-b27a444910ee', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '🔧 Dependencies'}, hash='3302ab107310e381d572f2410e8994d0b3737b78acc7729c18f8b7f100fd0078')}, hash='28d0ed4496c3bd0a8f0ace18c11be509eadfae4693a3a239c80a5ec1a6eaedd6', text='🔧 Dependencies\\n\\nThe main third-party package requirements are `tiktoken`, `openai`, and `langchain`.\\n\\nAll requirements should be contained within the `setup.py` file. To run the package locally without building the wheel, simply run `pip install -r requirements.txt`.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), TextNode(id_='a5af8ac3-57dd-4ed7-ab7f-fab6fb435a42', embedding=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '📖 Citation'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='12629a60-c584-4ec9-888d-ea120813f4df', node_type=None, metadata={'filename': 'README.md', 'extension': '.md', 'Header 1': '🗂️ LlamaIndex 🦙', 'Header 2': '📖 Citation'}, hash='ad2d72754f9faa42727bd38ba84f71ad43c9d65bc1b12a8c46d5dc951212f863')}, hash='f7df46992fbea69c394e73961c4d17ea0b49a587420b0c9f47986af12f787950', text='📖 Citation\\n\\nReference to cite if you use LlamaIndex in a paper:\\n\\n```\\n@software{Liu_LlamaIndex_2022,\\nauthor = {Liu, Jerry},\\ndoi = {10.5281/zenodo.1234},\\nmonth = {11},\\ntitle = {{LlamaIndex}},\\nurl = {https://github.com/jerryjliu/llama_index},\\nyear = {2022}\\n}\\n```', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"]}], "source": ["from llama_index.core.ingestion import IngestionPipeline\n", "\n", "pipeline = IngestionPipeline(\n", "    documents=reader.load_data(Path(\"./README.md\")),\n", "    transformations=[\n", "        SimpleFileNodeParser(),\n", "        SentenceSplitter(chunk_size=200, chunk_overlap=0),\n", "    ],\n", ")\n", "\n", "md_chunked_nodes = pipeline.run()\n", "print(md_chunked_nodes)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}