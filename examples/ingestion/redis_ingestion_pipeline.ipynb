{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Redis数据导入管道\n", "\n", "本教程演示了如何在数据导入管道中使用Redis作为向量存储、缓存和文档存储。\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 依赖\n", "\n", "安装并启动redis，设置OpenAI API密钥\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-storage-docstore-redis\n", "%pip install llama-index-vector-stores-redis\n", "%pip install llama-index-embeddings-huggingface"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["338c889086e8649aa80dfb79ebff4fffc98d72fc6d988ac158c6662e9e0cf04b\n"]}], "source": ["!docker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n", "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["```python\n", "# 创建种子数据\n", "\n", "在开始编写代码之前，我们需要创建一些种子数据来填充我们的数据库。这些种子数据将用于开发和测试我们的应用程序。\n", "\n", "我们将使用Python的字典数据结构来表示我们的种子数据。每个字典将代表一个实体，并包含该实体的属性和值。\n", "\n", "下面是一个示例，演示了如何创建一个用户的种子数据：\n", "\n", "seed_users = [\n", "    {\n", "        'username': 'user1',\n", "        'email': 'user1@example.com',\n", "        'password': 'password1'\n", "    },\n", "    {\n", "        'username': 'user2',\n", "        'email': 'user2@example.com',\n", "        'password': 'password2'\n", "    },\n", "    # 更多用户...\n", "]\n", "```\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 生成一些测试数据", "!rm -rf test_redis_data  # 删除已有的test_redis_data文件夹", "!mkdir -p test_redis_data  # 创建test_redis_data文件夹", "!echo \"这是一个测试文件：第一个！\" > test_redis_data/test1.txt  # 写入内容到test1.txt", "!echo \"这是一个测试文件：第二个！\" > test_redis_data/test2.txt  # 写入内容到test2.txt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader", "", "# 使用确定性ID加载文档", "documents = SimpleDirectoryReader(", "    \"./test_redis_data\", filename_as_id=True", ").load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 运行基于Redis的摄入管道\n", "\n", "在连接了向量存储之后，该管道将处理将数据插入到您的向量存储中。\n", "\n", "然而，如果您只想处理重复数据，您可以将策略更改为`DUPLICATES_ONLY`。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.embeddings.huggingface import HuggingFaceEmbedding", "from llama_index.core.ingestion import (", "    DocstoreStrategy,", "    IngestionPipeline,", "    IngestionCache,", ")", "from llama_index.storage.kvstore.redis import RedisKVStore as RedisCache", "from llama_index.storage.docstore.redis import RedisDocumentStore", "from llama_index.core.node_parser import SentenceSplitter", "from llama_index.vector_stores.redis import RedisVectorStore", "", "from redisvl.schema import IndexSchema", "", "", "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")", "", "custom_schema = IndexSchema.from_dict(", "    {", "        \"index\": {\"name\": \"redis_vector_store\", \"prefix\": \"doc\"},", "        # 自定义索引的字段", "        \"fields\": [", "            # llamaindex所需的必要字段", "            {\"type\": \"tag\", \"name\": \"id\"},", "            {\"type\": \"tag\", \"name\": \"doc_id\"},", "            {\"type\": \"text\", \"name\": \"text\"},", "            # 用于bge-small-en-v1.5嵌入的自定义向量字段", "            {", "                \"type\": \"vector\",", "                \"name\": \"vector\",", "                \"attrs\": {", "                    \"dims\": 384,", "                    \"algorithm\": \"hnsw\",", "                    \"distance_metric\": \"cosine\",", "                },", "            },", "        ],", "    }", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pipeline = IngestionPipeline(\n", "    transformations=[\n", "        SentenceSplitter(),\n", "        embed_model,\n", "    ],\n", "    docstore=RedisDocumentStore.from_host_and_port(\n", "        \"localhost\", 6379, namespace=\"document_store\"\n", "    ),\n", "    vector_store=RedisVectorStore(\n", "        schema=custom_schema,\n", "        redis_url=\"redis://localhost:6379\",\n", "    ),\n", "    cache=IngestionCache(\n", "        cache=RedisCache.from_host_and_port(\"localhost\", 6379),\n", "        collection=\"redis_cache\",\n", "    ),\n", "    docstore_strategy=DocstoreStrategy.UPSERTS,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Ingested 2 Nodes\n"]}], "source": ["nodes = pipeline.run(documents=documents)\n", "print(f\"Ingested {len(nodes)} Nodes\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 确认文档已被摄取\n", "\n", "我们可以使用向量存储创建一个向量索引，并快速查询哪些文档已被摄取。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "\n", "index = VectorStoreIndex.from_vector_store(\n", "    pipeline.vector_store, embed_model=embed_model\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["I see two documents.\n"]}], "source": ["print(\n", "    index.as_query_engine(similarity_top_k=10).query(\n", "        \"What documents do you see?\"\n", "    )\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 添加数据和摄取\n", "\n", "在这里，我们可以更新现有文件，也可以添加新文件！\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!echo \"This is a test file: three!\" > test_redis_data/test3.txt\n", "!echo \"This is a NEW test file: one!\" > test_redis_data/test1.txt"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["13:32:07 redisvl.index.index INFO   Index already exists, not overwriting.\n", "Ingested 2 Nodes\n"]}], "source": ["documents = SimpleDirectoryReader(\n", "    \"./test_redis_data\", filename_as_id=True\n", ").load_data()\n", "\n", "nodes = pipeline.run(documents=documents)\n", "\n", "print(f\"Ingested {len(nodes)} Nodes\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["You see three documents: test3.txt, test1.txt, and test2.txt.\n", "This is a test file: three!\n", "This is a NEW test file: one!\n", "This is a test file: two!\n"]}], "source": ["index = VectorStoreIndex.from_vector_store(\n", "    pipeline.vector_store, embed_model=embed_model\n", ")\n", "\n", "response = index.as_query_engine(similarity_top_k=10).query(\n", "    \"What documents do you see?\"\n", ")\n", "\n", "print(response)\n", "\n", "for node in response.source_nodes:\n", "    print(node.get_text())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["正如我们所看到的，数据已经正确地进行了去重和更新操作！即使我们运行了完整的流水线两次，索引中只有三个节点。\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}