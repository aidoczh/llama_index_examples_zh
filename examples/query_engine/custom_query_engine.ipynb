{"cells": [{"attachments": {}, "cell_type": "markdown", "id": "c91d7b0a", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/query_engine/custom_query_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n"]}, {"cell_type": "markdown", "id": "c2598b32-dfc1-48bf-8ccc-719873aeb05f", "metadata": {}, "source": ["# 定义自定义查询引擎\n", "\n", "您可以（也应该）定义自己的自定义查询引擎，以便将其插入到下游的LlamaIndex工作流中，无论您是在构建RAG、代理还是其他应用程序。\n", "\n", "我们提供了一个`CustomQueryEngine`，它可以方便地定义您自己的查询。\n"]}, {"cell_type": "markdown", "id": "f47027d5-19b4-4850-bc14-3f054899f472", "metadata": {}, "source": ["## 设置\n", "\n", "我们首先加载一些示例数据并对其进行索引。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "0193f7c3", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "76a072e4", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "c136286f", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "id": "1b6e547e-af15-4506-9a4f-ec28c4bf3ce8", "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"]}, {"attachments": {}, "cell_type": "markdown", "id": "29e58b49", "metadata": {}, "source": ["下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "8cdaf0f6", "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "id": "4a3b2e77-fbea-451a-b714-696f2c9eb396", "metadata": {}, "outputs": [], "source": ["# 加载文档", "documents = SimpleDirectoryReader(\"./data//paul_graham/\").load_data()"]}, {"cell_type": "code", "execution_count": null, "id": "b94287a5-4ae8-4369-a245-4a8c53790b17", "metadata": {}, "outputs": [], "source": ["index = VectorStoreIndex.from_documents(documents)\n", "retriever = index.as_retriever()"]}, {"cell_type": "markdown", "id": "0697f50b-5238-4b96-a925-68dfc70138fb", "metadata": {}, "source": ["## 构建自定义查询引擎\n", "\n", "我们构建一个自定义查询引擎，模拟一个RAG流水线。首先进行检索，然后进行合成。\n", "\n", "要定义一个 `CustomQueryEngine`，你只需要将一些初始化参数定义为属性，并实现 `custom_query` 函数。\n", "\n", "默认情况下，`custom_query` 可以返回一个 `Response` 对象（响应合成器返回的对象），但也可以只返回一个字符串。这分别是选项1和选项2。\n"]}, {"cell_type": "code", "execution_count": null, "id": "d3d9c2af-bf04-4ce6-b2ed-b6dd12c7176c", "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_engine import CustomQueryEngine\n", "from llama_index.core.retrievers import BaseRetriever\n", "from llama_index.core import get_response_synthesizer\n", "from llama_index.core.response_synthesizers import BaseSynthesizer"]}, {"cell_type": "markdown", "id": "480d6dde-84af-4185-a505-78c95dbf884d", "metadata": {}, "source": ["### 选项 1 (`RAGQueryEngine`)\n"]}, {"cell_type": "code", "execution_count": null, "id": "8fd6c9cb-2240-40da-8e69-1bb3d60913dd", "metadata": {}, "outputs": [], "source": ["class RAGQueryEngine(CustomQueryEngine):", "    \"\"\"RAG查询引擎。\"\"\"", "", "    retriever: BaseRetriever", "    response_synthesizer: BaseSynthesizer", "", "    def custom_query(self, query_str: str):", "        nodes = self.retriever.retrieve(query_str)", "        response_obj = self.response_synthesizer.synthesize(query_str, nodes)", "        return response_obj"]}, {"cell_type": "markdown", "id": "de9b50c1-8c0f-4ca1-9ca0-aa984d7270ae", "metadata": {}, "source": ["### 选项 2 (`RAGStringQueryEngine`)\n"]}, {"cell_type": "code", "execution_count": null, "id": "ef7b34bb-f14a-4e6e-a677-8213cb53f4b8", "metadata": {}, "outputs": [], "source": ["# 选项2：返回一个字符串（我们使用原始LLM调用进行说明）", "", "from llama_index.llms.openai import OpenAI", "from llama_index.core import PromptTemplate", "", "qa_prompt = PromptTemplate(", "    \"下面是上下文信息。\\n\"", "    \"---------------------\\n\"", "    \"{context_str}\\n\"", "    \"---------------------\\n\"", "    \"根据上下文信息和非先验知识，回答查询。\\n\"", "    \"查询：{query_str}\\n\"", "    \"答案：\"", ")", "", "", "class RAGStringQueryEngine(CustomQueryEngine):", "    \"\"\"RAG字符串查询引擎。\"\"\"", "", "    retriever: BaseRetriever", "    response_synthesizer: BaseSynthesizer", "    llm: OpenAI", "    qa_prompt: PromptTemplate", "", "    def custom_query(self, query_str: str):", "        nodes = self.retriever.retrieve(query_str)", "", "        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])", "        response = self.llm.complete(", "            qa_prompt.format(context_str=context_str, query_str=query_str)", "        )", "", "        return str(response)"]}, {"cell_type": "markdown", "id": "cb13986c-0431-4f29-9bc3-924424832373", "metadata": {}, "source": ["## 尝试一下\n", "\n", "现在我们将在我们的样本数据上尝试一下。\n"]}, {"cell_type": "markdown", "id": "022d0ae1-74e8-4cae-a460-8f895ed3b293", "metadata": {}, "source": ["### 尝试选项1（`RAGQueryEngine`）\n"]}, {"cell_type": "code", "execution_count": null, "id": "e2b22e9b-fd1d-40ff-ad66-1718384cb5ed", "metadata": {}, "outputs": [], "source": ["synthesizer = get_response_synthesizer(response_mode=\"compact\")\n", "query_engine = RAGQueryEngine(\n", "    retriever=retriever, response_synthesizer=synthesizer\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "26fd76a0-451c-4796-97f8-4524af499c3a", "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"What did the author do growing up?\")"]}, {"cell_type": "code", "execution_count": null, "id": "71d885de-10fc-4c85-b782-108a7c33805e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author worked on writing and programming outside of school before college. They wrote short stories and tried writing programs on an IBM 1401 computer using an early version of Fortran. They also mentioned getting a microcomputer, building it themselves, and writing simple games and programs on it.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "id": "d4da52b0-77a6-43dc-9860-d1a52cee907d", "metadata": {}, "outputs": [], "source": ["print(response.source_nodes[0].get_content())"]}, {"cell_type": "markdown", "id": "d0c7c860-06c4-445c-95a7-858240053648", "metadata": {}, "source": ["### 尝试选项2 (`RAGStringQueryEngine`)\n"]}, {"cell_type": "code", "execution_count": null, "id": "09bdd1c9-7bd2-4de0-a807-3eb20f6ede46", "metadata": {}, "outputs": [], "source": ["llm = OpenAI(model=\"gpt-3.5-turbo\")\n", "\n", "query_engine = RAGStringQueryEngine(\n", "    retriever=retriever,\n", "    response_synthesizer=synthesizer,\n", "    llm=llm,\n", "    qa_prompt=qa_prompt,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "370a3d91-e16f-4424-b839-5a57b28f21a6", "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"What did the author do growing up?\")"]}, {"cell_type": "code", "execution_count": null, "id": "307831fa-2c8d-4ab2-9d95-78e6c0c61a97", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The author worked on writing and programming before college. They wrote short stories and started programming on the IBM 1401 computer in 9th grade. They later got a microcomputer and continued programming, writing simple games and a word processor.\n"]}], "source": ["print(str(response))"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}