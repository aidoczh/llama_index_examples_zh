{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/query_engine/RetrieverRouterQueryEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# 检索器路由查询引擎\n", "在本教程中，我们基于一个检索器定义了一个路由查询引擎。检索器将选择一组节点，然后我们将选择正确的QueryEngine。\n", "\n", "我们将使用我们的新`ToolRetrieverRouterQueryEngine`类！\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 设置\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["如果您在Colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 注意：这仅在jupyter笔记本中是必要的。", "# 详情：Jupyter在后台运行一个事件循环。", "#       当我们启动一个事件循环来进行异步查询时，这会导致嵌套的事件循环。", "#       通常情况下是不允许的，我们使用nest_asyncio来允许它以方便起见。", "import nest_asyncio", "", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n", "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n", "NumExpr defaulting to 8 threads.\n"]}, {"name": "stderr", "output_type": "stream", "text": ["/Users/jerryliu/Programming/gpt_index/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n"]}], "source": ["import logging\n", "import sys\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n", "\n", "from llama_index.core import (\n", "    VectorStoreIndex,\n", "    SimpleDirectoryReader,\n", "    StorageContext,\n", ")\n", "from llama_index.core import SummaryIndex"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["下载数据\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 加载数据\n", "\n", "我们首先展示如何将一个文档转换为一组节点，并插入到文档存储中。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 加载文档", "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import Settings", "", "# 初始化设置（设置块大小）", "Settings.chunk_size = 1024", "nodes = Settings.node_parser.get_nodes_from_documents(documents)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 初始化存储上下文（默认情况下为内存中）", "storage_context = StorageContext.from_defaults()", "storage_context.docstore.add_documents(nodes)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 在相同数据上定义摘要索引和向量索引\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n", "> [build_index_from_nodes] Total embedding token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 17038 tokens\n", "> [build_index_from_nodes] Total embedding token usage: 17038 tokens\n"]}], "source": ["summary_index = SummaryIndex(nodes, storage_context=storage_context)\n", "vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 为这些索引定义查询引擎和工具\n", "\n", "我们为每个索引定义一个查询引擎。然后我们用我们的 `QueryEngineTool` 对它们进行封装。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.tools import QueryEngineTool\n", "\n", "list_query_engine = summary_index.as_query_engine(\n", "    response_mode=\"tree_summarize\", use_async=True\n", ")\n", "vector_query_engine = vector_index.as_query_engine(\n", "    response_mode=\"tree_summarize\", use_async=True\n", ")\n", "\n", "list_tool = QueryEngineTool.from_defaults(\n", "    query_engine=list_query_engine,\n", "    description=\"Useful for questions asking for a biography of the author.\",\n", ")\n", "vector_tool = QueryEngineTool.from_defaults(\n", "    query_engine=vector_query_engine,\n", "    description=(\n", "        \"Useful for retrieving specific snippets from the author's life, like\"\n", "        \" his time in college, his time in YC, or more.\"\n", "    ),\n", ")"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["### 定义检索增强路由查询引擎\n", "\n", "我们定义了一个带有检索机制的路由查询引擎，以帮助处理选择集过大的情况。\n", "\n", "为了实现这一点，我们首先在查询引擎工具集上定义了一个`ObjectIndex`。`ObjectIndex`定义了一个底层的索引数据结构（例如向量索引、关键词索引），并且可以将`QueryEngineTool`对象序列化到我们的索引中，以及从索引中反序列化`QueryEngineTool`对象。\n", "\n", "然后，我们使用我们的`ToolRetrieverRouterQueryEngine`类，并传入一个针对`QueryEngineTool`对象的`ObjectRetriever`。\n", "这个`ObjectRetriever`对应于我们的`ObjectIndex`。\n", "\n", "在查询时，这个检索器可以动态地检索相关的查询引擎。这使我们能够传入任意数量的查询引擎工具，而不必担心提示的限制。\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "> [build_index_from_nodes] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 59 tokens\n", "> [build_index_from_nodes] Total embedding token usage: 59 tokens\n"]}], "source": ["from llama_index.core import VectorStoreIndex\n", "from llama_index.core.objects import ObjectIndex\n", "\n", "obj_index = ObjectIndex.from_objects(\n", "    [list_tool, vector_tool],\n", "    index_cls=VectorStoreIndex,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_engine import ToolRetrieverRouterQueryEngine\n", "\n", "query_engine = ToolRetrieverRouterQueryEngine(obj_index.as_retriever())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 10 tokens\n", "> [retrieve] Total embedding token usage: 10 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n", "> [retrieve] Total embedding token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2111 tokens\n", "> [get_response] Total LLM token usage: 2111 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n", "> [retrieve] Total embedding token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 2148 tokens\n", "> [get_response] Total LLM token usage: 2148 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n", "INFO:llama_index.query_engine.router_query_engine:Combining responses from multiple query engines.\n", "Combining responses from multiple query engines.\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1063 tokens\n", "> [get_response] Total LLM token usage: 1063 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["response = query_engine.query(\"What is a biography of the author's life?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "The author is a creative person who has had a varied and interesting life. They grew up in the US and went to college, but then decided to take a break and pursue their passion for art. They applied to two art schools, RISD in the US and the Accademia di Belli Arti in Florence, and were accepted to both. They chose to go to Florence, where they took the entrance exam and passed. They then spent a year living in Florence, studying art at the Accademia and painting still lives in their bedroom. After their year in Florence, the author returned to the US and completed their BFA program at RISD. They then went on to pursue a PhD in computer science at MIT, where they wrote a dissertation on the evolution of computers. During their time at MIT, they also did consulting work and wrote essays on topics they had been thinking about. After completing their PhD, the author started a software company, Viaweb, which was eventually acquired by Yahoo. They then went on to write essays and articles about their experiences in the tech industry. They also wrote an essay about how to choose what to work on, which was based on their own experience. The author then moved back to Florence, where they found a rent-stabilized apartment and continued to pursue their interest in art. They wrote about their experiences in the art world, and experienced the reactions of readers to their essays. The author is now a successful writer and continues to write essays and articles about topics they are passionate about. \n", "\n", "In summary, the author's life has been a journey of exploration and creativity. They have experienced a wide range of different things in their life, from art school to computer science to the tech industry, and have used their experiences to inform their writing. They have pursued their passion for art, and have used their knowledge and experience to create meaningful work.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": ["\"\\nThe author is a creative person who has had a varied and interesting life. They grew up in the US and went to college, but then decided to take a break and pursue their passion for art. They applied to two art schools, RISD in the US and the Accademia di Belli Arti in Florence, and were accepted to both. They chose to go to Florence, where they took the entrance exam and passed. They then spent a year living in Florence, studying art at the Accademia and painting still lives in their bedroom. After their year in Florence, the author returned to the US and completed their BFA program at RISD. They then went on to pursue a PhD in computer science at MIT, where they wrote a dissertation on the evolution of computers. During their time at MIT, they also did consulting work and wrote essays on topics they had been thinking about. After completing their PhD, the author started a software company, Viaweb, which was eventually acquired by Yahoo. They then went on to write essays and articles about their experiences in the tech industry. They also wrote an essay about how to choose what to work on, which was based on their own experience. The author then moved back to Florence, where they found a rent-stabilized apartment and continued to pursue their interest in art. They wrote about their experiences in the art world, and experienced the reactions of readers to their essays. The author is now a successful writer and continues to write essays and articles about topics they are passionate about. \\n\\nIn summary, the author's life has been a journey of exploration and creativity. They have experienced a wide range of different things in their life, from art school to computer science to the tech industry, and have used their experiences to inform their writing. They have pursued their passion for art, and have used their knowledge and experience to create meaningful work.\""]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["response"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 11 tokens\n", "> [retrieve] Total embedding token usage: 11 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n", "> [retrieve] Total embedding token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1947 tokens\n", "> [get_response] Total LLM token usage: 1947 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total LLM token usage: 0 tokens\n", "> [retrieve] Total LLM token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [retrieve] Total embedding token usage: 0 tokens\n", "> [retrieve] Total embedding token usage: 0 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 1947 tokens\n", "> [get_response] Total LLM token usage: 1947 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n", "INFO:llama_index.query_engine.router_query_engine:Combining responses from multiple query engines.\n", "Combining responses from multiple query engines.\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total LLM token usage: 316 tokens\n", "> [get_response] Total LLM token usage: 316 tokens\n", "INFO:llama_index.token_counter.token_counter:> [get_response] Total embedding token usage: 0 tokens\n", "> [get_response] Total embedding token usage: 0 tokens\n"]}], "source": ["response = query_engine.query(\n", "    \"What did Paul Graham do during his time in college?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Paul Graham studied philosophy in college, but he did not pursue AI. He continued to work on programming outside of school, writing simple games, a program to predict how high his model rockets would fly, and a word processor. He eventually convinced his father to buy him a TRS-80 computer, which he used to further his programming skills.\n"]}], "source": ["print(str(response))"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}