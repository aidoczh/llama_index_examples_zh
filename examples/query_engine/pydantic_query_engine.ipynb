{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/query_engine/pydantic_query_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨ Colab ä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# ä½¿ç”¨Pydanticè¾“å‡ºçš„æŸ¥è¯¢å¼•æ“\n", "\n", "æ¯ä¸ªæŸ¥è¯¢å¼•æ“éƒ½æ”¯æŒåœ¨`RetrieverQueryEngine`ä¸­ä½¿ç”¨ä»¥ä¸‹`response_mode`æ¥é›†æˆç»“æ„åŒ–å“åº”ï¼š\n", "- `refine`\n", "- `compact`\n", "- `tree_summarize`\n", "- `accumulate`ï¼ˆæµ‹è¯•ç‰ˆï¼Œéœ€è¦é¢å¤–è§£æä»¥è½¬æ¢ä¸ºå¯¹è±¡ï¼‰\n", "- `compact_accumulate`ï¼ˆæµ‹è¯•ç‰ˆï¼Œéœ€è¦é¢å¤–è§£æä»¥è½¬æ¢ä¸ºå¯¹è±¡ï¼‰\n", "\n", "åœ¨è¿™ä¸ªç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€ä¸ªå°ä¾‹å­æ¥æ¼”ç¤ºä½¿ç”¨æ–¹æ³•ã€‚\n", "\n", "åœ¨åº•å±‚ï¼Œæ¯ä¸ªLLMå“åº”éƒ½å°†æ˜¯ä¸€ä¸ªpydanticå¯¹è±¡ã€‚å¦‚æœè¯¥å“åº”éœ€è¦è¢«ç²¾ç‚¼æˆ–æ€»ç»“ï¼Œå®ƒå°†è¢«è½¬æ¢ä¸ºä¸‹ä¸€ä¸ªå“åº”çš„JSONå­—ç¬¦ä¸²ã€‚ç„¶åï¼Œæœ€ç»ˆçš„å“åº”å°†ä½œä¸ºä¸€ä¸ªpydanticå¯¹è±¡è¿”å›ã€‚\n", "\n", "**æ³¨æ„ï¼š** ä»æŠ€æœ¯ä¸Šè®²ï¼Œè¿™å¯ä»¥ä¸ä»»ä½•LLMä¸€èµ·ä½¿ç”¨ï¼Œä½†éOpenAIçš„æ”¯æŒä»åœ¨å¼€å‘ä¸­ï¼Œè¢«è§†ä¸ºæµ‹è¯•ç‰ˆã€‚\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## è®¾ç½®\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-anthropic\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import openai\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["ä¸‹è½½æ•°æ®\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader\n", "\n", "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### åˆ›å»ºæˆ‘ä»¬çš„Pydanticè¾“å‡ºå¯¹è±¡\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from typing import List\n", "from pydantic import BaseModel\n", "\n", "\n", "class Biography(BaseModel):\n", "    \"\"\"ä¼ è®°çš„æ•°æ®æ¨¡å‹ã€‚\"\"\"\n", "\n", "    name: str\n", "    best_known_for: List[str]\n", "    extra_info: str"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## åˆ›å»ºç´¢å¼• + æŸ¥è¯¢å¼•æ“ï¼ˆOpenAIï¼‰\n", "\n", "åœ¨ä½¿ç”¨OpenAIæ—¶ï¼Œå°†åˆ©ç”¨å‡½æ•°è°ƒç”¨APIæ¥è·å¾—å¯é çš„ç»“æ„åŒ–è¾“å‡ºã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "from llama_index.llms.openai import OpenAI\n", "\n", "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n", "\n", "index = VectorStoreIndex.from_documents(\n", "    documents,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(\n", "    output_cls=Biography, response_mode=\"compact\", llm=llm\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"Who is Paul Graham?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Paul Graham\n", "['working on Bel', 'co-founding Viaweb', 'creating the programming language Arc']\n", "Paul Graham is a computer scientist, entrepreneur, and writer. He is best known for his work on Bel, a programming language, and for co-founding Viaweb, an early web application company that was later acquired by Yahoo. Graham also created the programming language Arc. He has written numerous essays on topics such as startups, programming, and life.\n"]}], "source": ["print(response.name)\n", "print(response.best_known_for)\n", "print(response.extra_info)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class '__main__.Biography'>\n"]}], "source": ["# è·å–å®Œæ•´çš„pydanitcå¯¹è±¡\n", "print(type(response.response))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## åˆ›å»ºç´¢å¼• + æŸ¥è¯¢å¼•æ“ï¼ˆéOpenAIï¼ŒBetaï¼‰\n", "\n", "å½“ä½¿ç”¨ä¸æ”¯æŒå‡½æ•°è°ƒç”¨çš„LLMæ—¶ï¼Œæˆ‘ä»¬ä¾èµ–LLMè‡ªå·±ç¼–å†™JSONï¼Œå¹¶å°†JSONè§£æä¸ºé€‚å½“çš„pydanticå¯¹è±¡ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex\n", "from llama_index.llms.anthropic import Anthropic\n", "\n", "llm = Anthropic(model=\"claude-instant-1.2\", temperature=0.1)\n", "\n", "index = VectorStoreIndex.from_documents(\n", "    documents,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(\n", "    output_cls=Biography, response_mode=\"tree_summarize\", llm=llm\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"Who is Paul Graham?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Paul Graham\n", "['Co-founder of Y Combinator', 'Essayist and programmer']\n", "He is known for creating Viaweb, one of the first web application builders, and for founding Y Combinator, one of the world's top startup accelerators. Graham has also written extensively about technology, investing, and philosophy.\n"]}], "source": ["print(response.name)\n", "print(response.best_known_for)\n", "print(response.extra_info)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["<class '__main__.Biography'>\n"]}], "source": ["# è·å–å®Œæ•´çš„pydanitcå¯¹è±¡\n", "print(type(response.response))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## ç´¯ç§¯ç¤ºä¾‹ï¼ˆBetaï¼‰\n", "\n", "ä½¿ç”¨pydanticå¯¹è±¡è¿›è¡Œç´¯ç§¯éœ€è¦ä¸€äº›é¢å¤–çš„è§£æã€‚è¿™ä»ç„¶æ˜¯ä¸€ä¸ªæµ‹è¯•åŠŸèƒ½ï¼Œä½†ä»ç„¶å¯ä»¥å¯¹pydanticå¯¹è±¡è¿›è¡Œç´¯ç§¯ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from typing import List\n", "from pydantic import BaseModel\n", "\n", "class Company(BaseModel):\n", "    \"\"\"å…¬å¸æåŠçš„æ•°æ®æ¨¡å‹ã€‚\"\"\"\n", "\n", "    company_name: str\n", "    context_info: str"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex,\n", "from llama_index.llms.openai import OpenAI\n", "\n", "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n", "\n", "index = VectorStoreIndex.from_documents(\n", "    documents,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = index.as_query_engine(\n", "    output_cls=Company, response_mode=\"accumulate\", llm=llm\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["response = query_engine.query(\"What companies are mentioned in the text?\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["åœ¨accumulateä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œå“åº”æ˜¯ç”±ä¸€ä¸ªé»˜è®¤åˆ†éš”ç¬¦åˆ†éš”ï¼Œå¹¶åœ¨å‰é¢åŠ ä¸Šä¸€ä¸ªå‰ç¼€ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["companies = []\n", "\n", "# ä»¥é»˜è®¤åˆ†éš”ç¬¦æ‹†åˆ†\n", "for response_str in str(response).split(\"\\n---------------------\\n\"):\n", "    # ç§»é™¤å‰ç¼€ -- æ¯ä¸ªå“åº”éƒ½ä»¥`Response 1: {...}`å¼€å¤´\n", "    # å› æ­¤ï¼Œæˆ‘ä»¬æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ‹¬å·å¹¶ç§»é™¤å®ƒä¹‹å‰çš„æ‰€æœ‰å†…å®¹\n", "    response_str = response_str[response_str.find(\"{\") :]\n", "    companies.append(Company.parse_raw(response_str))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[Company(company_name='Yahoo', context_info='Yahoo bought us'), Company(company_name='Yahoo', context_info=\"I'd been meaning to since Yahoo bought us\")]\n"]}], "source": ["print(companies)"]}], "metadata": {"kernelspec": {"display_name": "llama-index", "language": "python", "name": "llama-index"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 2}