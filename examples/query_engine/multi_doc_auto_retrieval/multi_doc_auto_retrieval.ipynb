{"cells": [{"cell_type": "markdown", "id": "010a246d-56ec-44b6-98a1-cca3723d784c", "metadata": {}, "source": ["# 结构化分层检索\n", "\n", "对多个文档进行良好的RAG是困难的。一个通用的框架是在给定用户查询后，首先选择相关文档，然后再选择其中的内容。\n", "\n", "但是选择文档可能很困难 - 我们如何根据用户查询动态选择具有不同属性的文档呢？\n", "\n", "在这个笔记本中，我们将向您展示我们的多文档RAG架构：\n", "\n", "- 将每个文档表示为一个简洁的**元数据**字典，其中包含不同的属性：提取的摘要以及结构化元数据。\n", "- 将这些元数据字典存储为向量数据库中的过滤器。\n", "- 给定用户查询，首先进行**自动检索** - 推断相关的语义查询和一组用于查询这些数据的过滤器（有效地结合了文本到SQL和语义搜索）。\n"]}, {"cell_type": "code", "execution_count": null, "id": "b1d4bb83", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-readers-github\n", "%pip install llama-index-vector-stores-weaviate\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "7a599ce1-48b1-44f6-846e-b9d3463635fd", "metadata": {}, "outputs": [], "source": ["!pip install llama-index llama-hub"]}, {"cell_type": "markdown", "id": "da8ab861-ea33-4057-8634-b3529c577d29", "metadata": {}, "source": ["## 设置和下载数据\n", "\n", "在这一部分，我们将加载LlamaIndex Github的问题。\n"]}, {"cell_type": "code", "execution_count": null, "id": "b49dc9d9-668a-4046-b86a-0ab39d74c9a8", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "id": "12776eb2-c61d-4661-89a5-b7d3be2d8c93", "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"GITHUB_TOKEN\"] = \"ghp_...\"\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "id": "c423120f-0dfe-4e77-aee5-3325c5d1442e", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Found 100 issues in the repo page 1\n", "Resulted in 100 documents\n", "Found 100 issues in the repo page 2\n", "Resulted in 200 documents\n", "Found 100 issues in the repo page 3\n", "Resulted in 300 documents\n", "Found 64 issues in the repo page 4\n", "Resulted in 364 documents\n", "No more issues found, stopping\n"]}], "source": ["import os\n", "\n", "from llama_index.readers.github import (\n", "    GitHubRepositoryIssuesReader,\n", "    GitHubIssuesClient,\n", ")\n", "\n", "github_client = GitHubIssuesClient()\n", "loader = GitHubRepositoryIssuesReader(\n", "    github_client,\n", "    owner=\"run-llama\",\n", "    repo=\"llama_index\",\n", "    verbose=True,\n", ")\n", "\n", "orig_docs = loader.load_data()\n", "\n", "limit = 100\n", "\n", "docs = []\n", "for idx, doc in enumerate(orig_docs):\n", "    doc.metadata[\"index_id\"] = int(doc.id_)\n", "    if idx >= limit:\n", "        break\n", "    docs.append(doc)"]}, {"cell_type": "markdown", "id": "f142129c", "metadata": {}, "source": ["## 设置向量存储和索引\n"]}, {"cell_type": "code", "execution_count": null, "id": "4f17643d", "metadata": {}, "outputs": [], "source": ["import weaviate", "", "# 云", "auth_config = weaviate.AuthApiKey(", "    api_key=\"XRa15cDIkYRT7AkrpqT6jLfE4wropK1c1TGk\"", ")", "client = weaviate.Client(", "    \"https://llama-index-test-v0oggsoz.weaviate.network\",", "    auth_client_secret=auth_config,", ")", "", "class_name = \"LlamaIndex_docs\""]}, {"cell_type": "code", "execution_count": null, "id": "75615d58", "metadata": {}, "outputs": [], "source": ["# 可选：删除模式", "client.schema.delete_class(class_name)"]}, {"cell_type": "code", "execution_count": null, "id": "b67acd54", "metadata": {}, "outputs": [], "source": ["from llama_index.vector_stores.weaviate import WeaviateVectorStore\n", "from llama_index.core import VectorStoreIndex, StorageContext\n", "\n", "vector_store = WeaviateVectorStore(\n", "    weaviate_client=client, index_name=class_name\n", ")\n", "storage_context = StorageContext.from_defaults(vector_store=vector_store)"]}, {"cell_type": "code", "execution_count": null, "id": "82d8ffb5", "metadata": {}, "outputs": [], "source": ["doc_index = VectorStoreIndex.from_documents(\n", "    docs, storage_context=storage_context\n", ")"]}, {"cell_type": "markdown", "id": "4cb80399", "metadata": {}, "source": ["## 创建用于检索和过滤的IndexNodes\n"]}, {"cell_type": "code", "execution_count": null, "id": "ee7f89b0-4c55-4bfe-83c8-8539b7939de2", "metadata": {}, "outputs": [], "source": ["from llama_index.core import SummaryIndex", "from llama_index.core.async_utils import run_jobs", "from llama_index.llms.openai import OpenAI", "from llama_index.core.schema import IndexNode", "from llama_index.core.vector_stores import (", "    FilterOperator,", "    MetadataFilter,", "    MetadataFilters,", ")", "", "", "async def aprocess_doc(doc, include_summary: bool = True):", "    \"\"\"处理文档。\"\"\"", "    metadata = doc.metadata", "", "    date_tokens = metadata[\"created_at\"].split(\"T\")[0].split(\"-\")", "    year = int(date_tokens[0])", "    month = int(date_tokens[1])", "    day = int(date_tokens[2])", "", "    assignee = (", "        \"\" if \"assignee\" not in doc.metadata else doc.metadata[\"assignee\"]", "    )", "    size = \"\"", "    if len(doc.metadata[\"labels\"]) > 0:", "        size_arr = [l for l in doc.metadata[\"labels\"] if \"size:\" in l]", "        size = size_arr[0].split(\":\")[1] if len(size_arr) > 0 else \"\"", "    new_metadata = {", "        \"state\": metadata[\"state\"],", "        \"year\": year,", "        \"month\": month,", "        \"day\": day,", "        \"assignee\": assignee,", "        \"size\": size,", "    }", "", "    # 现在提取摘要", "    summary_index = SummaryIndex.from_documents([doc])", "    query_str = \"给出这个问题的一句简洁的摘要。\"", "    query_engine = summary_index.as_query_engine(", "        llm=OpenAI(model=\"gpt-3.5-turbo\")", "    )", "    summary_txt = await query_engine.aquery(query_str)", "    summary_txt = str(summary_txt)", "", "    index_id = doc.metadata[\"index_id\"]", "    # 过滤特定的文档id", "    filters = MetadataFilters(", "        filters=[", "            MetadataFilter(", "                key=\"index_id\", operator=FilterOperator.EQ, value=int(index_id)", "            ),", "        ]", "    )", "", "    # 使用摘要文本创建索引节点", "    index_node = IndexNode(", "        text=summary_txt,", "        metadata=new_metadata,", "        obj=doc_index.as_retriever(filters=filters),", "        index_id=doc.id_,", "    )", "", "    return index_node", "", "", "async def aprocess_docs(docs):", "    \"\"\"处理文档的元数据。\"\"\"", "", "    index_nodes = []", "    tasks = []", "    for doc in docs:", "        task = aprocess_doc(doc)", "        tasks.append(task)", "", "    index_nodes = await run_jobs(tasks, show_progress=True, workers=3)", "", "    return index_nodes"]}, {"cell_type": "code", "execution_count": null, "id": "17728251-e7c8-47eb-b139-ee0a7246f894", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["  1%|          | 1/100 [00:00<00:55,  1.78it/s]/home/loganm/llama_index_proper/llama_index/.venv/lib/python3.11/site-packages/openai/_resource.py:38: ResourceWarning: unclosed <socket.socket fd=71, family=2, type=1, proto=6, laddr=('172.25.21.0', 40832), raddr=('104.18.7.192', 443)>\n", "  self._delete = client.delete\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=73 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=71 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", " 12%|█▏        | 12/100 [00:04<00:31,  2.79it/s]/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=76 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=77 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=78 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/llama_index_proper/llama_index/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:1337: ResourceWarning: unclosed <socket.socket fd=81, family=2, type=1, proto=6, laddr=('172.25.21.0', 40848), raddr=('104.18.7.192', 443)>\n", "  completions.create,\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=81 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=82 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=83 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=84 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", " 21%|██        | 21/100 [00:06<00:22,  3.58it/s]/home/loganm/llama_index_proper/llama_index/.venv/lib/python3.11/site-packages/openai/_resource.py:34: ResourceWarning: unclosed <socket.socket fd=81, family=2, type=1, proto=6, laddr=('172.25.21.0', 40866), raddr=('104.18.7.192', 443)>\n", "  self._get = client.get\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/llama_index_proper/llama_index/.venv/lib/python3.11/site-packages/openai/_resource.py:34: ResourceWarning: unclosed <socket.socket fd=82, family=2, type=1, proto=6, laddr=('172.25.21.0', 40868), raddr=('104.18.7.192', 443)>\n", "  self._get = client.get\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=86 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", " 38%|███▊      | 38/100 [00:12<00:24,  2.54it/s]/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=90 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=92 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/llama_index_proper/llama_index/.venv/lib/python3.11/site-packages/openai/_resource.py:34: ResourceWarning: unclosed <socket.socket fd=94, family=2, type=1, proto=6, laddr=('172.25.21.0', 40912), raddr=('104.18.7.192', 443)>\n", "  self._get = client.get\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=94 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", " 50%|█████     | 50/100 [00:17<00:19,  2.51it/s]/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=95 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=96 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=97 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", " 73%|███████▎  | 73/100 [00:24<00:07,  3.42it/s]/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=101 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=102 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", " 82%|████████▏ | 82/100 [00:27<00:06,  2.94it/s]/home/loganm/miniconda3/envs/llama_index/lib/python3.11/functools.py:76: ResourceWarning: unclosed <socket.socket fd=102, family=2, type=1, proto=6, laddr=('172.25.21.0', 40998), raddr=('104.18.7.192', 443)>\n", "  return partial(update_wrapper, wrapped=wrapped,\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", " 92%|█████████▏| 92/100 [00:32<00:03,  2.15it/s]/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=106 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "/home/loganm/miniconda3/envs/llama_index/lib/python3.11/asyncio/selector_events.py:835: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=111 read=idle write=<idle, bufsize=0>>\n", "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n", "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n", "100%|██████████| 100/100 [00:36<00:00,  2.71it/s]\n"]}], "source": ["index_nodes = await aprocess_docs(docs)"]}, {"cell_type": "code", "execution_count": null, "id": "c6907607-47b7-4966-9501-6c5320ec66e5", "metadata": {}, "outputs": [{"data": {"text/plain": ["{'state': 'open',\n", " 'year': 2024,\n", " 'month': 1,\n", " 'day': 13,\n", " 'assignee': '',\n", " 'size': 'XL'}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["index_nodes[5].metadata"]}, {"cell_type": "markdown", "id": "773585a7-3027-4a12-9349-7320822514e0", "metadata": {}, "source": ["## 创建顶层AutoRetriever\n", "\n", "我们将摘要元数据和原始文档加载到向量数据库中。\n", "1. **摘要元数据**：存储在 `LlamaIndex_auto` 集合中。\n", "2. **原始文档**：存储在 `LlamaIndex_docs` 集合中。\n", "\n", "通过存储摘要元数据和原始文档，我们可以执行结构化的、分层的检索策略。\n", "\n", "我们加载到支持自动检索的向量数据库中。\n"]}, {"cell_type": "markdown", "id": "55c7baf6-4267-4543-8a2c-7b44a8fcd017", "metadata": {}, "source": ["### 加载汇总的元数据\n", "\n", "这将进入 `LlamaIndex_auto`\n"]}, {"cell_type": "code", "execution_count": null, "id": "7fcbe94d-0fe4-48b1-954c-31d0f278ebc0", "metadata": {}, "outputs": [], "source": ["import weaviate", "", "# 云", "auth_config = weaviate.AuthApiKey(", "    api_key=\"XRa15cDIkYRT7AkrpqT6jLfE4wropK1c1TGk\"", ")", "client = weaviate.Client(", "    \"https://llama-index-test-v0oggsoz.weaviate.network\",", "    auth_client_secret=auth_config,", ")", "", "class_name = \"LlamaIndex_auto\""]}, {"cell_type": "code", "execution_count": null, "id": "782b544b-2e8e-4fde-a35f-fb77133a0cef", "metadata": {}, "outputs": [], "source": ["# 可选：删除模式", "client.schema.delete_class(class_name)"]}, {"cell_type": "code", "execution_count": null, "id": "dd78a330", "metadata": {}, "outputs": [], "source": ["from llama_index.vector_stores.weaviate import WeaviateVectorStore\n", "from llama_index.core import VectorStoreIndex, StorageContext\n", "\n", "vector_store_auto = WeaviateVectorStore(\n", "    weaviate_client=client, index_name=class_name\n", ")\n", "storage_context_auto = StorageContext.from_defaults(\n", "    vector_store=vector_store_auto\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "87910444-2cfb-4c47-8821-83d679486700", "metadata": {}, "outputs": [], "source": ["# 由于“index_nodes”是简洁的摘要，我们可以直接将它们作为对象输入到VectorStoreIndex中。 ", "index = VectorStoreIndex(", "    objects=index_nodes, storage_context=storage_context_auto", ")"]}, {"cell_type": "markdown", "id": "f44433ac-3a4c-4bda-bb5b-6bbfd18c1ddf", "metadata": {}, "source": ["## 设置可组合的自动检索器\n", "\n", "在这一部分，我们将设置我们的自动检索器。我们需要执行一些步骤。\n", "\n", "1. **定义模式**：定义向量数据库模式（例如元数据字段）。这将被放入LLM输入提示中，用于确定要推断的元数据过滤器。\n", "2. **实例化VectorIndexAutoRetriever类**：这将在我们总结的元数据索引之上创建一个检索器，并将定义的模式作为输入。\n", "3. **定义包装检索器**：这允许我们将每个节点后处理为`IndexNode`，并使用索引ID将其链接回源文档。这将允许我们在下一节进行递归检索（依赖于链接到下游检索器/查询引擎/其他节点的IndexNode对象）。**注意**：我们正在努力改进这个抽象。\n", "\n", "运行此检索器将基于我们的文本摘要和顶层`IndexNode`对象的元数据进行检索。然后，它们的基础检索器将用于从特定的GitHub问题中检索内容。\n"]}, {"cell_type": "markdown", "id": "8c926128-c1aa-4f42-8061-b3f2df1272d0", "metadata": {}, "source": ["### 1. 定义模式\n"]}, {"cell_type": "code", "execution_count": null, "id": "5d01aade-d676-49db-b851-2ac62b4e53c5", "metadata": {}, "outputs": [], "source": ["from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n", "\n", "\n", "vector_store_info = VectorStoreInfo(\n", "    content_info=\"Github Issues\",\n", "    metadata_info=[\n", "        MetadataInfo(\n", "            name=\"state\",\n", "            description=\"Whether the issue is `open` or `closed`\",\n", "            type=\"string\",\n", "        ),\n", "        MetadataInfo(\n", "            name=\"year\",\n", "            description=\"The year issue was created\",\n", "            type=\"integer\",\n", "        ),\n", "        MetadataInfo(\n", "            name=\"month\",\n", "            description=\"The month issue was created\",\n", "            type=\"integer\",\n", "        ),\n", "        MetadataInfo(\n", "            name=\"day\",\n", "            description=\"The day issue was created\",\n", "            type=\"integer\",\n", "        ),\n", "        MetadataInfo(\n", "            name=\"assignee\",\n", "            description=\"The assignee of the ticket\",\n", "            type=\"string\",\n", "        ),\n", "        MetadataInfo(\n", "            name=\"size\",\n", "            description=\"How big the issue is (XS, S, M, L, XL, XXL)\",\n", "            type=\"string\",\n", "        ),\n", "    ],\n", ")"]}, {"cell_type": "markdown", "id": "d6a69271-e597-40c6-88aa-a755bfd75754", "metadata": {}, "source": ["### 2. 实例化VectorIndexAutoRetriever\n"]}, {"cell_type": "code", "execution_count": null, "id": "9125a832-940a-44f3-be91-ad17cdfc267b", "metadata": {}, "outputs": [], "source": ["from llama_index.core.retrievers import VectorIndexAutoRetriever", "", "retriever = VectorIndexAutoRetriever(", "    index,", "    vector_store_info=vector_store_info,", "    similarity_top_k=2,", "    empty_query_top_k=10,  # 如果只指定了元数据过滤器，则这是限制", "    verbose=True,", ")"]}, {"cell_type": "markdown", "id": "97a91a51-9a2c-447e-9848-53ee0a705baa", "metadata": {}, "source": ["## 试一试\n", "\n", "现在我们可以开始在Github Issues中检索相关的上下文了！\n", "\n", "为了完成RAG管道的设置，我们将把我们的递归检索器与我们的`RetrieverQueryEngine`结合起来，以生成响应以及检索到的节点。\n"]}, {"cell_type": "markdown", "id": "6a2032ef-e776-49d8-b8d4-f8b79fbd3599", "metadata": {}, "source": ["### 尝试检索\n"]}, {"cell_type": "code", "execution_count": null, "id": "7136a40b-c615-4f32-bc98-fdc3f72f3085", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Using query str: issues\n", "Using filters: [('day', '==', '11'), ('month', '==', '01')]\n", "\u001b[1;3;38;2;11;159;203mRetrieval entering 9995: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n", "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 9985: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n", "\u001b[0m"]}], "source": ["from llama_index.core import QueryBundle\n", "\n", "nodes = retriever.retrieve(QueryBundle(\"Tell me about some issues on 01/11\"))"]}, {"cell_type": "markdown", "id": "002f3135-6b88-45fa-a25d-1b4e7e977489", "metadata": {}, "source": ["结果是相关文档中的源代码块。\n", "\n", "让我们看一下源代码块附加的日期（原始元数据中存在）。\n"]}, {"cell_type": "code", "execution_count": null, "id": "3804c74f-2bb5-4935-b15b-ff16ce0a7475", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Number of source nodes: 2\n"]}, {"data": {"text/plain": ["{'state': 'open',\n", " 'created_at': '2024-01-11T20:37:34Z',\n", " 'url': 'https://api.github.com/repos/run-llama/llama_index/issues/9995',\n", " 'source': 'https://github.com/run-llama/llama_index/pull/9995',\n", " 'labels': ['size:XXL'],\n", " 'index_id': 9995}"]}, "execution_count": null, "metadata": {}, "output_type": "execute_result"}], "source": ["print(f\"Number of source nodes: {len(nodes)}\")\n", "nodes[0].node.metadata"]}, {"cell_type": "markdown", "id": "97cba7dd-b5f5-4759-9dcf-b9ee06a7ec29", "metadata": {}, "source": ["### 插入到 `RetrieverQueryEngine` 中\n", "\n", "我们插入到 `RetrieverQueryEngine` 中以合成结果。\n"]}, {"cell_type": "code", "execution_count": null, "id": "c9bdaf13-71b4-43c4-b49c-8c9a109819f7", "metadata": {}, "outputs": [], "source": ["from llama_index.core.query_engine import RetrieverQueryEngine\n", "from llama_index.llms.openai import OpenAI\n", "\n", "llm = OpenAI(model=\"gpt-3.5-turbo\")\n", "\n", "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"]}, {"cell_type": "code", "execution_count": null, "id": "10d52659-c297-458c-aba6-496995655ba2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Using query str: issues\n", "Using filters: [('day', '==', '11'), ('month', '==', '01')]\n", "\u001b[1;3;38;2;11;159;203mRetrieval entering 9995: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n", "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 9985: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query issues\n", "\u001b[0m"]}], "source": ["response = query_engine.query(\"Tell me about some issues on 01/11\")"]}, {"cell_type": "code", "execution_count": null, "id": "f591dae0-0438-4018-9732-c5aa9357938a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["There are two issues that were created on 01/11. The first issue is related to ensuring backwards compatibility with the new Pinecone client version bifurcation. The second issue is a feature request to implement the Language Agent Tree Search (LATS) agent in llama-index.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "code", "execution_count": null, "id": "6450df09-f8c4-4dee-aa10-6c85c0ea362b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Using query str: agents\n", "Using filters: [('state', '==', 'open')]\n", "\u001b[1;3;38;2;11;159;203mRetrieval entering 10058: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query agents\n", "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 9899: VectorIndexRetriever\n", "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object VectorIndexRetriever with query agents\n", "\u001b[0m"]}], "source": ["response = query_engine.query(\n", "    \"Tell me about some open issues related to agents\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "cb0d36e2-f4c4-4cca-a4b9-ddd4f6d2e9a1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["There are two open issues related to agents. One issue is about adding context for agents, updating a stale link, and adding a notebook to demo a react agent with context. The other issue is a feature request for parallelism when using the top agent from a multi-document agent while comparing multiple documents.\n"]}], "source": ["print(str(response))"]}, {"cell_type": "markdown", "id": "230a7945-0a77-4d0c-a324-f24e2398c146", "metadata": {}, "source": ["## 总结思路\n", "\n", "这展示了如何在文档摘要上创建一个结构化的检索层，使您能够根据用户查询动态地获取相关文档。\n", "\n", "您可能会注意到这与我们的[多文档代理](https://docs.llamaindex.ai/en/stable/examples/agent/multi_document_agents.html)之间的相似之处。这两种架构都旨在实现强大的多文档检索。\n", "\n", "本笔记的目标是向您展示如何在多文档设置中应用结构化查询。实际上，您也可以将这种自动检索算法应用到我们的多代理设置中。多代理设置主要专注于在文档和每个文档之间添加代理推理，使用思维链实现多部分查询。\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}