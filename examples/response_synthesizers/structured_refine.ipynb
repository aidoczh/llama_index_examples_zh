{"cells": [{"cell_type": "markdown", "id": "def266be", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/response_synthesizers/structured_refine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "540ff471-dcea-4b3e-9c0c-a3173f1c640e", "metadata": {}, "source": ["# 通过结构化答案过滤进行优化\n", "在使用我们的Refine响应合成器进行响应合成时，过滤掉非答案非常关键。经常遇到的问题是传播单个无用的响应，比如“我不知道答案”，这可能会在合成过程中持续存在，并导致最终得到相同性质的答案。即使其他更相关部分中存在实际答案，这种情况也可能发生。\n", "\n", "可以通过将`structured_answer_filtering`设置为`True`来过滤掉这些无用的响应。默认情况下它设置为`False`，因为目前只有在使用支持函数调用的OpenAI模型时才能发挥最佳作用。\n"]}, {"cell_type": "markdown", "id": "154d14c2", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "2e9a84d2", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "f9584f42", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"attachments": {}, "cell_type": "markdown", "id": "158b08a8-32d3-4397-ad37-75870416226b", "metadata": {}, "source": ["## 加载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "bf6b6f5c-5852-41be-8ce8-d94c520e0e50", "metadata": {}, "outputs": [], "source": ["texts = [\n", "    \"The president in the year 2040 is John Cena.\",\n", "    \"The president in the year 2050 is Florence Pugh.\",\n", "    'The president in the year 2060 is Dwayne \"The Rock\" Johnson.',\n", "]"]}, {"attachments": {}, "cell_type": "markdown", "id": "efed56ee-fcd3-439c-a1b2-53c643f15c8e", "metadata": {}, "source": ["## 总结\n"]}, {"cell_type": "code", "execution_count": null, "id": "903f9dcb", "metadata": {}, "outputs": [], "source": ["import os\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""]}, {"cell_type": "code", "execution_count": null, "id": "4ae4dad4-6044-4c9c-becd-4e2908b54a30", "metadata": {}, "outputs": [], "source": ["from llama_index.llms.openai import OpenAI\n", "\n", "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")"]}, {"cell_type": "code", "execution_count": null, "id": "52c48278-f5b2-47bb-a240-6b66a191c6db", "metadata": {}, "outputs": [], "source": ["from llama_index.core import get_response_synthesizer\n", "\n", "summarizer = get_response_synthesizer(\n", "    response_mode=\"refine\", llm=llm, verbose=True\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "834ac725-54ce-4243-bc09-4a50e2590b28", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["> Refine context: The president in the year 2050 is Florence Pugh.\n", "> Refine context: The president in the year 2060 is Dwayne \"The R...\n"]}], "source": ["response = summarizer.get_response(\"who is president in the year 2050?\", texts)"]}, {"cell_type": "markdown", "id": "8cc2744b", "metadata": {}, "source": ["### 失败的结果\n", "正如你所看到的，由于最初的“我不知道”回答一直传播到响应合成的最后，我们无法从输入的“texts”字符串中得到正确的答案。\n"]}, {"cell_type": "code", "execution_count": null, "id": "a600aa73-74b8-4a20-8f56-1b273417f788", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["I'm sorry, but I don't have access to information about the future.\n"]}], "source": ["print(response)"]}, {"cell_type": "markdown", "id": "218b85d5", "metadata": {}, "source": ["现在我们将再次尝试使用 `structured_answer_filtering=True`。\n"]}, {"cell_type": "code", "execution_count": null, "id": "27488623", "metadata": {}, "outputs": [], "source": ["from llama_index.core import get_response_synthesizer\n", "\n", "summarizer = get_response_synthesizer(\n", "    response_mode=\"refine\",\n", "    llm=llm,\n", "    verbose=True,\n", "    structured_answer_filtering=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "8eac8681", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Function call: StructuredRefineResponse with args: {\n", "  \"answer\": \"It is not possible to determine who the president is in the year 2050 based on the given context information.\",\n", "  \"query_satisfied\": false\n", "}\n", "> Refine context: The president in the year 2050 is Florence Pugh.\n", "Function call: StructuredRefineResponse with args: {\n", "  \"answer\": \"Florence Pugh\",\n", "  \"query_satisfied\": true\n", "}\n", "> Refine context: The president in the year 2060 is Dwayne \"The R...\n", "Function call: StructuredRefineResponse with args: {\n", "  \"answer\": \"Florence Pugh\",\n", "  \"query_satisfied\": false\n", "}\n"]}], "source": ["response = summarizer.get_response(\"who is president in the year 2050?\", texts)"]}, {"cell_type": "markdown", "id": "e3ed92fb", "metadata": {}, "source": ["### 成功的结果\n", "正如你所看到的，我们能够通过过滤“texts”字符串，找到实际包含我们问题答案的字符串，从而确定了正确的答案。\n"]}, {"cell_type": "code", "execution_count": null, "id": "cf0503c8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Florence Pugh\n"]}], "source": ["print(response)"]}, {"cell_type": "markdown", "id": "be6668e7", "metadata": {}, "source": ["## 无需函数调用的LLMs\n", "您可能希望在不提供函数调用API的LLM中使用此过滤功能。\n", "\n", "在这种情况下，`Refine` 模块将自动切换到使用结构化输出 `Program`，而不依赖于外部函数调用API。\n"]}, {"cell_type": "code", "execution_count": null, "id": "92f6f384", "metadata": {}, "outputs": [], "source": ["# 我们将继续使用OpenAI，但使用一个不支持函数调用的旧模型", "instruct_llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")"]}, {"cell_type": "code", "execution_count": null, "id": "53fddd33", "metadata": {}, "outputs": [], "source": ["from llama_index.core import get_response_synthesizer\n", "\n", "summarizer = get_response_synthesizer(\n", "    response_mode=\"refine\",\n", "    llm=instruct_llm,\n", "    verbose=True,\n", "    structured_answer_filtering=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "e90911bc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Florence Pugh\n"]}], "source": ["response = summarizer.get_response(\"who is president in the year 2050?\", texts)\n", "print(response)"]}, {"cell_type": "markdown", "id": "f6e39730", "metadata": {}, "source": ["### `CompactAndRefine`\n", "由于`CompactAndRefine`是建立在`Refine`之上的，因此该响应模式也支持结构化答案过滤。\n"]}, {"cell_type": "code", "execution_count": null, "id": "2cf1c840", "metadata": {}, "outputs": [], "source": ["from llama_index.core import get_response_synthesizer\n", "\n", "summarizer = get_response_synthesizer(\n", "    response_mode=\"compact\",\n", "    llm=instruct_llm,\n", "    verbose=True,\n", "    structured_answer_filtering=True,\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "bbf9213a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Florence Pugh\n"]}], "source": ["response = summarizer.get_response(\"who is president in the year 2050?\", texts)\n", "print(response)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}