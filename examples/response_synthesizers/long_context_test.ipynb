{"cells": [{"cell_type": "markdown", "id": "75e073b4-5751-4789-93d3-28d32dc870c7", "metadata": {}, "source": ["# 使用召回任务对长上下文语言模型进行压力测试\n", "\n", "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/agent/openai_retrieval_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在Colab中打开\"/></a>\n", "\n", "在本节中，我们将对GPT-4和Claude v2的长上下文召回能力进行压力测试。这受到了[Greg Kamradt的推文](https://x.com/GregKamradt/status/1722386725635580292?s=20)的启发。\n", "\n", "类似地，我们分析了长上下文语言模型的“大海捞针”召回能力。我们通过以下两个逐步扩展来展示：1）添加了Claude，2）测试召回，其中上下文**超出**上下文窗口，触发了响应合成策略。\n", "\n", "我们使用了一个固定的文档 - 2021年Uber的10-K报告，其中包含约290,000个标记。\n"]}, {"cell_type": "code", "execution_count": null, "id": "99c90638", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-openai\n", "%pip install llama-index-llms-anthropic"]}, {"cell_type": "code", "execution_count": null, "id": "afdab75a-a7eb-43e7-86cd-03682bf99680", "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "id": "17049cdb-c609-413e-bfa9-f86b6f0e6989", "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader, Document\n", "from llama_index.core import SummaryIndex\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.llms.anthropic import Anthropic\n", "from llama_index.core.evaluation import CorrectnessEvaluator"]}, {"cell_type": "markdown", "id": "1b94aec7-096a-4362-aa2e-e6224a8fcc37", "metadata": {}, "source": ["## 设置数据 / 索引\n", "\n", "我们加载Uber的10-k报告\n"]}, {"cell_type": "code", "execution_count": null, "id": "6777a0c7-5d8c-4b5c-9715-3056a3d23ed9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2023-11-09 00:35:55--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 1880483 (1.8M) [application/octet-stream]\n", "Saving to: ‘data/10k/uber_2021.pdf’\n", "\n", "data/10k/uber_2021. 100%[===================>]   1.79M  --.-KB/s    in 0.1s    \n", "\n", "2023-11-09 00:36:04 (18.2 MB/s) - ‘data/10k/uber_2021.pdf’ saved [1880483/1880483]\n", "\n", "--2023-11-09 00:36:04--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/lyft_2021.pdf\n", "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n", "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 1440303 (1.4M) [application/octet-stream]\n", "Saving to: ‘data/10k/lyft_2021.pdf’\n", "\n", "data/10k/lyft_2021. 100%[===================>]   1.37M  --.-KB/s    in 0.06s   \n", "\n", "2023-11-09 00:36:05 (24.7 MB/s) - ‘data/10k/lyft_2021.pdf’ saved [1440303/1440303]\n", "\n"]}], "source": ["!mkdir -p 'data/10k/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/10k/uber_2021.pdf' -O 'data/10k/uber_2021.pdf'"]}, {"cell_type": "code", "execution_count": null, "id": "c14722aa-546b-49f0-b517-3e49fd97b080", "metadata": {}, "outputs": [], "source": ["## 加载数据", "uber_docs0 = SimpleDirectoryReader(", "    input_files=[\"./data/10k/uber_2021.pdf\"]", ").load_data()", "uber_doc = Document(text=\"\\n\\n\".join([d.get_content() for d in uber_docs0]))"]}, {"cell_type": "markdown", "id": "d1e5100b-5816-4e05-a307-90c983e4734b", "metadata": {}, "source": ["我们在下面打印了标记的数量。请注意，这超出了现有LLM的上下文窗口，需要响应合成策略。\n"]}, {"cell_type": "code", "execution_count": null, "id": "42defbca-60ae-4201-88c3-3a0d9584095d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["NUM TOKENS: 291129\n"]}], "source": ["# 计算标记的数量", "from llama_index.core.utils import globals_helper", "", "num_tokens = len(globals_helper.tokenizer(uber_doc.get_content()))", "print(f\"标记数量：{num_tokens}\")"]}, {"cell_type": "markdown", "id": "cfefd8e7-9669-474c-9398-3b42c73c452c", "metadata": {}, "source": ["在这个notebook中，我们将尝试不同的实验，包括改变模型的超参数、尝试不同的数据预处理方法以及尝试不同的模型架构。我们将观察这些实验对模型性能的影响，并找出最佳的组合来优化模型。\n"]}, {"cell_type": "markdown", "id": "ed5e622c-d221-41ef-8333-680189b9301c", "metadata": {}, "source": ["### 定义上下文字符串\n", "\n", "在这里，我们插入一个单个句子的上下文，我们将在整个文档的不同位置“隐藏”它。\n"]}, {"cell_type": "code", "execution_count": null, "id": "11b381c9-2f3e-428f-ac5b-ee0e7260f2b6", "metadata": {}, "outputs": [], "source": ["context_str = \"Jerry's favorite snack is Hot Cheetos.\"\n", "query_str = \"What is Jerry's favorite snack?\""]}, {"cell_type": "code", "execution_count": null, "id": "e6189314-e1c2-4e83-b16d-52220f94ff40", "metadata": {}, "outputs": [], "source": ["def augment_doc(doc_str, context, position):", "    \"\"\"在给定位置用额外的上下文来增强文档。\"\"\"", "    doc_str1 = doc_str[:position]", "    doc_str2 = doc_str[position:]", "", "    return f\"{doc_str1}...\\n\\n{context}\\n\\n...{doc_str2}\""]}, {"cell_type": "code", "execution_count": null, "id": "5b51f45f-6292-4472-8e19-f3b9cfce47fa", "metadata": {}, "outputs": [], "source": ["test_str = augment_doc(\n", "    uber_doc.get_content(), context_str, int(0.5 * len(uber_doc.get_content()))\n", ")"]}, {"cell_type": "markdown", "id": "6aa3a124-03f7-480a-a759-9373b4990171", "metadata": {}, "source": ["### 定义实验循环\n", "\n", "实验循环如下：\n", "1. 遍历位置集合（以文档长度的百分比表示）\n", "2. 对于每个位置，在该位置注入上下文字符串。\n", "3. 将整个文档加载到我们的`SummaryIndex`中，获取相应的查询引擎。\n", "4. 当提出问题时，我们在整个文档上触发响应合成（创建和完善，或树状摘要）。\n", "5. 使用我们的`CorrectnessEvaluator`比较预测的响应与期望的响应。\n"]}, {"cell_type": "code", "execution_count": null, "id": "87357366-3de6-4264-90a0-1a5fc7a84052", "metadata": {}, "outputs": [], "source": ["async def run_experiments(", "    doc, position_percentiles, context_str, query, llm, response_mode=\"compact\"", "):", "    eval_llm = OpenAI(model=\"gpt-4-1106-preview\")", "", "    correctness_evaluator = CorrectnessEvaluator(llm=eval_llm)", "    eval_scores = {}", "    for idx, position_percentile in enumerate(position_percentiles):", "        print(f\"Position percentile: {position_percentile}\")", "        position_idx = int(position_percentile * len(uber_doc.get_content()))", "        new_doc_str = augment_doc(", "            uber_doc.get_content(), context_str, position_idx", "        )", "        new_doc = Document(text=new_doc_str)", "        index = SummaryIndex.from_documents(", "            [new_doc],", "        )", "        query_engine = index.as_query_engine(", "            response_mode=response_mode, llm=llm", "        )", "        print(f\"Query: {query}\")", "", "        # 取消注释以进行异步操作", "        # response = await query_engine.aquery(query)", "        response = query_engine.query(query)", "        print(f\"Response: {str(response)}\")", "        eval_result = correctness_evaluator.evaluate(", "            query=query, response=str(response), reference=context_str", "        )", "        eval_score = eval_result.score", "        print(f\"Eval score: {eval_score}\")", "        eval_scores[position_percentile] = eval_score", "    return eval_scores"]}, {"cell_type": "code", "execution_count": null, "id": "d4b8334b-faef-49ad-9766-3603cdd16cb0", "metadata": {}, "outputs": [], "source": ["position_percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"]}, {"cell_type": "code", "execution_count": null, "id": "18f7e4d9-e86d-42e7-9bc5-44babb39e35a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Position percentile: 0.0\n", "Query: What is Jerry's favorite snack?\n", "Response: Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.1\n", "Query: What is Jerry's favorite snack?\n", "Response: Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.2\n", "Query: What is Jerry's favorite snack?\n", "Response: Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.3\n", "Query: What is Jerry's favorite snack?\n", "Response: Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.4\n", "Query: What is Jerry's favorite snack?\n", "Response: Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.5\n", "Query: What is Jerry's favorite snack?\n", "Response: Jerry's favorite snack is not specified in the provided information.\n", "Eval score: 2.0\n", "Position percentile: 0.6\n", "Query: What is Jerry's favorite snack?\n", "Response: Repeat the original answer.\n", "Eval score: 1.0\n", "Position percentile: 0.7\n", "Query: What is Jerry's favorite snack?\n", "Response: Repeat the original answer.\n", "Eval score: 1.0\n", "Position percentile: 0.8\n", "Query: What is Jerry's favorite snack?\n", "Response: Jerry's favorite snack is Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.9\n", "Query: What is Jerry's favorite snack?\n", "Response: Jerry's favorite snack is Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 1.0\n", "Query: What is Jerry's favorite snack?\n", "Response: Hot Cheetos.\n", "Eval score: 5.0\n"]}], "source": ["llm = OpenAI(model=\"gpt-4-1106-preview\")\n", "\n", "eval_scores_gpt4 = await run_experiments(\n", "    [uber_doc],\n", "    position_percentiles,\n", "    context_str,\n", "    query_str,\n", "    llm,\n", "    response_mode=\"compact\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "c049408d-dc6f-4165-8f20-6bfab497582a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Position percentile: 0.0\n", "Query: What is Jerry's favorite snack?\n", "Response: Jerry's favorite snack is Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.1\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack from the information provided.\n", "Eval score: 1.0\n", "Position percentile: 0.2\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack as there is no information provided about Jerry or his snack preferences.\n", "Eval score: 2.0\n", "Position percentile: 0.3\n", "Query: What is Jerry's favorite snack?\n", "Response: Jerry's favorite snack is Hot Cheetos.\n", "Eval score: 5.0\n", "Position percentile: 0.4\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack from the information provided.\n", "Eval score: 1.0\n", "Position percentile: 0.5\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack from the information available.\n", "Eval score: 2.0\n", "Position percentile: 0.6\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack as there is no information provided about his preferences.\n", "Eval score: 2.0\n", "Position percentile: 0.7\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack from the information provided.\n", "Eval score: 1.0\n", "Position percentile: 0.8\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack as there is no information provided about Jerry's preferences.\n", "Eval score: 2.0\n", "Position percentile: 0.9\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack from the information provided.\n", "Eval score: 1.0\n", "Position percentile: 1.0\n", "Query: What is Jerry's favorite snack?\n", "Response: It is not possible to determine Jerry's favorite snack from the information available.\n", "Eval score: 2.0\n"]}], "source": ["llm = OpenAI(model=\"gpt-4-1106-preview\")\n", "eval_scores_gpt4_ts = await run_experiments(\n", "    [uber_doc],\n", "    position_percentiles,\n", "    context_str,\n", "    query_str,\n", "    llm,\n", "    response_mode=\"tree_summarize\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "46e1d38b-acfe-4147-818a-fd71a1007392", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Position percentile: 0.0\n", "Query: What is Jerry's favorite snack?\n", "Response:  Unfortunately I do not have enough context to determine what Jerry's favorite snack is, as the new context provided does not contain any information about his preferences or favorite snacks. Without more details about Jerry as an individual, I cannot refine my original answer about his favorite snack. I would need additional information about his tastes, habits, or direct statements from him about his snack preferences in order to update my response. The new context alone does not give me any clues to determine his favorite snack.\n", "Eval score: 2.0\n", "Position percentile: 0.1\n", "Query: What is Jerry's favorite snack?\n", "Response:  I apologize, but the new context you provided does not contain any information about someone named Jerry or what his favorite snack is. The new context discusses an intercreditor agreement, secured obligations, liens and other legal/financial details related to Uber Technologies. It does not mention anything about a person named Jerry or his favorite snacks. Since the new context does not provide any relevant information, I do not have enough details to determine or provide an answer regarding Jerry's favorite snack.\n", "Eval score: 1.0\n", "Position percentile: 0.2\n", "Query: What is Jerry's favorite snack?\n", "Response:  Unfortunately I do not have enough context to determine what Jerry's favorite snack is. The new context provided does not contain any specific information about Jerry's snack preferences. Without more details about Jerry as an individual, such as his tastes, habits, or direct statements about favorite snacks, I cannot confidently provide or refine an answer. I apologize that I cannot give a more substantive response without additional relevant information about Jerry's favorite snack.\n", "Eval score: 2.0\n", "Position percentile: 0.3\n", "Query: What is Jerry's favorite snack?\n", "Response:  Unfortunately I do not have enough context to determine Jerry's favorite snack. The original answer stated that this information was not provided in the context, and the new context does not add any relevant details about Jerry's preferences. Since the original answer already indicated the favorite snack was not mentioned, and the new context does not provide any additional relevant information, I cannot refine the original answer further.\n", "Eval score: 2.0\n", "Position percentile: 0.4\n", "Query: What is Jerry's favorite snack?\n", "Response:  Unfortunately I do not have enough context to determine what Jerry's favorite snack is. The passage provided is an excerpt from a loan agreement that does not mention anyone named Jerry or provide information about snack preferences. The passage defines lien intercreditor agreement terms, but does not give any information about a person named Jerry or their favorite snacks. Without more context about Jerry and his preferences, I cannot infer what his favorite snack is based solely on the loan agreement terms provided in the new context.\n", "Eval score: 1.0\n", "Position percentile: 0.5\n", "Query: What is Jerry's favorite snack?\n", "Response:  I do not have enough context to determine what Jerry's favorite snack is, as the excerpt from Uber's annual 10-K filing provided does not mention anyone named Jerry or provide any details about snack preferences. The passage discusses Uber's business operations and financials, but does not contain information to infer what a specific person named Jerry's favorite snack may be. Without more details about who Jerry is and the context for the question, I cannot determine Jerry's favorite snack based solely on this 10-K excerpt.\n", "Eval score: 1.0\n", "Position percentile: 0.6\n", "Query: What is Jerry's favorite snack?\n", "Response:  Unfortunately I do not have enough context to determine what Jerry's favorite snack is based on the new information provided. The new context is an excerpt from Uber's credit agreement and does not mention anything about a person named Jerry or provide any clues about his favorite snack. Since the new information does not give any details relevant to the original question, I cannot refine my previous response about Jerry's favorite snack. My original answer stating that there is insufficient information to determine Jerry's favorite snack remains applicable.\n", "Eval score: 1.0\n", "Position percentile: 0.7\n", "Query: What is Jerry's favorite snack?\n", "Response:  I apologize, but the passage provided does not contain any information about someone named Jerry or their favorite snack. The passage discusses intercreditor agreements, secured obligations and other financial and legal matters unrelated to an individual's snack preferences. Without any context about Jerry or snacks mentioned in the passage, I do not have enough information to determine what Jerry's favorite snack is.\n", "Eval score: 1.0\n", "Position percentile: 0.8\n", "Query: What is Jerry's favorite snack?\n", "Response:  Unfortunately I do not have enough context to determine what Jerry's favorite snack is, as the provided information is about Uber's financial agreements and does not mention anything about a person named Jerry or his snack preferences. The new context given does not provide any additional clues to help identify Jerry's favorite snack. I would need more specific information about Jerry as an individual to be able to determine his favorite snack.\n", "Eval score: 1.0\n", "Position percentile: 0.9\n", "Query: What is Jerry's favorite snack?\n", "Response:  Unfortunately the new context you provided does not contain any information about Jerry or his favorite snack. The passage discusses intercreditor agreements, amendments, assignments, and other financial and legal matters related to Uber, but does not mention anything about a person named Jerry or provide any clues as to what his favorite snack might be. Since the new context does not add any relevant information about Jerry or his favorite snack, I do not have enough context to determine what Jerry's favorite snack is. My original response that there is insufficient context to determine Jerry's favorite snack remains applicable.\n", "Eval score: 1.0\n", "Position percentile: 1.0\n", "Query: What is Jerry's favorite snack?\n", "Response:  Based on the additional context provided, Jerry's favorite snack is Hot Cheetos.\n", "Eval score: 5.0\n"]}], "source": ["llm = Anthropic(model=\"claude-2\")\n", "\n", "eval_scores_anthropic = await run_experiments(\n", "    [uber_doc], position_percentiles, context_str, query_str, llm\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "139b1fd9-c4ca-4a59-8947-8d11b873ea45", "metadata": {}, "outputs": [], "source": ["# 注意：不完整，遇到超时错误", "llm = Anthropic(model=\"claude-2\")", "eval_scores_anthropic = await run_experiments(", "    [uber_doc],", "    position_percentiles,", "    context_str,", "    query_str,", "    llm,", "    response_mode=\"tree_summarize\",", ")"]}], "metadata": {"kernelspec": {"display_name": "llama_index_v2", "language": "python", "name": "llama_index_v2"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}