{"cells": [{"cell_type": "markdown", "id": "beda3039", "metadata": {}, "source": ["<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/chat_engine/chat_engine_react.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "2509fcb7-f84e-4c01-b23c-6e0942c2df10", "metadata": {}, "source": ["# 聊天引擎 - ReAct代理模式\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "84aca142-a611-4a18-8673-c261edd1631c", "metadata": {}, "source": ["ReAct是一个基于代理的聊天模式，构建在对您的数据进行查询的引擎之上。\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "9d9dcfd5-ec96-4407-a5b7-ca3fadf22419", "metadata": {}, "source": ["对于每次聊天互动，代理程序会进入一个ReAct循环：\n", "* 首先决定是否使用查询引擎工具，并提出适当的输入\n", "* （可选）使用查询引擎工具并观察其输出\n", "* 决定是重复还是给出最终回应\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "d765ea5c-e193-4298-96cc-978c87ffcafe", "metadata": {}, "source": ["这种方法很灵活，因为它可以灵活地选择是否查询知识库。\n", "然而，性能也更多地取决于LLM的质量。\n", "您可能需要更多的强制措施，以确保它在正确的时机选择查询知识库，而不是产生幻觉式的答案。\n"]}, {"cell_type": "markdown", "id": "82918147", "metadata": {}, "source": ["如果您在colab上打开这个笔记本，您可能需要安装LlamaIndex 🦙。\n"]}, {"cell_type": "code", "execution_count": null, "id": "c3262785", "metadata": {}, "outputs": [], "source": ["%pip install llama-index-llms-anthropic\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "id": "e2c7c749", "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"attachments": {}, "cell_type": "markdown", "id": "8acd322f", "metadata": {}, "source": ["## 下载数据\n"]}, {"cell_type": "code", "execution_count": null, "id": "29c01fe1", "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"attachments": {}, "cell_type": "markdown", "id": "70be3c76-98d4-4df5-9cc6-8ceba34f23fb", "metadata": {}, "source": ["## 五行代码即可开始\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "8841bc6e-a196-4416-b886-31ea93e33d13", "metadata": {}, "source": ["加载数据并构建索引\n"]}, {"cell_type": "code", "execution_count": null, "id": "8bf4cb33-46c7-4f45-a43c-b478f68e9b0b", "metadata": {}, "outputs": [], "source": ["from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.llms.anthropic import Anthropic\n", "\n", "llm = OpenAI()\n", "data = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n", "index = VectorStoreIndex.from_documents(data)"]}, {"attachments": {}, "cell_type": "markdown", "id": "0df6a939-9f2c-44cc-ad9d-8203029e8fbd", "metadata": {}, "source": ["配置聊天引擎\n"]}, {"cell_type": "code", "execution_count": null, "id": "05eb7917-998c-4c36-b41a-84d29d3468c9", "metadata": {}, "outputs": [], "source": ["chat_engine = index.as_chat_engine(chat_mode=\"react\", llm=llm, verbose=True)"]}, {"attachments": {}, "cell_type": "markdown", "id": "4bbfb7bb-827d-4968-b3ae-7c9f92224e8c", "metadata": {}, "source": ["与您的数据交谈\n"]}, {"cell_type": "code", "execution_count": null, "id": "0a618731-2f1f-4379-8dac-6dc3564961c1", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[38;5;200m\u001b[1;3mThought: I need to use a tool to help me answer the question.\n", "Action: query_engine_tool\n", "Action Input: {'input': 'What did Paul Graham do in the summer of 1995?'}\n", "\u001b[0m\u001b[36;1m\u001b[1;3mObservation: \n", "In the summer of 1995, Paul Graham worked on building a web application for making web applications. He recruited Dan Giffin, who had worked for Viaweb, and two undergrads who wanted summer jobs, and they got to work trying to build what it's now clear is about twenty companies and several open source projects worth of software. The language for defining applications would of course be a dialect of Lisp.\n", "\u001b[0m\u001b[38;5;200m\u001b[1;3mResponse: In the summer of 1995, Paul Graham worked on building a web application for making web applications. He recruited Dan Giffin, who had worked for Viaweb, and two undergrads who wanted summer jobs, and they got to work trying to build what it's now clear is about twenty companies and several open source projects worth of software. The language for defining applications would of course be a dialect of Lisp.\n", "\u001b[0m"]}], "source": ["response = chat_engine.chat(\n", "    \"Use the tool to answer what did Paul Graham do in the summer of 1995?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "id": "a7c5c258-397d-47f2-87d3-7c1e62138211", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["In the summer of 1995, Paul Graham worked on building a web application for making web applications. He recruited Dan Giffin, who had worked for Viaweb, and two undergrads who wanted summer jobs, and they got to work trying to build what it's now clear is about twenty companies and several open source projects worth of software. The language for defining applications would of course be a dialect of Lisp.\n"]}], "source": ["print(response)"]}, {"attachments": {}, "cell_type": "markdown", "id": "ac91c351-d6ec-4958-9290-ad3078e7abe3", "metadata": {}, "source": ["### 自定义LLM\n"]}, {"attachments": {}, "cell_type": "markdown", "id": "60b87162-1a18-4390-b9a5-ddec6c1d1e36", "metadata": {}, "source": ["使用Anthropic（“claude-2”）\n"]}, {"cell_type": "code", "execution_count": null, "id": "29bb0f40-3173-474f-8f78-a5b82f4e5645", "metadata": {}, "outputs": [], "source": ["llm = Anthropic()"]}, {"attachments": {}, "cell_type": "markdown", "id": "aae7f923-cbd5-40af-ba2e-d3c2e3babcbf", "metadata": {}, "source": ["配置聊天引擎\n"]}, {"cell_type": "code", "execution_count": null, "id": "fa0a82b1-76db-4ba1-a6c3-b71d087ba132", "metadata": {}, "outputs": [], "source": ["chat_engine = index.as_chat_engine(llm=llm, chat_mode=\"react\", verbose=True)"]}, {"cell_type": "code", "execution_count": null, "id": "cf5d2e49-04f8-4a61-9a30-ebc227d87d41", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[38;5;200m\u001b[1;3mThought: I need to use a tool to help me answer the question.\n", "Action: query_engine_tool\n", "Action Input: {'input': 'what did Paul Graham do in the summer of 1995?'}\n", "\u001b[0m\u001b[36;1m\u001b[1;3mObservation:  Based on the context, in the summer of 1995 Paul Graham:\n", "\n", "- Painted a second still life using the same objects he had used for a previous still life painting.\n", "\n", "- Looked for an apartment to buy in New York, trying to find a neighborhood similar to Cambridge, MA. \n", "\n", "- Realized there wasn't really a \"Cambridge of New York\" after visiting the actual Cambridge.\n", "\n", "The passage does not mention what Paul Graham did in the summer of 1995 specifically. It talks about painting a second still life at some point and looking for an apartment in New York at some point, but it does not connect those events to the summer of 1995.\n", "\u001b[0m\u001b[38;5;200m\u001b[1;3mResponse: The passage does not provide enough information to know specifically what Paul Graham did in the summer of 1995. It mentions some activities like painting and looking for an apartment in New York, but does not say these occurred specifically in the summer of 1995.\n", "\u001b[0m"]}], "source": ["response = chat_engine.chat(\"what did Paul Graham do in the summer of 1995?\")"]}, {"cell_type": "code", "execution_count": null, "id": "8c6a387e-afdc-4829-852f-6916499a7352", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["The passage does not provide enough information to know specifically what Paul Graham did in the summer of 1995. It mentions some activities like painting and looking for an apartment in New York, but does not say these occurred specifically in the summer of 1995.\n"]}], "source": ["print(response)"]}, {"cell_type": "code", "execution_count": null, "id": "48d8f1c6-ad3e-49fa-b365-a7404cc25f96", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[38;5;200m\u001b[1;3mResponse:  You asked me \"what did Paul Graham do in the summer of 1995?\".\n", "\u001b[0m"]}], "source": ["response = chat_engine.chat(\"What did I ask you before?\")"]}, {"cell_type": "code", "execution_count": null, "id": "1d5c0588-ed90-4930-92d8-89d8830b120c", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": [" You asked me \"what did Paul Graham do in the summer of 1995?\".\n"]}], "source": ["print(response)"]}, {"attachments": {}, "cell_type": "markdown", "id": "42c4a975-8034-447f-a5fe-1e71e59f02c0", "metadata": {}, "source": ["把聊天引擎重置\n"]}, {"cell_type": "code", "execution_count": null, "id": "c1326c7a-8056-47b9-924d-4a836a501312", "metadata": {}, "outputs": [], "source": ["chat_engine.reset()"]}, {"cell_type": "code", "execution_count": null, "id": "365074a3-b9b3-4b73-a5a1-230267379175", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[38;5;200m\u001b[1;3mResponse: I'm afraid I don't have any context about previous questions in our conversation. This seems to be the start of a new conversation between us.\n", "\u001b[0m"]}], "source": ["response = chat_engine.chat(\"What did I ask you before?\")"]}, {"cell_type": "code", "execution_count": null, "id": "37a387f8-4b90-4a73-9ffc-3960e6dbac5b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["I'm afraid I don't have any context about previous questions in our conversation. This seems to be the start of a new conversation between us.\n"]}], "source": ["print(response)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}