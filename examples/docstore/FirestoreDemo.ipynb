{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Firestoreæ¼”ç¤º\n", "\n", "æœ¬æŒ‡å—å‘æ‚¨å±•ç¤ºå¦‚ä½•ç›´æ¥ä½¿ç”¨ç”±Google Firestoreæ”¯æŒçš„`DocumentStore`æŠ½è±¡ã€‚é€šè¿‡å°†èŠ‚ç‚¹æ”¾å…¥æ–‡æ¡£å­˜å‚¨ä¸­ï¼Œè¿™ä½¿æ‚¨èƒ½å¤Ÿåœ¨ç›¸åŒçš„åŸºç¡€æ–‡æ¡£å­˜å‚¨ä¸Šå®šä¹‰å¤šä¸ªç´¢å¼•ï¼Œè€Œä¸æ˜¯åœ¨ç´¢å¼•ä¹‹é—´å¤åˆ¶æ•°æ®ã€‚\n", "\n", "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/docstore/FirestoreDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"åœ¨Colabä¸­æ‰“å¼€\"/></a>\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["å¦‚æœæ‚¨åœ¨Colabä¸Šæ‰“å¼€è¿™ä¸ªç¬”è®°æœ¬ï¼Œæ‚¨å¯èƒ½éœ€è¦å®‰è£…LlamaIndex ğŸ¦™ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%pip install llama-index-storage-docstore-firestore\n", "%pip install llama-index-storage-kvstore-firestore\n", "%pip install llama-index-storage-index-store-firestore\n", "%pip install llama-index-llms-openai"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install llama-index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging\n", "import sys\n", "\n", "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n", "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core import SimpleDirectoryReader, StorageContext\n", "from llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex\n", "from llama_index.core import SummaryIndex\n", "from llama_index.core import ComposableGraph\n", "from llama_index.llms.openai import OpenAI\n", "from llama_index.core.response.notebook_utils import display_response\n", "from llama_index.core import Settings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### ä¸‹è½½æ•°æ®\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!mkdir -p 'data/paul_graham/'\n", "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### åŠ è½½æ–‡æ¡£\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["reader = SimpleDirectoryReader(\"./data/paul_graham/\")\n", "documents = reader.load_data()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### è§£æåˆ°èŠ‚ç‚¹\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.core.node_parser import SentenceSplitter\n", "\n", "nodes = SentenceSplitter().get_nodes_from_documents(documents)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### æ·»åŠ åˆ°æ–‡æ¡£åº“\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from llama_index.storage.kvstore.firestore import FirestoreKVStore\n", "from llama_index.storage.docstore.firestore import FirestoreDocumentStore\n", "from llama_index.storage.index_store.firestore import FirestoreIndexStore"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["kvstore = FirestoreKVStore()\n", "\n", "storage_context = StorageContext.from_defaults(\n", "    docstore=FirestoreDocumentStore(kvstore),\n", "    index_store=FirestoreIndexStore(kvstore),\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["storage_context.docstore.add_documents(nodes)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### å®šä¹‰å¤šä¸ªç´¢å¼•\n", "\n", "æ¯ä¸ªç´¢å¼•ä½¿ç”¨ç›¸åŒçš„åŸºç¡€èŠ‚ç‚¹ã€‚\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["summary_index = SummaryIndex(nodes, storage_context=storage_context)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vector_index = VectorStoreIndex(nodes, storage_context=storage_context)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["keyword_table_index = SimpleKeywordTableIndex(\n", "    nodes, storage_context=storage_context\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# æ³¨æ„ï¼šæ–‡æ¡£å­˜å‚¨ä»ç„¶å…·æœ‰ç›¸åŒçš„èŠ‚ç‚¹\n", "len(storage_context.docstore.docs)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### æµ‹è¯•ä¿å­˜å’ŒåŠ è½½\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# æ³¨æ„ï¼šé»˜è®¤æƒ…å†µä¸‹ï¼Œdocstoreå’Œindex_storeä¼šæŒä¹…åŒ–åˆ°Firestoreä¸­\n", "# æ³¨æ„ï¼šè¿™é‡Œåªéœ€è¦å°†ç®€å•çš„å‘é‡å­˜å‚¨æŒä¹…åŒ–åˆ°ç£ç›˜ä¸­\n", "storage_context.persist()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# è®°å½•ç´¢å¼•ID\n", "list_id = summary_index.index_id  # æ‘˜è¦ç´¢å¼•ID\n", "vector_id = vector_index.index_id  # å‘é‡ç´¢å¼•ID\n", "keyword_id = keyword_table_index.index_id  # å…³é”®è¯è¡¨ç´¢å¼•ID"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "from llama_index.core import load_index_from_storage\n", "\n", "kvstore = FirestoreKVStore()\n", "\n", "# é‡æ–°åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡\n", "storage_context = StorageContext.from_defaults(\n", "    docstore=FirestoreDocumentStore(kvstore),\n", "    index_store=FirestoreIndexStore(kvstore),\n", ")\n", "\n", "# åŠ è½½ç´¢å¼•\n", "summary_index = load_index_from_storage(\n", "    storage_context=storage_context, index_id=list_id\n", ")\n", "vector_index = load_index_from_storage(\n", "    storage_context=storage_context, vector_id=vector_id\n", ")\n", "keyword_table_index = load_index_from_storage(\n", "    storage_context=storage_context, keyword_id=keyword_id\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### æµ‹è¯•ä¸€äº›æŸ¥è¯¢è¯­å¥\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chatgpt = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n", "Settings.llm = chatgpt\n", "Settings.chunk_size = 1024"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = summary_index.as_query_engine()\n", "list_response = query_engine.query(\"What is a summary of this document?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["display_response(list_response)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = vector_index.as_query_engine()\n", "vector_response = query_engine.query(\"What did the author do growing up?\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["display_response(vector_response)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["query_engine = keyword_table_index.as_query_engine()\n", "keyword_response = query_engine.query(\n", "    \"What did the author do after his time at YC?\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["display_response(keyword_response)"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 4}